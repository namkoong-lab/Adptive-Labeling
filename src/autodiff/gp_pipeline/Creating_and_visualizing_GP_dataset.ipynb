{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azuF34aVtftE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gpytorch\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_init_train_samples = 20\n",
        "num_pool_samples = 5\n",
        "num_test_samples = 20\n",
        "input_dim = 1"
      ],
      "metadata": {
        "id": "Hw-zjP0gtqXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_train_x = torch.rand((num_init_train_samples, input_dim))*50.0\n",
        "test_x_1 = torch.rand((num_test_samples, input_dim))*50.0\n",
        "test_x_2 = 75.0 + torch.rand((num_test_samples, input_dim))*50.0\n",
        "test_x_3 = 175.0 + torch.rand((num_test_samples, input_dim))*50.0\n",
        "test_x = torch.cat([test_x_1,test_x_2,test_x_3])\n",
        "pool_x_1 = 24 + torch.rand((num_pool_samples, input_dim))*2\n",
        "pool_x_2 = 99 + torch.rand((num_pool_samples, input_dim))*2\n",
        "pool_x_3 = 199 + torch.rand((num_pool_samples, input_dim))*2\n",
        "pool_x = torch.cat([pool_x_1,pool_x_2,pool_x_3])\n",
        "y = torch.zeros(num_init_train_samples+3*num_pool_samples+3*num_test_samples)"
      ],
      "metadata": {
        "id": "EIm9FZ_Itr6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_train_x_numpy = init_train_x.numpy()\n",
        "init_train_y = torch.zeros(init_train_x.size(0))\n",
        "test_x_numpy = test_x.numpy()\n",
        "test_y = torch.ones(test_x.size(0))\n",
        "pool_x_numpy = pool_x.numpy()\n",
        "pool_y = torch.empty(pool_x.size(0)).fill_(0.5)\n",
        "\n",
        "\n",
        "plt.scatter(init_train_x_numpy, init_train_y.numpy(), s=20, label='train')\n",
        "plt.scatter(test_x_numpy, test_y.numpy(), s=20, label='test')\n",
        "plt.scatter(pool_x_numpy, pool_y.numpy(), s=20, label='pool')\n",
        "\n",
        "plt.yticks([])  # Hide y-axis ticks\n",
        "plt.xlabel('X values')\n",
        "plt.legend()\n",
        "plt.title('Distribution of X values along a real line')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ONBjIBa1tuP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.cat([init_train_x,test_x,pool_x])"
      ],
      "metadata": {
        "id": "SVvi0pKvtwaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters for the model\n",
        "mean_constant = 0.0  # Mean of the GP\n",
        "length_scale = 25.0   # Length scale of the RBF kernel\n",
        "noise_std = 0.01     # Standard deviation of the noise\n",
        "\n",
        "\n",
        "mean_module = gpytorch.means.ConstantMean()\n",
        "base_kernel = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
        "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
        "\n",
        "\n",
        "mean_module.constant = mean_constant\n",
        "base_kernel.base_kernel.lengthscale = length_scale\n",
        "likelihood.noise_covar.noise = noise_std**2"
      ],
      "metadata": {
        "id": "gBWj2ddctyHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomizableGPModel(gpytorch.models.ExactGP):\n",
        "    def __init__(self, train_x, train_y, mean_module, base_kernel, likelihood):\n",
        "        super(CustomizableGPModel, self).__init__(train_x, train_y, likelihood)\n",
        "        self.mean_module = mean_module\n",
        "        self.covar_module = base_kernel\n",
        "        self.likelihood = likelihood\n",
        "\n",
        "    def forward(self, x):\n",
        "        return gpytorch.distributions.MultivariateNormal(self.mean_module(x), self.covar_module(x))\n",
        "\n",
        "\n",
        "model = CustomizableGPModel(x, y, mean_module, base_kernel, likelihood)"
      ],
      "metadata": {
        "id": "IteaLbdKtzvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample from the prior for training data\n",
        "model.eval()\n",
        "likelihood.eval()\n",
        "with torch.no_grad():\n",
        "    prior_dist = likelihood(model(x))\n",
        "    y_new = prior_dist.sample()"
      ],
      "metadata": {
        "id": "-dod_dkOt1pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x[:num_init_train_samples],y_new[:num_init_train_samples], label='train')\n",
        "plt.scatter(x[num_init_train_samples:num_init_train_samples+3*num_test_samples],y_new[num_init_train_samples:num_init_train_samples+3*num_test_samples], label='test')\n",
        "plt.scatter(x[num_init_train_samples+3*num_test_samples:],y_new[num_init_train_samples+3*num_test_samples:], label='pool')\n",
        "plt.ylim(-0.1, 0.1)"
      ],
      "metadata": {
        "id": "t63vTcMKt3Vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConstantValueNetwork(nn.Module):\n",
        "    def __init__(self, constant_value=1.0, output_size=1):\n",
        "        super(ConstantValueNetwork, self).__init__()\n",
        "        # Define the constant value and output size\n",
        "        self.constant_value = nn.Parameter(torch.tensor([constant_value]*output_size), requires_grad=False)\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is your input tensor. Its value is ignored in this model.\n",
        "        # Return a 1-D tensor with the constant value for each item in the batch.\n",
        "        batch_size = x.size(0)  # Get the batch size from the input\n",
        "        return self.constant_value.expand(batch_size, self.output_size)"
      ],
      "metadata": {
        "id": "lCJ-JxcxrwzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Adapting L_2 loss for the GP pipeine\n",
        "\n",
        "def var_l2_loss_estimator(model, test_x, Predictor, device, para):\n",
        "\n",
        "    N_iter =  100\n",
        "    seed = 0\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    latent_posterior = model(test_x)\n",
        "    latent_posterior_sample = latent_posterior.rsample(sample_shape=torch.Size([N_iter]))\n",
        "    prediction = Predictor(test_x).squeeze()\n",
        "    L_2_loss_each_point = torch.square(torch.subtract(latent_posterior_sample, prediction))\n",
        "    L_2_loss_each_f = torch.mean(L_2_loss_each_point, dim=1)\n",
        "    L_2_loss_variance = torch.var(L_2_loss_each_f)\n",
        "    print(\"L_2_loss_variance:\",L_2_loss_variance)\n",
        "\n",
        "    L_2_loss_mean = torch.mean(L_2_loss_each_f)+model.likelihood.noise\n",
        "    print(\"L_2_loss_mean:\", L_2_loss_mean)\n",
        "\n",
        "    return L_2_loss_variance\n",
        "\n"
      ],
      "metadata": {
        "id": "se8sF00yr1nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def l2_loss(test_x, test_y, Predictor, device):\n",
        "    prediction = Predictor(test_x).squeeze()\n",
        "    #print(\"prediction:\", prediction)\n",
        "    #print(\"test_y:\", test_y)\n",
        "    diff_square = torch.square(torch.subtract(test_y, prediction))\n",
        "    #print(\"diff_square:\", diff_square)\n",
        "    return torch.mean(diff_square)"
      ],
      "metadata": {
        "id": "Wt76RKMQr_XG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Predictor = ConstantValueNetwork(constant_value=0.0, output_size=1)"
      ],
      "metadata": {
        "id": "YRqLU2eXz-6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.set_train_data(inputs=x[:num_init_train_samples], targets=y_new[:num_init_train_samples], strict=False)       ####### CAN ALSO USE TRAINING OVER NLL HERE########\n",
        "\n",
        "### IMP LINK - https://github.com/cornellius-gp/gpytorch/issues/1409\n",
        "### IMP LINK - https://docs.gpytorch.ai/en/latest/examples/01_Exact_GPs/Simple_GP_Regression.html\n",
        "posterior = (model(x))\n",
        "posterior_mean = posterior.mean\n",
        "posterior_var = posterior.variance\n",
        "#print(\"posterior_var:\",posterior_var)\n",
        "\n",
        "\n",
        "plt.scatter(x,posterior_mean.detach().numpy())\n",
        "plt.scatter(x.squeeze(),posterior_mean.detach().numpy()-2*torch.sqrt(posterior_var).detach().numpy(),alpha=0.2)\n",
        "plt.scatter(x.squeeze(),posterior_mean.detach().numpy()+2*torch.sqrt(posterior_var).detach().numpy(),alpha=0.2)\n",
        "plt.ylim(-2, 2)\n",
        "\n",
        "var_l2_loss_estimator(model, test_x, Predictor, None, None)\n",
        "\n",
        "l_2_loss_actual = l2_loss(test_x, y_new[num_init_train_samples:num_init_train_samples+3*num_test_samples], Predictor, None)\n",
        "print(\"l_2_loss_actual:\", l_2_loss_actual)"
      ],
      "metadata": {
        "id": "eFC_KAPD0DF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_train_x = torch.cat([x[:num_init_train_samples],x[-2:]])\n",
        "new_train_y = torch.cat([y_new[:num_init_train_samples],y_new[-2:]])\n",
        "\n",
        "model.set_train_data(inputs=new_train_x, targets=new_train_y, strict=False)       ####### CAN ALSO USE TRAINING OVER NLL HERE########\n",
        "\n",
        "posterior = likelihood(model(x))\n",
        "posterior_mean = posterior.mean\n",
        "posterior_var = posterior.variance\n",
        "\n",
        "\n",
        "plt.scatter(x,posterior_mean.detach().numpy())\n",
        "plt.scatter(x.squeeze(),posterior_mean.detach().numpy()-2*torch.sqrt(posterior_var).detach().numpy(),alpha=0.2)\n",
        "plt.scatter(x.squeeze(),posterior_mean.detach().numpy()+2*torch.sqrt(posterior_var).detach().numpy(),alpha=0.2)\n",
        "plt.ylim(-2, 2)\n",
        "var_l2_loss_estimator(model, test_x, Predictor, None, None)"
      ],
      "metadata": {
        "id": "6hGwoGlO0FPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_train_x = torch.cat([x[:num_init_train_samples],x[num_init_train_samples+num_test_samples*3+num_pool_samples+1:num_init_train_samples+num_test_samples*3+num_pool_samples*2+2],x[-1:]])\n",
        "new_train_y = torch.cat([y_new[:num_init_train_samples],y[num_init_train_samples+num_test_samples*3+num_pool_samples+1:num_init_train_samples+num_test_samples*3+num_pool_samples*2+2],y_new[-1:]])\n",
        "\n",
        "model.set_train_data(inputs=new_train_x, targets=new_train_y, strict=False)       ####### CAN ALSO USE TRAINING OVER NLL HERE########\n",
        "\n",
        "posterior = likelihood(model(x))\n",
        "posterior_mean = posterior.mean\n",
        "posterior_var = posterior.variance\n",
        "\n",
        "\n",
        "plt.scatter(x,posterior_mean.detach().numpy())\n",
        "plt.scatter(x.squeeze(),posterior_mean.detach().numpy()-2*torch.sqrt(posterior_var).detach().numpy(),alpha=0.2)\n",
        "plt.scatter(x.squeeze(),posterior_mean.detach().numpy()+2*torch.sqrt(posterior_var).detach().numpy(),alpha=0.2)\n",
        "plt.ylim(-2, 2)\n",
        "var_l2_loss_estimator(model, test_x, Predictor, None, None)"
      ],
      "metadata": {
        "id": "RNmyVJEx0I3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RBFKernel(nn.Module):\n",
        "    def __init__(self, length_scale= 0.6931471824645996, output_scale = 0.6931471824645996):\n",
        "        super(RBFKernel, self).__init__()\n",
        "        self.length_scale = length_scale\n",
        "        self.output_scale = output_scale\n",
        "    def forward(self, x1, x2):\n",
        "        dist_matrix = torch.cdist(x1.unsqueeze(0), x2.unsqueeze(0), p=2).squeeze(0)**2\n",
        "        return self.output_scale*torch.exp(-0.5 * dist_matrix / self.length_scale**2)\n"
      ],
      "metadata": {
        "id": "KeQUiEibsZfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussianProcessCholesky(nn.Module):\n",
        "    def __init__(self, kernel):\n",
        "        super(GaussianProcessCholesky, self).__init__()\n",
        "        self.kernel = kernel\n",
        "\n",
        "    def forward(self, x_train, y_train, w_train, x_test, noise=1e-4):\n",
        "\n",
        "        # Apply weights only to non-diagonal elements\n",
        "\n",
        "        K = self.kernel(x_train, x_train) + noise * torch.eye(x_train.size(0)) + 1e-6 * torch.eye(x_train.size(0))\n",
        "        non_diag_mask = 1 - torch.eye(K.size(-2), K.size(-1))\n",
        "        weight_matrix = w_train.unsqueeze(-1) * w_train.unsqueeze(-2)\n",
        "        weighted_K =  K * (non_diag_mask * weight_matrix + (1 - non_diag_mask))\n",
        "\n",
        "\n",
        "\n",
        "        K_s = self.kernel(x_train, x_test)\n",
        "        weighted_K_s = torch.diag(w_train)@K_s\n",
        "\n",
        "        K_ss = self.kernel(x_test, x_test) + 1e-6 * torch.eye(x_test.size(0))\n",
        "\n",
        "        L = torch.linalg.cholesky(weighted_K)\n",
        "        alpha = torch.cholesky_solve(y_train.unsqueeze(1), L)\n",
        "        mu = weighted_K_s.t().matmul(alpha).squeeze(-1)\n",
        "\n",
        "        v = torch.linalg.solve(L, weighted_K_s)\n",
        "        cov = K_ss - v.t().matmul(v)\n",
        "\n",
        "        return mu, cov\n"
      ],
      "metadata": {
        "id": "UNw4PwrCsW_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def sample_multivariate_normal(mu, cov, n_samples):\n",
        "    \"\"\"\n",
        "    Sample from a multivariate normal distribution using the reparameterization trick.\n",
        "\n",
        "    Parameters:\n",
        "    - mu (torch.Tensor): The mean vector of the distribution.    1-D dimension [D]\n",
        "    - cov (torch.Tensor): The covariance matrix of the distribution.  2-D dimension [D,D]\n",
        "    - n_samples (int): The number of samples to generate.\n",
        "\n",
        "    Returns:\n",
        "    - torch.Tensor: Samples from the multivariate normal distribution.\n",
        "    \"\"\"\n",
        "    # Ensure mu and cov are tensors\n",
        "    #mu = torch.tensor(mu, dtype=torch.float32)\n",
        "    #cov = torch.tensor(cov, dtype=torch.float32)\n",
        "\n",
        "    # Cholesky decomposition of the covariance matrix\n",
        "    L = torch.linalg.cholesky(cov + 1e-5 * torch.eye(cov.size(0)))\n",
        "\n",
        "    #L = torch.linalg.cholesky(cov + 1e-8 * torch.eye(cov.size(0)))\n",
        "\n",
        "    # Sample Z from a standard normal distribution\n",
        "    Z = torch.randn(n_samples, mu.size(0))           # Z: [n_samples, D]\n",
        "\n",
        "    # Transform Z to obtain samples from the target distribution\n",
        "    samples = mu + Z @ L.T\n",
        "\n",
        "    return samples    #[n_samples, D]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "frG5aU7asgY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Adapting L_2 loss for the GP pipeine\n",
        "\n",
        "def var_l2_loss_custom_gp_estimator(mu, cov, noise, test_x, Predictor, device, para):\n",
        "\n",
        "\n",
        "    N_iter =  100\n",
        "    seed = 0\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    latent_posterior_sample = sample_multivariate_normal(mu, cov, N_iter)\n",
        "    prediction = Predictor(test_x).squeeze()\n",
        "    L_2_loss_each_point = torch.square(torch.subtract(latent_posterior_sample, prediction))\n",
        "    L_2_loss_each_f = torch.mean(L_2_loss_each_point, dim=1)\n",
        "    L_2_loss_variance = torch.var(L_2_loss_each_f)\n",
        "    print(\"L_2_loss_variance:\",L_2_loss_variance)\n",
        "\n",
        "    L_2_loss_mean = torch.mean(L_2_loss_each_f)+noise\n",
        "    print(\"L_2_loss_mean:\", L_2_loss_mean)\n",
        "\n",
        "    return L_2_loss_variance\n",
        "\n"
      ],
      "metadata": {
        "id": "zt26-x9KsiYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x[:num_init_train_samples]\n",
        "x_pool_1 = x[num_init_train_samples+num_test_samples*3:num_init_train_samples+num_test_samples*3+num_pool_samples*3-2]\n",
        "x_pool_2 = x[num_init_train_samples+num_test_samples*3+num_pool_samples*3-2:]\n",
        "\n",
        "y_train = y_new[:num_init_train_samples]\n",
        "y_pool_1 = y_new[num_init_train_samples+num_test_samples*3:num_init_train_samples+num_test_samples*3+num_pool_samples*3-2]\n",
        "y_pool_2 = y_new[num_init_train_samples+num_test_samples*3+num_pool_samples*3-2:]\n",
        "\n",
        "\n",
        "x_gp = torch.cat([x_train,x_pool_1,x_pool_2], dim=0)\n",
        "y_gp = torch.cat([y_train,y_pool_1,y_pool_2], dim=0)\n",
        "\n",
        "w_train = torch.ones(x_train.size(0), requires_grad = True)\n",
        "w_pool_1 = torch.zeros(x_pool_1.size(0), requires_grad = True)\n",
        "w_pool_2 = torch.zeros(x_pool_2.size(0), requires_grad = True)\n",
        "w_gp = torch.cat([w_train,w_pool_1,w_pool_2])\n",
        "\n",
        "\n",
        "\n",
        "kernel = RBFKernel(length_scale=25.0, output_scale = 0.6931471824645996)\n",
        "gp = GaussianProcessCholesky(kernel=kernel)\n",
        "noise = 1e-4\n",
        "# Prediction\n",
        "mu2, cov2 = gp(x_gp, y_gp, w_gp, test_x, noise)\n",
        "\n",
        "var_l2_loss_custom_gp_estimator(mu2, cov2, 1e-4, test_x, Predictor, None, None)\n",
        "\n",
        "plt.scatter(test_x,mu2.detach().numpy())\n",
        "plt.scatter(test_x.squeeze(),mu2.detach().numpy()-2*torch.sqrt(torch.diag(cov2)).detach().numpy(),alpha=0.2)\n",
        "plt.scatter(test_x.squeeze(),mu2.detach().numpy()+2*torch.sqrt(torch.diag(cov2)).detach().numpy(),alpha=0.2)\n",
        "plt.ylim(-2, 2)"
      ],
      "metadata": {
        "id": "Vssor2wI2F1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x[:num_init_train_samples]\n",
        "x_pool_1 = x[num_init_train_samples+num_test_samples*3:num_init_train_samples+num_test_samples*3+num_pool_samples*3-2]\n",
        "x_pool_2 = x[num_init_train_samples+num_test_samples*3+num_pool_samples*3-2:]\n",
        "\n",
        "y_train = y_new[:num_init_train_samples]\n",
        "y_pool_1 = y_new[num_init_train_samples+num_test_samples*3:num_init_train_samples+num_test_samples*3+num_pool_samples*3-2]\n",
        "y_pool_2 = y_new[num_init_train_samples+num_test_samples*3+num_pool_samples*3-2:]\n",
        "\n",
        "\n",
        "x_gp = torch.cat([x_train,x_pool_1,x_pool_2], dim=0)\n",
        "y_gp = torch.cat([y_train,y_pool_1,y_pool_2], dim=0)\n",
        "\n",
        "w_train = torch.ones(x_train.size(0), requires_grad = True)\n",
        "w_pool_1 = torch.zeros(x_pool_1.size(0), requires_grad = True)\n",
        "w_pool_2 = torch.ones(x_pool_2.size(0), requires_grad = True)\n",
        "w_gp = torch.cat([w_train,w_pool_1,w_pool_2])\n",
        "\n",
        "\n",
        "\n",
        "kernel = RBFKernel(length_scale=25.0)\n",
        "gp = GaussianProcessCholesky(kernel=kernel)\n",
        "\n",
        "# Prediction\n",
        "mu, cov = gp(x_gp, y_gp, w_gp, test_x)\n",
        "\n",
        "var_loss = var_l2_loss_custom_gp_estimator(mu, cov, 1e-4, test_x, Predictor, None, None)\n",
        "\n",
        "var_loss.backward()\n",
        "\n",
        "plt.scatter(test_x,mu.detach().numpy())\n",
        "plt.scatter(test_x.squeeze(),mu.detach().numpy()-2*torch.sqrt(torch.diag(cov)).detach().numpy(),alpha=0.2)\n",
        "plt.scatter(test_x.squeeze(),mu.detach().numpy()+2*torch.sqrt(torch.diag(cov)).detach().numpy(),alpha=0.2)\n",
        "plt.ylim(-2, 2)"
      ],
      "metadata": {
        "id": "yIAXXTkT2H7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Advanced version for training as well\n",
        "\n",
        "\n",
        "class RBFKernelAdvanced(nn.Module):\n",
        "    def __init__(self, length_scale_init=0.6931471824645996, variance_init=0.6931471824645996):\n",
        "        super(RBFKernelAdvanced, self).__init__()\n",
        "        self.raw_length_scale = nn.Parameter(torch.tensor([length_scale_init], dtype=torch.float))\n",
        "        self.raw_variance = nn.Parameter(torch.tensor([variance_init], dtype=torch.float))\n",
        "\n",
        "        self.softplus = nn.Softplus()\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        length_scale = self.softplus(self.raw_length_scale)\n",
        "        variance = self.softplus(self.raw_variance)\n",
        "        #length_scale = self.raw_length_scale\n",
        "        #variance = self.raw_variance\n",
        "        #sqdist = torch.cdist(x1, x2) ** 2\n",
        "        dist_matrix = torch.cdist(x1.unsqueeze(0), x2.unsqueeze(0), p=2).squeeze(0)**2\n",
        "        return variance * torch.exp(-0.5  * dist_matrix / length_scale ** 2)\n",
        "\n",
        "\n",
        "class GaussianProcessCholeskyAdvanced(nn.Module):\n",
        "    def __init__(self, length_scale_init=0.6931471824645996, variance_init=0.6931471824645996, noise_var_init=0.1):\n",
        "        super(GaussianProcessCholeskyAdvanced, self).__init__()\n",
        "        self.rbf_kernel = RBFKernelAdvanced(length_scale_init=length_scale_init, variance_init=variance_init)\n",
        "        self.raw_noise_var = nn.Parameter(torch.tensor([noise_var_init], dtype=torch.float))\n",
        "\n",
        "        self.softplus = nn.Softplus()\n",
        "\n",
        "    def forward(self, x_train, y_train, w_train, x_test):\n",
        "\n",
        "        # Apply weights only to non-diagonal elements\n",
        "\n",
        "        noise_var = self.softplus(self.raw_noise_var)\n",
        "\n",
        "        K = self.kernel(x_train, x_train) + noise_var * torch.eye(x_train.size(0), device=x_train.device) + 1e-6 * torch.eye(x_train.size(0), device=x_train.device)\n",
        "        non_diag_mask = 1 - torch.eye(K.size(-2), K.size(-1), device=x_train.device)\n",
        "        weight_matrix = w_train.unsqueeze(-1) * w_train.unsqueeze(-2)\n",
        "        weighted_K =  K * (non_diag_mask * weight_matrix + (1 - non_diag_mask))\n",
        "\n",
        "\n",
        "\n",
        "        K_s = self.kernel(x_train, x_test)\n",
        "        weighted_K_s = torch.diag(w_train)@K_s\n",
        "\n",
        "        K_ss = self.kernel(x_test, x_test) + 1e-6 * torch.eye(x_test.size(0), device=x_test.device)\n",
        "\n",
        "        L = torch.linalg.cholesky(weighted_K)\n",
        "        alpha = torch.cholesky_solve(y_train.unsqueeze(1), L)\n",
        "        mu = weighted_K_s.t().matmul(alpha).squeeze(-1)\n",
        "\n",
        "        v = torch.linalg.solve(L, weighted_K_s)\n",
        "        cov = K_ss - v.t().matmul(v)\n",
        "\n",
        "        return mu, cov\n",
        "\n",
        "    def nll(self, x_train, y_train):\n",
        "\n",
        "        noise_var = self.softplus(self.raw_noise_var)\n",
        "        #noise_var = self.raw_noise_var\n",
        "\n",
        "        K = self.rbf_kernel(x_train, x_train) + noise_var * torch.eye(x_train.size(0), device=x_train.device) + 1e-6 * torch.eye(x_train.size(0), device=x_train.device)\n",
        "        #print(K)\n",
        "        #print(\"K:\", K)\n",
        "        L = torch.linalg.cholesky(K)\n",
        "        #print(\"L:\", L)\n",
        "        alpha = torch.cholesky_solve(y_train.unsqueeze(1), L)\n",
        "        #print(\"alpha:\", alpha)\n",
        "        nll = 0.5 * y_train.dot(alpha.flatten())\n",
        "        #print(\"nll:\", nll)\n",
        "        nll += torch.log(torch.diag(L)).sum()\n",
        "        #print(\"nll:\", nll)\n",
        "        #print(\"0.5 * len(x_train) * torch.log(2 * torch.pi):\", 0.5 * len(x_train) * torch.log(torch.tensor(2 * torch.pi, device=nll.device)))\n",
        "        nll += 0.5 * len(x_train) * torch.log(torch.tensor(2 * torch.pi, device=nll.device))\n",
        "        print(\"nll:\", nll)\n",
        "        return nll\n"
      ],
      "metadata": {
        "id": "-G7eaHqPsqLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gp_model = GaussianProcessCholeskyAdvanced(length_scale_init=25.0, variance_init=0.6931471824645996, noise_var_init=0.0001)\n",
        "loss = gp_model.nll(x_train, y_train)"
      ],
      "metadata": {
        "id": "zB0CiLD-syvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gp_model(gp_model, x_train, y_train, optimizer, num_epochs=50):\n",
        "    gp_model.train()  # Set the model to training mode\n",
        "    for epoch in range(num_epochs):\n",
        "        optimizer.zero_grad()  # Clear previous gradients\n",
        "        loss = gp_model.nll(x_train, y_train)  # Compute the loss (NLL)\n",
        "        loss.backward()  # Compute gradients\n",
        "        optimizer.step()  # Update parameters\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print_model_parameters(gp_model)\n",
        "            print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
        ""
      ],
      "metadata": {
        "id": "-ViIcfBFs0hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_model_parameters(model):\n",
        "    for name, param in model.named_parameters():\n",
        "        print(f\"{name}: {param.data}\")"
      ],
      "metadata": {
        "id": "a4OIBZnYs9ZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gp_model = GaussianProcessCholeskyAdvanced(length_scale_init=25.0, variance_init=0.69, noise_var_init=0.0001)\n",
        "optimizer = torch.optim.Adam(gp_model.parameters(), lr=0.01, weight_decay = 0.01)\n",
        "print_model_parameters(gp_model)"
      ],
      "metadata": {
        "id": "yUIHjVHVs7YE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x[:num_init_train_samples]\n",
        "y_train = y_new[:num_init_train_samples]\n",
        "\n",
        "\n",
        "# Train the model\n",
        "train_gp_model(gp_model, x_train, y_train, optimizer, num_epochs=10000)"
      ],
      "metadata": {
        "id": "w-wnFWbRtBRJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}