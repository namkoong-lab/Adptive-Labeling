{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Mg6M-w6h0ei"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import typing\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.distributions as distributions\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import higher\n",
        "\n",
        "from dataloader import TabularDataset\n",
        "from dataloader import TabularDatasetPool\n",
        "\n",
        "from k_subset_sampling import SubsetOperator\n",
        "from NN_feature_weights import NN_feature_weights\n",
        "from enn import basenet_with_learnable_epinet_and_ensemble_prior\n",
        "\n",
        "\n",
        "from ENN_loss_func import weighted_nll_loss\n",
        "\n",
        "\n",
        "from var_recall_estimator import var_recall_estimator    #Yuanzhe\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define a configuration class for dataset-related parameters\n",
        "@dataclass\n",
        "class DatasetConfig:\n",
        "    csv_file_train: str\n",
        "    csv_file_test: str\n",
        "    csv_file_pool: str\n",
        "    y_column: str  # Assuming same column name across above 3 sets\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    batch_size_train: int\n",
        "    batch_size_test: int\n",
        "    batch_size_query: int\n",
        "    temp_k_subset: float\n",
        "    hidden_sizes_weight_NN: list\n",
        "    meta_opt_lr: float\n",
        "    n_classes: int\n",
        "    n_epoch: int\n",
        "    init_train_lr: float\n",
        "    init_train_weight_decay: float\n",
        "    n_train_init: int\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    n_train_iter: int\n",
        "    n_ENN_iter: int\n",
        "    ENN_opt_lr: float\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ENNConfig:\n",
        "    basenet_hidden_sizes: list\n",
        "    exposed_layers: list\n",
        "    z_dim: int\n",
        "    learnable_epinet_hiddens: list\n",
        "    hidden_sizes_prior: list\n",
        "    seed_base: int\n",
        "    seed_learnable_epinet: int\n",
        "    seed_prior_epinet: int\n",
        "    alpha: float\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TLFaQXIpoMhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def experiment(dataset_config: DatasetConfig, model_config: ModelConfig, train_config: TrainConfig, enn_config: ENNConfig, Predictor):\n",
        "\n",
        "\n",
        "    # Predictor here has already been pretrained\n",
        "\n",
        "\n",
        "    # ------ see how to define a global seed --------- and separate controllable seeds for reproducibility\n",
        "    # see how to do this for dataset_train and dataset_test\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # ---------- ADD TO DEVICE ---------- everywhere, wherever necessaary\n",
        "\n",
        "\n",
        "    #to device and seed for this ----\n",
        "    dataset_train = TabularDataset(csv_file=dataset_config.csv_file_train, y_column=dataset_config.y_column)\n",
        "    dataset_train = TabularDataset(csv_file=csv_file_train, y_column=y_column)\n",
        "    dataloader_train = DataLoader(dataset_train, batch_size=model_config.batch_size_train, shuffle=True)     # gives batch for training features and labels  (both in float 32)\n",
        "\n",
        "    dataset_test = TabularDataset(csv_file=dataset_config.csv_file_test, y_column=dataset_config.y_column)\n",
        "    dataloader_test = DataLoader(dataset_test, batch_size=model_config.batch_size_test, shuffle=True)       # gives batch for test features and label    (both in float 32)\n",
        "\n",
        "    dataset_pool = TabularDataset(csv_file=dataset_config.csv_file_pool, y_column=dataset_config.y_column)\n",
        "    pool_size = len(dataset_pool)\n",
        "    dataloader_pool = DataLoader(dataset_pool, batch_size=pool_size, shuffle=False)       # gives all the pool features and label   (both in float 32) - needed for input in NN_weights\n",
        "\n",
        "    dataset_pool_train = TabularDatasetPool(csv_file=dataset_config.csv_file_pool, y_column=dataset_config.y_column)\n",
        "    dataloader_pool_train = DataLoader(dataset_pool_train, batch_size=model_config.batch_size_train, shuffle=True)       # gives batch of the pool features and label   (both in float 32) - needed for updating the posterior of ENN - as we will do batchwise update\n",
        "\n",
        "\n",
        "\n",
        "    sample, label = dataset_train[0]\n",
        "    input_feature_size = sample.shape[0]       # Size of input features  ---- assuming 1D features\n",
        "\n",
        "    NN_weights = NN_feature_weights(input_feature_size, model_config.hidden_sizes_weight_NN, 1)\n",
        "    # --- TO INITIAL PARAMETRIZATION WITHIN  [0,1] , ALSO SET SEED ----------\n",
        "\n",
        "\n",
        "    meta_opt = optim.Adam(NN_weights.parameters(), lr=model_config.meta_opt_lr)       # meta_opt is optimizer for parameters of NN_weights\n",
        "\n",
        "    #seed for this\n",
        "    SubsetOperator = SubsetOperator(model_config.batch_size_query, model_config.temp_k_subset, False)\n",
        "\n",
        "    #seed for this\n",
        "    SubsetOperatortest = SubsetOperator(model_config.batch_size_query, model_config.temp_k_subset, True)\n",
        "\n",
        "\n",
        "    # to_device\n",
        "    ENN = basenet_with_learnable_epinet_and_ensemble_prior(input_feature_size, enn_config.basenet_hidden_sizes, model_config.n_classes, enn_config.exposed_layers, enn_config.z_dim, enn_config.learnable_epinet_hiddens, enn_config.hidden_sizes_prior, enn_config.seed_base, enn_config.seed_learnable_epinet, enn_config.seed_prior_epinet, enn_config.alpha)\n",
        "\n",
        "\n",
        "    loss_fn_init = nn.CrossEntropyLoss()\n",
        "    optimizer_init = optim.Adam(ENN.parameters(), lr=model_config.init_train_lr, weight_decay=model_config.init_train_weight_decay)\n",
        "    # ------- seed for this training\n",
        "    # ------- train ENN on initial training data  # save the state - ENN_initial_state  # define a separate optimizer for this # how to sample z's ---- separately for each batch\n",
        "    # ------- they also sampled the data each time and not a dataloader - kind of a bootstrap\n",
        "\n",
        "    for i in range(model_config.n_train_init):\n",
        "        ENN.train()\n",
        "        for (inputs, labels) in dataloader_train:\n",
        "            z = torch.randn(8)   #set seed for this  #set to_device for this\n",
        "            optimizer_init.zero_grad()\n",
        "            outputs = ENN(inputs,z)\n",
        "            loss = loss_fn_init(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer_init.step()\n",
        "\n",
        "\n",
        "\n",
        "    # Predictor =       # model for which we will evaluate recall   # load pretrained weights for the Predictor or train it\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for epoch in range(model_config.n_epoch):\n",
        "        train(train_config, dataloader_pool, dataloader_pool_train, dataloader_test, device, NN_weights, meta_opt, SubsetOperator, ENN, Predictor)\n",
        "\n",
        "    test(train_config, dataloader_pool, dataloader_pool_train, dataloader_test, device,  NN_weights, SubsetOperatortest, ENN, Predictor)\n",
        "\n"
      ],
      "metadata": {
        "id": "4LOirB_oib6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_config, dataloader_pool, dataloader_pool_train, dataloader_test, device, NN_weights, meta_opt, SubsetOperator, ENN, Predictor):\n",
        "  ENN.train()\n",
        "\n",
        "  for i in range(train_config.n_train_iter):    # Should we do this multiple times or not\n",
        "    start_time = time.time()\n",
        "    x_pool, y_pool = next(iter(dataloader_pool))\n",
        "    pool_weights = NN_weights(x_pool)   #pool_weights has shape [pool_size,1]\n",
        "    pool_weights_t = pool_weights.t()  #convert pool_weights to shape [1, pool_size]\n",
        "\n",
        "    #set seed\n",
        "    soft_k_vector = SubsetOperator(pool_weights_t)     #soft_k_vector has shape  [1,pool_size]\n",
        "    soft_k_vector_squeeze = soft_k_vector.squeeze()\n",
        "\n",
        "\n",
        "    z_pool = torch.randn(8) # set seed for z #set to device\n",
        "    x_pool_label_ENN_logits = ENN(x_pool,z_pool)  #use here complete dataset\n",
        "    x_pool_label_ENN_probabilities = F.softmax(logits, dim=1) #see if dim=1 is correct\n",
        "    x_pool_label_ENN_categorical = distributions.Categorical(x_pool_label_ENN_probabilities)\n",
        "    x_pool_label_ENN = x_pool_label_ENN_categorical.sample() # set seed for labels           # Do we need to take aeverages over multiple z's here? - No, do this for multiple z's\n",
        "\n",
        "\n",
        "\n",
        "    ENN_opt = torch.optim.Adam(ENN.parameters(), lr=train_config.ENN_opt_lr)\n",
        "\n",
        "                                                                              #copy_initial_weights - will be important if we are doing multisteps  # how to give initial training weights to ENN -- this is resolved , if we use same instance of the model everywhere - weights get stored\n",
        "    meta_opt.zero_grad()\n",
        "    with higher.innerloop_ctx(ENN, ENN_opt, copy_initial_weights=False) as (fnet, diffopt):\n",
        "\n",
        "      for _ in range(train_config.n_ENN_iter):\n",
        "\n",
        "        for (idx_batch, x_batch, label_batch) in dataloader_pool_train:\n",
        "\n",
        "          z_pool_train = torch.randn(8)\n",
        "\n",
        "          fnet_logits = fnet(x_batch, z_pool_train)    # Forward pass (outputs are logits) #DEFINE fnet sampler through fnet\n",
        "          batch_log_probs = F.log_softmax(fnet_logits, dim=1)     #see if here dim=1 is correct or not   # Apply log-softmax to get log probabilities\n",
        "\n",
        "\n",
        "          batch_weights = soft_k_vector_squeeze[idx_batch]        # Retrieve weights for the current batch\n",
        "          x_batch_label_ENN = x_pool_label_ENN[idx_batch]         # Retrieve labels for the current batch\n",
        "\n",
        "          # Calculate loss\n",
        "          ENN_loss = weighted_nll_loss(batch_log_probs,x_batch_label_ENN,batch_weights)       #expects log_probabilities as inputs    #CHECK WORKING OF THIS\n",
        "\n",
        "          diffopt.step(ENN_loss)\n",
        "\n",
        "      #derivative of fnet_parmaeters w.r.t NN (sampling policy) parameters is known - now we need derivative of var recall w.r.t fnet_parameters\n",
        "      meta_loss = var_recall_estimator(fnet, dataloader_test, Predictor)      #see where does this calculation for meta_loss happens that is it outside the innerloop_ctx or within it\n",
        "      meta_loss.backward()\n",
        "\n",
        "    meta_opt.step()\n",
        "    # log all important metrics and also save model configs\n"
      ],
      "metadata": {
        "id": "cwKo0rfYE_Fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(train_config, dataloader_pool, dataloader_pool_train, dataloader_test, device,  NN_weights, SubsetOperatortest, ENN, Predictor):\n",
        "\n",
        "  ENN.train()\n",
        "  x_pool,y_pool = = next(iter(dataloader_pool))                     #corect this with arguments if we needed\n",
        "  pool_weights = NN_weights(x_pool)   #pool_weights has shape [pool_size,1]\n",
        "  pool_weights_t = pool_weights.t()  #convert pool_weights to shape [1, pool_size]\n",
        "\n",
        "  #set seed\n",
        "  hard_k_vector = SubsetOperatortest(pool_weights_t)     #soft_k_vector has shape  [1,pool_size]\n",
        "  hard_k_vector_squeeze = soft_k_vector.squeeze()\n",
        "\n",
        "\n",
        "  ENN_opt = torch.optim.Adam(ENN.parameters(), lr=train_config.ENN_opt_lr)\n",
        "\n",
        "\n",
        "  with higher.innerloop_ctx(ENN, ENN_opt, track_higher_grads=False) as (fnet, diffopt):\n",
        "\n",
        "    for _ in range(train_config.n_ENN_iter):\n",
        "\n",
        "      for (idx_batch, x_batch, label_batch) in dataloader_pool_train:\n",
        "\n",
        "          z_pool_train = torch.randn(8)\n",
        "\n",
        "          fnet_logits = fnet(x_batch, z_pool_train)    # Forward pass (outputs are logits) #DEFINE fnet sampler through fnet\n",
        "          batch_log_probs = F.log_softmax(fnet_logits, dim=1)     #see if here dim=1 is correct or not   # Apply log-softmax to get log probabilities\n",
        "\n",
        "\n",
        "          batch_weights = hard_k_vector_squeeze[idx_batch]        # Retrieve weights for the current batch\n",
        "          y_batch = y_pool[idx_batch]         # Retrieve labels for the current batch\n",
        "\n",
        "          # Calculate loss\n",
        "          ENN_loss = weighted_nll_loss(batch_log_probs,y_batch,batch_weights)       #expects log_probabilities as inputs    #CHECK WORKING OF THIS\n",
        "\n",
        "          diffopt.step(ENN_loss)\n",
        "\n",
        "    meta_loss = var_recall_estimator(fnet, dataloader_test, Predictor).detach()\n",
        "    #see what does detach() do and if needed here\n",
        "\n",
        "\n",
        "  #log and print important things here"
      ],
      "metadata": {
        "id": "s0LOULRIaCkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "\n",
        "\n",
        "dataset_cfg = DatasetConfig(\"train.csv\", \"test.csv\", \"pool.csv\", \"y_col\")\n",
        "model_cfg = ModelConfig(batch_size_train = 64, batch_size_test = 64, batch_size_query = 100, temp_k_subset = 0.1, hidden_sizes_weight_NN = [50,50], meta_opt_lr = 0.001, n_classes = 2, n_epoch = 10, init_train_lr = 0.001, init_train_weight_decay = 0.1, n_train_init = 20)\n",
        "train_cfg = TrainConfig(n_train_iter = 15, n_ENN_iter = 15, ENN_opt_lr = 0.001)\n",
        "enn_cfg = ENNConfig(input_size = 2, basenet_hidden_sizes = [50,50], n_classes = 2, exposed_layers = [False, True], z_dim = 8, learnable_epinet_hiddens = [15,15], hidden_sizes_prior = [5,5], seed_base = 2, seed_learnable_epinet = 1, seed_prior_epinet = 0, alpha = 0.1)\n",
        "\n",
        "#Predictor = .......\n",
        "\n",
        "experiment(dataset_cfg, model_cfg, train_cfg, enn_cfg, Predictor)"
      ],
      "metadata": {
        "id": "agC1Sx3loZQv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}