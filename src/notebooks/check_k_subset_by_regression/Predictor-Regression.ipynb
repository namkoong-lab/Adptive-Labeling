{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d052b3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the model class\n",
    "class LinearRegressor(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegressor, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class LogisticRegression(torch.nn.Module):    \n",
    "    # build the constructor\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(input_size, output_size)\n",
    "    # make predictions\n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e93bef7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.3715963661670685\n",
      "Epoch 10, Loss: 0.3563101291656494\n",
      "Epoch 20, Loss: 0.34123674035072327\n",
      "Epoch 30, Loss: 0.3264453709125519\n",
      "Epoch 40, Loss: 0.3120012581348419\n",
      "Epoch 50, Loss: 0.297963410615921\n",
      "Epoch 60, Loss: 0.2843833565711975\n",
      "Epoch 70, Loss: 0.27130401134490967\n",
      "Epoch 80, Loss: 0.25875863432884216\n",
      "Epoch 90, Loss: 0.24677112698554993\n",
      "Epoch 100, Loss: 0.23535597324371338\n",
      "Epoch 110, Loss: 0.22451893985271454\n",
      "Epoch 120, Loss: 0.21425795555114746\n",
      "Epoch 130, Loss: 0.20456427335739136\n",
      "Epoch 140, Loss: 0.19542361795902252\n",
      "Epoch 150, Loss: 0.18681728839874268\n",
      "Epoch 160, Loss: 0.17872336506843567\n",
      "Epoch 170, Loss: 0.17111767828464508\n",
      "Epoch 180, Loss: 0.16397465765476227\n",
      "Epoch 190, Loss: 0.15726816654205322\n",
      "Epoch 200, Loss: 0.15097184479236603\n",
      "Epoch 210, Loss: 0.14505991339683533\n",
      "Epoch 220, Loss: 0.1395072489976883\n",
      "Epoch 230, Loss: 0.1342896968126297\n",
      "Epoch 240, Loss: 0.1293843686580658\n",
      "Epoch 250, Loss: 0.12476955354213715\n",
      "Epoch 260, Loss: 0.12042485922574997\n",
      "Epoch 270, Loss: 0.11633121222257614\n",
      "Epoch 280, Loss: 0.11247086524963379\n",
      "Epoch 290, Loss: 0.10882732272148132\n",
      "Epoch 300, Loss: 0.10538527369499207\n",
      "Epoch 310, Loss: 0.10213060677051544\n",
      "Epoch 320, Loss: 0.09905019402503967\n",
      "Epoch 330, Loss: 0.09613195061683655\n",
      "Epoch 340, Loss: 0.09336477518081665\n",
      "Epoch 350, Loss: 0.09073840826749802\n",
      "Epoch 360, Loss: 0.08824334293603897\n",
      "Epoch 370, Loss: 0.08587083965539932\n",
      "Epoch 380, Loss: 0.08361285924911499\n",
      "Epoch 390, Loss: 0.08146195858716965\n",
      "Epoch 400, Loss: 0.07941126078367233\n",
      "Epoch 410, Loss: 0.07745443284511566\n",
      "Epoch 420, Loss: 0.07558563351631165\n",
      "Epoch 430, Loss: 0.07379943877458572\n",
      "Epoch 440, Loss: 0.07209081947803497\n",
      "Epoch 450, Loss: 0.0704551488161087\n",
      "Epoch 460, Loss: 0.0688881054520607\n",
      "Epoch 470, Loss: 0.0673857182264328\n",
      "Epoch 480, Loss: 0.06594426929950714\n",
      "Epoch 490, Loss: 0.06456030905246735\n",
      "Epoch 500, Loss: 0.06323064863681793\n",
      "Epoch 510, Loss: 0.061952292919158936\n",
      "Epoch 520, Loss: 0.060722459107637405\n",
      "Epoch 530, Loss: 0.0595385804772377\n",
      "Epoch 540, Loss: 0.05839822068810463\n",
      "Epoch 550, Loss: 0.05729912221431732\n",
      "Epoch 560, Loss: 0.056239183992147446\n",
      "Epoch 570, Loss: 0.055216435343027115\n",
      "Epoch 580, Loss: 0.0542290098965168\n",
      "Epoch 590, Loss: 0.05327516421675682\n",
      "Epoch 600, Loss: 0.05235330015420914\n",
      "Epoch 610, Loss: 0.051461879163980484\n",
      "Epoch 620, Loss: 0.05059947445988655\n",
      "Epoch 630, Loss: 0.0497647225856781\n",
      "Epoch 640, Loss: 0.04895637184381485\n",
      "Epoch 650, Loss: 0.0481732152402401\n",
      "Epoch 660, Loss: 0.0474141500890255\n",
      "Epoch 670, Loss: 0.0466780923306942\n",
      "Epoch 680, Loss: 0.045964036136865616\n",
      "Epoch 690, Loss: 0.04527105763554573\n",
      "Epoch 700, Loss: 0.04459824413061142\n",
      "Epoch 710, Loss: 0.04394477233290672\n",
      "Epoch 720, Loss: 0.043309807777404785\n",
      "Epoch 730, Loss: 0.04269262030720711\n",
      "Epoch 740, Loss: 0.04209248721599579\n",
      "Epoch 750, Loss: 0.04150873050093651\n",
      "Epoch 760, Loss: 0.040940698236227036\n",
      "Epoch 770, Loss: 0.04038777947425842\n",
      "Epoch 780, Loss: 0.03984940052032471\n",
      "Epoch 790, Loss: 0.03932499885559082\n",
      "Epoch 800, Loss: 0.038814060389995575\n",
      "Epoch 810, Loss: 0.03831607103347778\n",
      "Epoch 820, Loss: 0.037830550223588943\n",
      "Epoch 830, Loss: 0.03735707327723503\n",
      "Epoch 840, Loss: 0.036895159631967545\n",
      "Epoch 850, Loss: 0.03644444793462753\n",
      "Epoch 860, Loss: 0.03600451350212097\n",
      "Epoch 870, Loss: 0.035574983805418015\n",
      "Epoch 880, Loss: 0.03515548259019852\n",
      "Epoch 890, Loss: 0.034745730459690094\n",
      "Epoch 900, Loss: 0.0343453511595726\n",
      "Epoch 910, Loss: 0.03395404666662216\n",
      "Epoch 920, Loss: 0.033571500331163406\n",
      "Epoch 930, Loss: 0.03319744020700455\n",
      "Epoch 940, Loss: 0.032831594347953796\n",
      "Epoch 950, Loss: 0.032473694533109665\n",
      "Epoch 960, Loss: 0.03212348744273186\n",
      "Epoch 970, Loss: 0.03178073838353157\n",
      "Epoch 980, Loss: 0.03144519403576851\n",
      "Epoch 990, Loss: 0.03111664578318596\n"
     ]
    }
   ],
   "source": [
    "# Example Data (Replace with your actual data)\n",
    "# X_train: tensor of shape [n_samples, n_features]\n",
    "# y_train: tensor of shape [n_samples, 1]\n",
    "#data generated from https://github.com/dakshmittal30/Adaptive_sampling/blob/7cf3996c786ce33db90fcb7aef8584054169557c/src/notebooks/Selection_bias.ipynb\n",
    "\n",
    "\n",
    "#below is biased training data\n",
    "\n",
    "directory = '/shared/share_mala/yuanzhe/adaptive_sampling/pipeline_datasets/'\n",
    "train_csv_name = directory + '/biased_new/input_dim_1_train_init_data_mean_0.0ln_1.0sig_0.1no.2000_random_prop_score_selected_2_16.0__.csv'\n",
    "test_csv_name = directory + 'input_dim_1_test_final_data_mean_0.0ln_1.0sig_0.1no.2000.csv'\n",
    "pool_csv_name = directory + 'input_dim_1_pool_data_mean_0.0ln_1.0sig_0.1no.2000.csv'\n",
    "\n",
    "file_list = [train_csv_name, test_csv_name, pool_csv_name]\n",
    "name_list = ['train','test','pool']\n",
    "\n",
    "df = pd.read_csv(train_csv_name)\n",
    "X_train = np.array(df[['Column0']])\n",
    "y_train = np.array(df[['EVENT_LABEL']])  \n",
    "#y_train = y_train >0 #convert regression into classifier\n",
    "\n",
    "# Convert data to PyTorch tensors if they aren't already\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 1000  # Number of training iterations\n",
    "learning_rate = 0.01\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Model, Loss and Optimizer\n",
    "model = LogisticRegression(input_size=X_train.shape[1], output_size=1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass: Compute predicted y by passing X to the model\n",
    "    y_pred = model(X_train)\n",
    "    \n",
    "    #print test/ pool loss \n",
    "    #print X/Y train/ test/ pool\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    if epoch % 10 == 0:  # Print every 10th epoch\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "#torch.save(model, url + 'predictor.pkl')\n",
    "\n",
    "model_scripted = torch.jit.script(model) # Export to TorchScript\n",
    "model_scripted.save(url + 'predictor_0214_regression.pt') # Save\n",
    "\n",
    "#https://stackoverflow.com/questions/55488795/unpickling-saved-pytorch-model-throws-attributeerror-cant-get-attribute-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adb03743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l2_loss train 0.0307949036359787\n",
      "l2_loss test 0.07647431641817093\n",
      "l2_loss pool 0.08316905051469803\n"
     ]
    }
   ],
   "source": [
    "##TBD\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def l2_loss(x_test, y_test, model, device): #input is dataloader_test and classifier/ model c, output is true recall given labels\n",
    " \n",
    "    prediction_list = model(x_test)\n",
    "    res = torch.square(torch.subtract(prediction_list, y_test))\n",
    "    return torch.mean(res)\n",
    "\n",
    "\n",
    "def compute_l2_loss(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    X_test = np.array(df[['Column0']])\n",
    "    y_test = np.array(df[['EVENT_LABEL']]) \n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "    \n",
    "    return float(l2_loss(X_test,y_test,model,device))\n",
    "    \n",
    "for i,f in enumerate(file_list):\n",
    "    print('l2_loss', name_list[i], compute_l2_loss(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2374173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert reg to classification, no need to run again\n",
    "# url = '/shared/share_mala/yuanzhe/adaptive_sampling/pipeline_datasets/'\n",
    "# train_csv_name = 'input_dim_1_train_init_data_mean_0.0ln_1.0sig_0.1no.2000.csv'\n",
    "# test_csv_name = 'input_dim_1_test_final_data_mean_0.0ln_1.0sig_0.1no.2000.csv'\n",
    "# pool_csv_name = 'input_dim_1_pool_data_mean_0.0ln_1.0sig_0.1no.2000.csv'\n",
    "# file_list  = [train_csv_name, test_csv_name, pool_csv_name]\n",
    "# #convert y into 0/1\n",
    "\n",
    "# for f in file_list:\n",
    "#     df = pd.read_csv(url + f)\n",
    "#     df['EVENT_LABEL'] = df['EVENT_LABEL'] > 0\n",
    "#     df.to_csv(url+'classifier_'+ f, index = False)\n",
    "# #/user/ym2865/Adaptive Sampling/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52293d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,f in enumerate(file_list):\n",
    "#     df = pd.read_csv(f)\n",
    "#     plt.title(name_list[i]+'_distributions')\n",
    "#     sns.histplot(df, x=\"Column0\", hue=\"EVENT_LABEL\", element=\"poly\")\n",
    "\n",
    "#     plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07bbd29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yuanzhe_new",
   "language": "python",
   "name": "yuanzhe_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
