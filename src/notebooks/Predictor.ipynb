{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d052b3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Define the model class\n",
    "class LinearRegressor(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegressor, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class LogisticRegression(torch.nn.Module):    \n",
    "    # build the constructor\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(input_size, output_size)\n",
    "    # make predictions\n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e93bef7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.0371397733688354\n",
      "Epoch 10, Loss: 1.005934476852417\n",
      "Epoch 20, Loss: 0.9767131805419922\n",
      "Epoch 30, Loss: 0.9494389295578003\n",
      "Epoch 40, Loss: 0.9240630269050598\n",
      "Epoch 50, Loss: 0.900526762008667\n",
      "Epoch 60, Loss: 0.8787612318992615\n",
      "Epoch 70, Loss: 0.858690619468689\n",
      "Epoch 80, Loss: 0.8402321338653564\n",
      "Epoch 90, Loss: 0.8232988715171814\n",
      "Epoch 100, Loss: 0.8078004121780396\n",
      "Epoch 110, Loss: 0.793645441532135\n",
      "Epoch 120, Loss: 0.7807420492172241\n",
      "Epoch 130, Loss: 0.7689998149871826\n",
      "Epoch 140, Loss: 0.7583305835723877\n",
      "Epoch 150, Loss: 0.7486493587493896\n",
      "Epoch 160, Loss: 0.7398746013641357\n",
      "Epoch 170, Loss: 0.7319294810295105\n",
      "Epoch 180, Loss: 0.7247414588928223\n",
      "Epoch 190, Loss: 0.7182427644729614\n"
     ]
    }
   ],
   "source": [
    "# Example Data (Replace with your actual data)\n",
    "# X_train: tensor of shape [n_samples, n_features]\n",
    "# y_train: tensor of shape [n_samples, 1]\n",
    "#data generated from https://github.com/dakshmittal30/Adaptive_sampling/blob/7cf3996c786ce33db90fcb7aef8584054169557c/src/notebooks/Selection_bias.ipynb\n",
    "\n",
    "\n",
    "url = '/shared/share_mala/yuanzhe/adaptive_sampling/pipeline_datasets/'\n",
    "csv_file = 'input_dim_1_train_init_data_mean_0.0ln_1.0sig_0.1no.2000.csv'\n",
    "#below is biased training data\n",
    "csv_file =  '/biased_new/classifier_input_dim_1_train_init_data_mean_0.0ln_1.0sig_0.1no.2000_random_prop_score_selected_2_16.0__.csv'\n",
    "df = pd.read_csv(url + csv_file)\n",
    "X_train = np.array(df[['Column0']])\n",
    "y_train = np.array(df[['EVENT_LABEL']])  \n",
    "y_train = y_train >0 #convert regression into classifier\n",
    "\n",
    "# Convert data to PyTorch tensors if they aren't already\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 200  # Number of training iterations\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Model, Loss and Optimizer\n",
    "model = LogisticRegression(input_size=X_train.shape[1], output_size=1)\n",
    "criterion = nn.MSELoss()\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass: Compute predicted y by passing X to the model\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    if epoch % 10 == 0:  # Print every 10th epoch\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "#torch.save(model, url + 'predictor.pkl')\n",
    "\n",
    "model_scripted = torch.jit.script(model) # Export to TorchScript\n",
    "model_scripted.save(url + 'predictor_0206.pt') # Save\n",
    "\n",
    "#https://stackoverflow.com/questions/55488795/unpickling-saved-pytorch-model-throws-attributeerror-cant-get-attribute-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adb03743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall on training data tensor(0.9780)\n",
      "Recall on test data tensor(0.4907)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def Recall_True(x_test, y_test, model, device): #input is dataloader_test and classifier/ model c, output is true recall given labels\n",
    " \n",
    "    prediction_list = model(x_test)\n",
    "    predicted_class = torch.argmax(prediction_list)\n",
    "    predicted_class = prediction_list >= 0.5 #may need to use the previous code if model predicts probs of two classes\n",
    "\n",
    "    x = torch.sum(torch.mul(y_test, predicted_class))\n",
    "    y = torch.sum(y_test)\n",
    "    return x/y\n",
    "\n",
    "test_csv_name = url + 'classifier_input_dim_1_test_final_data_mean_0.0ln_1.0sig_0.1no.2000.csv'\n",
    "df_test = pd.read_csv(test_csv_name)\n",
    "X_test = np.array(df_test[['Column0']])\n",
    "y_test = np.array(df_test[['EVENT_LABEL']]) \n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "print('Recall on training data', Recall_True(X_train,y_train,model,device))\n",
    "print('Recall on test data', Recall_True(X_test,y_test,model,device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2374173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert reg to classification, no need to run again\n",
    "# url = '/shared/share_mala/yuanzhe/adaptive_sampling/pipeline_datasets/'\n",
    "# train_csv_name = 'input_dim_1_train_init_data_mean_0.0ln_1.0sig_0.1no.2000.csv'\n",
    "# test_csv_name = 'input_dim_1_test_final_data_mean_0.0ln_1.0sig_0.1no.2000.csv'\n",
    "# pool_csv_name = 'input_dim_1_pool_data_mean_0.0ln_1.0sig_0.1no.2000.csv'\n",
    "# file_list  = [train_csv_name, test_csv_name, pool_csv_name]\n",
    "# #convert y into 0/1\n",
    "\n",
    "# for f in file_list:\n",
    "#     df = pd.read_csv(url + f)\n",
    "#     df['EVENT_LABEL'] = df['EVENT_LABEL'] > 0\n",
    "#     df.to_csv(url+'classifier_'+ f, index = False)\n",
    "# #/user/ym2865/Adaptive Sampling/src"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yuanzhe_new",
   "language": "python",
   "name": "yuanzhe_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
