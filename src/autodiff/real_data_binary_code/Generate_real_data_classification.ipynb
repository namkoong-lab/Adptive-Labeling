{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1594b1e",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa872f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFor real data -\\nSelect 10 features - use the old file  — code we have - we need to change the code somewhat\\nChange the setting to a continuous one - use the \\nForm the clusters - how many? - 51 clusters\\nReduce the dataset size -  use the old file — on fly \\n\\nTrain points - 100 - 500\\nPool points - 1 cluster - 250 points |||||  50 clusters 5 points each \\nNumber of clusters - 1+50\\nBatch size to be acquired  - 5\\nHorizons - 5 horizons\\n\\nMarginal distribution of x is not same as given dataset\\nTest points - 1 cluster - 20 points\\n                     25 clusters - 20 points\\n                       25 clusters  - 2 points\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from line_profiler import LineProfiler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.stats import percentileofscore\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotnine as gg\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "#from dataloader import TabularDataset\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import higher\n",
    "\n",
    "from torch import nn\n",
    "# from acme.utils.loggers.terminal import TerminalLogger\n",
    "import dataclasses\n",
    "#import chex\n",
    "#import haiku as hk\n",
    "#import jax\n",
    "#import jax.numpy as jnp\n",
    "#import optax\n",
    "import pandas as pd\n",
    "#import warnings\n",
    "import gpytorch\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "#warnings.filterwarnings('ignore')\n",
    "#import pipeline_var_l2_loss\n",
    "import seaborn as sns\n",
    "#from dataloader import TabularDataset\n",
    "#from var_l2_loss_estimator import *\n",
    "#from ENN import basenet_with_learnable_epinet_and_ensemble_prior\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#wandb.init()\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import torch.nn.init as init\n",
    "from datetime import datetime\n",
    " \n",
    "#data from https://drive.google.com/drive/u/1/folders/1WuXIzpYLrLNH6pn9zBMx6oRCCQPRz0F1\n",
    "\n",
    "directory = '/shared/share_mala/data/eicu_train_test/'\n",
    "train_csv = 'eicu_train_final.csv'\n",
    "test_csv = 'eicu_test_final.csv'\n",
    "\n",
    "df_train = pd.read_csv(directory + train_csv)\n",
    "df_test = pd.read_csv(directory + test_csv)\n",
    "df = pd.concat([df_train, df_test], axis = 0)\n",
    "X_col = list(df.columns)[:-1]\n",
    "Y = 'EVENT_LABEL'\n",
    "\n",
    "\n",
    "#df = df.groupby('EVENT_LABEL', group_keys=False).apply(lambda x: x.sample(2000))\n",
    "Y_data = np.array(df[Y])\n",
    "\n",
    "'''\n",
    "For real data -\n",
    "Select 10 features - use the old file  — code we have - we need to change the code somewhat\n",
    "Change the setting to a continuous one - use the \n",
    "Form the clusters - how many? - 51 clusters\n",
    "Reduce the dataset size -  use the old file — on fly \n",
    "\n",
    "Train points - 100 - 500\n",
    "Pool points - 1 cluster - 250 points |||||  50 clusters 5 points each \n",
    "Number of clusters - 1+50\n",
    "Batch size to be acquired  - 5\n",
    "Horizons - 5 horizons\n",
    "\n",
    "Marginal distribution of x is not same as given dataset\n",
    "Test points - 1 cluster - 20 points\n",
    "                     25 clusters - 20 points\n",
    "                       25 clusters  - 2 points\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19bcf88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(\n",
    "n_estimators=100,  # Number of trees in the forest\n",
    "criterion='gini',  # Function to measure the quality of a split. Can also be 'entropy'.\n",
    "max_depth=None,    # Maximum depth of the tree. If None, nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "min_samples_split=2,  # Minimum number of samples required to split an internal node\n",
    "min_samples_leaf=1,   # Minimum number of samples required to be at a leaf node\n",
    "bootstrap=True,       # Whether bootstrap samples are used when building trees\n",
    "oob_score=False,      # Whether to use out-of-bag samples to estimate the generalization score\n",
    "random_state=None,    # Controls both the randomness of the bootstrapping and the sampling of features to consider when looking for the best split\n",
    "verbose=0,            # Controls the verbosity of the process\n",
    "class_weight=None,    # Weights associated with classes. Can be 'balanced'.\n",
    ")\n",
    "    \n",
    "def get_feat_importance(model, X_col): # get most important features, input is a randomforest classifier/regressor\n",
    "    importances = model.feature_importances_\n",
    "    forest_importances = pd.Series(importances, index = X_col)\n",
    "    forest_importances = forest_importances.nlargest(10)  #only keep most important 10 feature\n",
    "    feature_importances = list(forest_importances.index)\n",
    "    return feature_importances #return the columns   \n",
    "\n",
    "\n",
    "Y_train = df['EVENT_LABEL']\n",
    "X_col = list(df.columns)\n",
    "X_col.remove('EVENT_LABEL')\n",
    "X_train = df[X_col]\n",
    "\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "X_col = list(X_train.columns)\n",
    "X_col_important = get_feat_importance(model, X_col)\n",
    "# X_col_important.append('EVENT_LABEL')\n",
    "\n",
    "X_train_imp = df[X_col_important]\n",
    "\n",
    "Y_pred = model.predict_proba(X_train)\n",
    "Y_pred = Y_pred[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a88cffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/ym2865/.conda/envs/yuanzhe_new/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            mean  count\n",
      "labels                 \n",
      "0       0.080972   3458\n",
      "1       0.051369   2959\n",
      "2       0.275904    830\n",
      "3       0.032513   5198\n",
      "4       0.232380   1433\n",
      "5       0.044001   2909\n",
      "6       0.679825    456\n",
      "7       0.096234    239\n",
      "8       0.050847   2360\n",
      "9       0.273519    574\n",
      "10      0.067797    354\n",
      "11      0.071653   1591\n",
      "12      0.313167   1124\n",
      "13      0.231447    795\n",
      "14      0.142857      7\n",
      "15      0.016506   6967\n",
      "16      0.027959   2754\n",
      "17      0.187726    277\n",
      "18      0.269113    654\n",
      "19      0.051419   4687\n",
      "20      0.854286    350\n",
      "21      0.097585   1035\n",
      "22      0.056099   1533\n",
      "23      0.156275   1235\n",
      "24      0.263208   1060\n",
      "25      0.248982   1474\n",
      "26      0.057462   3933\n",
      "27      0.414634    205\n",
      "28      0.108501   2341\n",
      "29      0.201152   2257\n"
     ]
    }
   ],
   "source": [
    "# Normalize data\n",
    "n_cluster = 30\n",
    "data = np.array(X_train_imp)\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(data)\n",
    "kmeans = KMeans(n_clusters = n_cluster, random_state = 1)\n",
    "kmeans.fit(data_normalized)\n",
    "\n",
    "labels = kmeans.predict(data_normalized)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame({'labels': labels, 'Y': Y_train})\n",
    "\n",
    "# Calculate mean of Y for each unique value in X\n",
    "result = data.groupby('labels')['Y'].agg(['mean', 'count'])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fce84b",
   "metadata": {},
   "source": [
    "# Select points from clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83cbf5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 40 110\n"
     ]
    }
   ],
   "source": [
    "\n",
    "good_cluster_ind = 27\n",
    "usual_cluster_ind = 3\n",
    "\n",
    "\n",
    "\n",
    "n = df.shape[0]\n",
    "labels = kmeans.labels_\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "\n",
    "\n",
    "def close_point_cluster(data, labels, centers, num_closest_points, desired_cluster_index):\n",
    "    # Index of the desired cluster center\n",
    "    X = data[labels == desired_cluster_index]\n",
    "\n",
    "    desired_cluster_index = desired_cluster_index  # Change this to the desired cluster index\n",
    "\n",
    "    # Calculate distances from each point to the desired cluster center\n",
    "    distances_to_desired_center = np.linalg.norm(X - centers[desired_cluster_index], axis=1)\n",
    "\n",
    "    # Sort the distances and select the indices of the 10 closest points\n",
    "    closest_indices = np.argsort(distances_to_desired_center)[:num_closest_points]\n",
    "\n",
    "\n",
    "    return closest_indices\n",
    "\n",
    "def random_partition_list(lst, sizes):\n",
    "    # Shuffle the list randomly\n",
    "    random.shuffle(lst)\n",
    "    \n",
    "    # Partition the shuffled list into three parts with the specified sizes\n",
    "    partitions = [lst[start:start+size] for start, size in zip([0, sizes[0], sizes[0] + sizes[1]], sizes)]\n",
    "    \n",
    "    return partitions\n",
    "\n",
    "'''\n",
    "ENN/Ensemble:\n",
    "   2 clusters:\n",
    "100 training points from 1 cluster\n",
    "100 pool points from this useless cluster and 10 pool points from the other cluster\n",
    "1 or 2 points - batch size\n",
    "Test set - 20 points each \n",
    "Graphs: The setting (appendix), just compare it with random baselines, consistency results for multi clusters\n",
    "Challenge: ENN - consistency is issue, Ensemble - slow (parallelization is necessary)\n",
    "'''\n",
    "\n",
    "good_cluster_indices = close_point_cluster(data_normalized, labels, centers, num_closest_points = 10 + 20, desired_cluster_index = good_cluster_ind)\n",
    "usual_cluster_indices = close_point_cluster(data_normalized, labels, centers, num_closest_points = 100 +  100 + 20, desired_cluster_index = usual_cluster_ind)\n",
    "\n",
    "[pool_indices_0, test_indices_0] = random_partition_list(good_cluster_indices, sizes = [10,20])\n",
    "[train_indices, pool_indices_1, test_indices_1] = random_partition_list(usual_cluster_indices, sizes = [100, 100,20])\n",
    "\n",
    " # #train_indices\n",
    "pool_indices_all = np.concatenate((pool_indices_0, pool_indices_1))\n",
    "test_indices_all = np.concatenate((test_indices_0, test_indices_1)) \n",
    "\n",
    "print(len(train_indices), len(test_indices_all), len(pool_indices_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49127079",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/shared/share_mala/data/eicu_train_test/0520_data_binary/'\n",
    "\n",
    "index_list = [train_indices, pool_indices_all, test_indices_all]\n",
    "name_list = ['train','pool','test']\n",
    "\n",
    "for i in range(3):\n",
    "    pd.DataFrame(data_normalized[index_list[i]]).to_csv(data_dir + name_list[i] + '_x.csv')\n",
    "    pd.DataFrame(Y_train[index_list[i]]).to_csv(data_dir + name_list[i] + '_y.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yuanzhe_new",
   "language": "python",
   "name": "yuanzhe_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
