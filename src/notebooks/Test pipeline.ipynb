{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cff566a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-27 02:13:23.471278: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/opt/gurobi/linux64/lib:/opt/gurobi/linux64/lib\n"
     ]
    }
   ],
   "source": [
    "import neural_testbed\n",
    "from neural_testbed.agents import factories as agent_factories\n",
    "from neural_testbed.agents.factories.sweeps import testbed_2d as agent_sweeps\n",
    "from neural_testbed import base\n",
    "from neural_testbed import generative\n",
    "from neural_testbed import leaderboard\n",
    "from typing import Callable, NamedTuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotnine as gg\n",
    "import torch\n",
    "# from acme.utils.loggers.terminal import TerminalLogger\n",
    "import dataclasses\n",
    "import chex\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "df57f345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubsetOperator()\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SubsetOperator' object has no attribute 'squeeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m model_predictor\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menn_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_predictor\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[42], line 205\u001b[0m, in \u001b[0;36mexperiment\u001b[0;34m(dataset_config, model_config, train_config, enn_config, Predictor)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m#initial training completed\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Predictor =       # model for which we will evaluate recall   # load pretrained weights for the Predictor or train it\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(model_config\u001b[38;5;241m.\u001b[39mn_epoch):\n\u001b[0;32m--> 205\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_pool_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNN_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSubsetOperator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mENN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPredictor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m test(train_config, dataloader_pool, dataloader_pool_train, dataloader_test, device,  NN_weights, SubsetOperatortest, ENN, Predictor)\n",
      "Cell \u001b[0;32mIn[42], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_config, dataloader_pool, dataloader_pool_train, dataloader_test, device, NN_weights, meta_opt, SubsetOperator, ENN, Predictor)\u001b[0m\n\u001b[1;32m     11\u001b[0m soft_k_vector \u001b[38;5;241m=\u001b[39m k_subset_sampling\u001b[38;5;241m.\u001b[39mSubsetOperator(pool_weights_t)     \u001b[38;5;66;03m#soft_k_vector has shape  [1,pool_size]\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(soft_k_vector)\n\u001b[0;32m---> 13\u001b[0m soft_k_vector_squeeze \u001b[38;5;241m=\u001b[39m \u001b[43msoft_k_vector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m()\n\u001b[1;32m     16\u001b[0m z_pool \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m8\u001b[39m) \u001b[38;5;66;03m# set seed for z #set to device\u001b[39;00m\n\u001b[1;32m     17\u001b[0m x_pool_label_ENN_logits \u001b[38;5;241m=\u001b[39m ENN(x_pool,z_pool)  \u001b[38;5;66;03m#use here complete dataset\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/yuanzhe_new/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SubsetOperator' object has no attribute 'squeeze'"
     ]
    }
   ],
   "source": [
    "url = '/shared/share_mala/yuanzhe/adaptive_sampling/pipeline_datasets/'\n",
    "train_csv_name = url + 'classifier_input_dim_1_train_init_data_mean_0.0ln_1.0sig_0.1no.2000.csv'\n",
    "test_csv_name = url + 'classifier_input_dim_1_test_final_data_mean_0.0ln_1.0sig_0.1no.2000.csv'\n",
    "pool_csv_name = url + 'classifier_input_dim_1_pool_data_mean_0.0ln_1.0sig_0.1no.2000.csv'\n",
    "\n",
    "dataset_cfg = pipeline.DatasetConfig(train_csv_name, test_csv_name, pool_csv_name, \"EVENT_LABEL\")\n",
    "model_cfg = pipeline.ModelConfig(batch_size_train = 64, batch_size_test = 64, batch_size_query = 100, temp_k_subset = 0.1, hidden_sizes_weight_NN = [50,50], meta_opt_lr = 0.001, n_classes = 2, n_epoch = 10, init_train_lr = 0.001, init_train_weight_decay = 0.1, n_train_init = 20)\n",
    "train_cfg = pipeline.TrainConfig(n_train_iter = 15, n_ENN_iter = 15, ENN_opt_lr = 0.001)\n",
    "enn_cfg = pipeline.ENNConfig(basenet_hidden_sizes = [50,50],  exposed_layers = [False, True], z_dim = 8, learnable_epinet_hiddens = [15,15], hidden_sizes_prior = [5,5], seed_base = 2, seed_learnable_epinet = 1, seed_prior_epinet = 0, alpha = 0.1)\n",
    "# model_predictor = torch.load(url + 'predictor.pkl')\n",
    "# model_predictor.eval()\n",
    "\n",
    "\n",
    "model_predictor = torch.jit.load(url + 'predictor.pt')\n",
    "model_predictor.eval()\n",
    "\n",
    "# Example usage\n",
    "\n",
    "experiment(dataset_cfg, model_cfg, train_cfg, enn_cfg, model_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a7d7fb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_config, dataloader_pool, dataloader_pool_train, dataloader_test, device, NN_weights, meta_opt, SubsetOperator, ENN, Predictor):\n",
    "  ENN.train()\n",
    "\n",
    "  for i in range(train_config.n_train_iter):    # Should we do this multiple times or not\n",
    "    start_time = time.time()\n",
    "    x_pool, y_pool = next(iter(dataloader_pool))\n",
    "    pool_weights = NN_weights(x_pool)   #pool_weights has shape [pool_size,1]\n",
    "    pool_weights_t = pool_weights.t()  #convert pool_weights to shape [1, pool_size]\n",
    "\n",
    "    #set seed\n",
    "    soft_k_vector = k_subset_sampling.SubsetOperator(pool_weights_t)     #soft_k_vector has shape  [1,pool_size]\n",
    "    print(soft_k_vector)\n",
    "    soft_k_vector_squeeze = soft_k_vector.squeeze()\n",
    "\n",
    "\n",
    "    z_pool = torch.randn(8) # set seed for z #set to device\n",
    "    x_pool_label_ENN_logits = ENN(x_pool,z_pool)  #use here complete dataset\n",
    "    x_pool_label_ENN_probabilities = F.softmax(logits, dim=1) #see if dim=1 is correct\n",
    "    x_pool_label_ENN_categorical = distributions.Categorical(x_pool_label_ENN_probabilities)\n",
    "    x_pool_label_ENN = x_pool_label_ENN_categorical.sample() # set seed for labels           # Do we need to take aeverages over multiple z's here? - No, do this for multiple z's\n",
    "\n",
    "\n",
    "\n",
    "    ENN_opt = torch.optim.Adam(ENN.parameters(), lr=train_config.ENN_opt_lr)\n",
    "\n",
    "                                                                              #copy_initial_weights - will be important if we are doing multisteps  # how to give initial training weights to ENN -- this is resolved , if we use same instance of the model everywhere - weights get stored\n",
    "    meta_opt.zero_grad()\n",
    "    with higher.innerloop_ctx(ENN, ENN_opt, copy_initial_weights=False) as (fnet, diffopt):\n",
    "\n",
    "      for _ in range(train_config.n_ENN_iter):\n",
    "\n",
    "        for (idx_batch, x_batch, label_batch) in dataloader_pool_train:\n",
    "\n",
    "          z_pool_train = torch.randn(8)\n",
    "\n",
    "          fnet_logits = fnet(x_batch, z_pool_train)    # Forward pass (outputs are logits) #DEFINE fnet sampler through fnet\n",
    "          batch_log_probs = F.log_softmax(fnet_logits, dim=1)     #see if here dim=1 is correct or not   # Apply log-softmax to get log probabilities\n",
    "\n",
    "\n",
    "          batch_weights = soft_k_vector_squeeze[idx_batch]        # Retrieve weights for the current batch\n",
    "          x_batch_label_ENN = x_pool_label_ENN[idx_batch]         # Retrieve labels for the current batch\n",
    "\n",
    "          # Calculate loss\n",
    "          ENN_loss = weighted_nll_loss(batch_log_probs,x_batch_label_ENN,batch_weights)       #expects log_probabilities as inputs    #CHECK WORKING OF THIS\n",
    "\n",
    "          diffopt.step(ENN_loss)\n",
    "\n",
    "      #derivative of fnet_parmaeters w.r.t NN (sampling policy) parameters is known - now we need derivative of var recall w.r.t fnet_parameters\n",
    "      meta_loss = var_recall_estimator(fnet, dataloader_test, Predictor)      #see where does this calculation for meta_loss happens that is it outside the innerloop_ctx or within it\n",
    "      meta_loss.backward()\n",
    "\n",
    "    meta_opt.step()\n",
    "    \n",
    "    \n",
    "import argparse\n",
    "import typing\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.distributions as distributions\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import higher\n",
    "\n",
    "from Dataloader import TabularDataset\n",
    "from Dataloader import TabularDatasetPool\n",
    "\n",
    "import k_subset_sampling #import SubsetOperator\n",
    "from nn_feature_weights import NN_feature_weights\n",
    "from ENN import basenet_with_learnable_epinet_and_ensemble_prior\n",
    "\n",
    "\n",
    "from enn_loss_func import weighted_nll_loss\n",
    "\n",
    "\n",
    "from var_recall_estimator import var_recall_estimator    #Yuanzhe\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    csv_file_train: str\n",
    "    csv_file_test: str\n",
    "    csv_file_pool: str\n",
    "    y_column: str  # Assuming same column name across above 3 sets\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    batch_size_train: int\n",
    "    batch_size_test: int\n",
    "    batch_size_query: int\n",
    "    temp_k_subset: float\n",
    "    hidden_sizes_weight_NN: list\n",
    "    meta_opt_lr: float\n",
    "    n_classes: int\n",
    "    n_epoch: int\n",
    "    init_train_lr: float\n",
    "    init_train_weight_decay: float\n",
    "    n_train_init: int\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    n_train_iter: int\n",
    "    n_ENN_iter: int\n",
    "    ENN_opt_lr: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ENNConfig:\n",
    "    basenet_hidden_sizes: list\n",
    "    exposed_layers: list\n",
    "    z_dim: int\n",
    "    learnable_epinet_hiddens: list\n",
    "    hidden_sizes_prior: list\n",
    "    seed_base: int\n",
    "    seed_learnable_epinet: int\n",
    "    seed_prior_epinet: int\n",
    "    alpha: float\n",
    "        \n",
    "def experiment(dataset_config: DatasetConfig, model_config: ModelConfig, train_config: TrainConfig, enn_config: ENNConfig, Predictor):\n",
    "\n",
    "\n",
    "    # Predictor here has already been pretrained\n",
    "\n",
    "\n",
    "    # ------ see how to define a global seed --------- and separate controllable seeds for reproducibility\n",
    "    # see how to do this for dataset_train and dataset_test\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # ---------- ADD TO DEVICE ---------- everywhere, wherever necessaary\n",
    "\n",
    "\n",
    "    #to device and seed for this ----\n",
    "    dataset_train = TabularDataset(csv_file=dataset_config.csv_file_train, y_column=dataset_config.y_column)\n",
    "    dataset_train = TabularDataset(csv_file=dataset_config.csv_file_train, y_column=dataset_config.y_column)\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=model_config.batch_size_train, shuffle=True)     # gives batch for training features and labels  (both in float 32)\n",
    "\n",
    "    dataset_test = TabularDataset(csv_file=dataset_config.csv_file_test, y_column=dataset_config.y_column)\n",
    "    dataloader_test = DataLoader(dataset_test, batch_size=model_config.batch_size_test, shuffle=True)       # gives batch for test features and label    (both in float 32)\n",
    "\n",
    "    dataset_pool = TabularDataset(csv_file=dataset_config.csv_file_pool, y_column=dataset_config.y_column)\n",
    "    pool_size = len(dataset_pool)\n",
    "    dataloader_pool = DataLoader(dataset_pool, batch_size=pool_size, shuffle=False)       # gives all the pool features and label   (both in float 32) - needed for input in NN_weights\n",
    "\n",
    "    dataset_pool_train = TabularDatasetPool(csv_file=dataset_config.csv_file_pool, y_column=dataset_config.y_column)\n",
    "    dataloader_pool_train = DataLoader(dataset_pool_train, batch_size=model_config.batch_size_train, shuffle=True)       # gives batch of the pool features and label   (both in float 32) - needed for updating the posterior of ENN - as we will do batchwise update\n",
    "\n",
    "\n",
    "\n",
    "    sample, label = dataset_train[0]\n",
    "    input_feature_size = sample.shape[0]       # Size of input features  ---- assuming 1D features\n",
    "\n",
    "    NN_weights = NN_feature_weights(input_feature_size, model_config.hidden_sizes_weight_NN, 1)\n",
    "    # --- TO INITIAL PARAMETRIZATION WITHIN  [0,1] , ALSO SET SEED ----------\n",
    "\n",
    "\n",
    "    meta_opt = optim.Adam(NN_weights.parameters(), lr=model_config.meta_opt_lr)       # meta_opt is optimizer for parameters of NN_weights\n",
    "\n",
    "    #seed for this\n",
    "    SubsetOperator = k_subset_sampling.SubsetOperator(model_config.batch_size_query, model_config.temp_k_subset, False)\n",
    "\n",
    "    #seed for this\n",
    "    SubsetOperatortest = k_subset_sampling.SubsetOperator(model_config.batch_size_query, model_config.temp_k_subset, True)\n",
    "\n",
    "\n",
    "    # to_device\n",
    "    ENN = basenet_with_learnable_epinet_and_ensemble_prior(input_feature_size, enn_config.basenet_hidden_sizes, model_config.n_classes, enn_config.exposed_layers, enn_config.z_dim, enn_config.learnable_epinet_hiddens, enn_config.hidden_sizes_prior, enn_config.seed_base, enn_config.seed_learnable_epinet, enn_config.seed_prior_epinet, enn_config.alpha)\n",
    "\n",
    "\n",
    "    loss_fn_init = nn.CrossEntropyLoss()\n",
    "    optimizer_init = optim.Adam(ENN.parameters(), lr=model_config.init_train_lr, weight_decay=model_config.init_train_weight_decay)\n",
    "    # ------- seed for this training\n",
    "    # ------- train ENN on initial training data  # save the state - ENN_initial_state  # define a separate optimizer for this # how to sample z's ---- separately for each batch\n",
    "    # ------- they also sampled the data each time and not a dataloader - kind of a bootstrap\n",
    "\n",
    "    for i in range(model_config.n_train_init):\n",
    "        ENN.train()\n",
    "        for (inputs, labels) in dataloader_train:\n",
    "            \n",
    "            \n",
    "            z = torch.randn(8)   #set seed for this  #set to_device for this\n",
    "            optimizer_init.zero_grad()\n",
    "            outputs = ENN(inputs,z)\n",
    "            \n",
    "            labels = torch.tensor(labels, dtype=torch.long)\n",
    "            loss = loss_fn_init(outputs, torch.squeeze(labels))\n",
    "            loss.backward()\n",
    "            optimizer_init.step()\n",
    "\n",
    "    #initial training completed\n",
    "\n",
    "    # Predictor =       # model for which we will evaluate recall   # load pretrained weights for the Predictor or train it\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(model_config.n_epoch):\n",
    "        train(train_config, dataloader_pool, dataloader_pool_train, dataloader_test, device, NN_weights, meta_opt, SubsetOperator, ENN, Predictor)\n",
    "\n",
    "    test(train_config, dataloader_pool, dataloader_pool_train, dataloader_test, device,  NN_weights, SubsetOperatortest, ENN, Predictor)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yuanzhe_new",
   "language": "python",
   "name": "yuanzhe_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
