{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Set initial training data weights to be 1.0\n",
        "*   Set the pool data training weights to be from the neural net\n",
        "\n",
        "\n",
        "Different pipelines:\n",
        "\n",
        "1) We know original parameters of the GP versus we optimize for the parameters.\n",
        "\n",
        "2) We know the pool labels versus we do not know - when we know the pool labels - the problem is like we weant to select data (with known labels) from the pool.\n",
        "\n",
        "\n",
        "\n",
        "*   Concatenate all the weights and the data\n",
        "\n"
      ],
      "metadata": {
        "id": "m6rDk8Zry8yN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gpytorch"
      ],
      "metadata": {
        "id": "BV0ISOqYxpWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class WeightedKernel(gpytorch.kernels.Kernel):\n",
        "    has_lengthscale = True\n",
        "\n",
        "    def __init__(self, base_kernel: gpytorch.kernels.Kernel, weights: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Initializes the WeightedKernel with explicit weights.\n",
        "\n",
        "        Args:\n",
        "            base_kernel (gpytorch.kernels.Kernel): The base kernel.\n",
        "            weights (Tensor): A tensor of precomputed weights.\n",
        "        \"\"\"\n",
        "        super(WeightedKernel, self).__init__()\n",
        "        self.base_kernel = base_kernel\n",
        "        self.weights = weights\n",
        "\n",
        "    def forward(self, x1, x2, diag=False, **params):\n",
        "        weighted_covar = self.base_kernel(x1, x2, diag=diag, **params)\n",
        "        if not diag:\n",
        "            weight_matrix = self.weights.unsqueeze(-1) * self.weights.unsqueeze(-2)\n",
        "            weighted_covar = weighted_covar * weight_matrix\n",
        "        else:\n",
        "            weighted_covar = weighted_covar * self.weights * self.weights\n",
        "        return weighted_covar\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class WeightedGPModel(gpytorch.models.ExactGP):\n",
        "    def __init__(self, train_x, train_y, mean_module, base_kernel, likelihood, weights):\n",
        "        super(WeightedGPModel, self).__init__(train_x, train_y, likelihood)\n",
        "        self.mean_module = mean_module\n",
        "        self.covar_module = WeightedKernel(base_kernel=base_kernel, weights=weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean_x = self.mean_module(x)\n",
        "        covar_x = self.covar_module(x, x)\n",
        "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WWkI_t3n0z9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "train_x = torch.rand(10, 2)  # Example training data\n",
        "train_y = torch.sin(train_x[:,0])  # Example target data\n",
        "\n",
        "\n",
        "mean_module = gpytorch.means.ConstantMean()\n",
        "base_kernel = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
        "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
        "\n",
        "\n",
        "mean_module.constant = mean_constant\n",
        "base_kernel.base_kernel.lengthscale = length_scale\n",
        "likelihood.noise_covar.noise = noise_std**2\n",
        "\n",
        "\n",
        "# Example weights, one weight per training data point\n",
        "weights = torch.rand(train_x.size(0), grad = True)\n",
        "\n",
        "# Initialize model\n",
        "model = MyGPModel(train_x, train_y, mean_module, base_kernel, likelihood, weights)\n"
      ],
      "metadata": {
        "id": "C2hBikYHAF0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "observed_pred = likelihood(model(test_x))\n",
        "\n",
        "# Sample from the predictive distribution using reparameterization  (rsample)\n",
        "samples = observed_pred.rsample(sample_shape=torch.Size([1000]))\n",
        "\n",
        "\n",
        "\n",
        "# Define multidimensional test data points\n",
        "num_test_samples = 100\n",
        "test_x = torch.rand((num_test_samples, input_dim))\n",
        "\n",
        "\n",
        "posterior_test_latent = model(test_x)\n",
        "test_f = posterior_test_latent.rsample()\n",
        "posterior_test = likelihood(test_f)\n",
        "test_y = posterior_test.rsample()"
      ],
      "metadata": {
        "id": "fc2JieQE_5sU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam([\n",
        "    {'params': model.parameters()},  # Includes both mean and covariance parameters\n",
        "    {'params': likelihood.parameters()},  # Includes noise parameter\n",
        "], lr=0.1)"
      ],
      "metadata": {
        "id": "WFdBVm88OgVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make predictions on a test set that also has associated weights, you would typically use the trained model directly without altering it for the test weights. The weights in the WeightedKernel are intended to model the importance or characteristics of the training points. For prediction, the model's learned parameters and the nature of the GP will determine how it generalizes to new points.\n",
        "\n",
        "If you have a specific scenario where the test set weights need to directly influence the prediction (which is unusual in GP modeling), you would need to adjust the model's design or prediction mechanism to account for test set weights, potentially by creating a custom prediction method that incorporates these weights into the prediction process in a meaningful way.\n",
        "\n",
        "However, in standard practice, the test set weights are not used in the same way as the training set weights. The test set weights might be used for evaluating model performance (e.g., weighted RMSE) but not typically for influencing the model's predictions."
      ],
      "metadata": {
        "id": "kxpy-oJK8GTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam([\n",
        "    {'params': model.parameters()},  # Includes both mean and covariance parameters\n",
        "    {'params': likelihood.parameters()},  # Includes noise parameter\n",
        "], lr=0.1)\n",
        "\n",
        "\n",
        "# \"Loss\" for GPs - the marginal log likelihood\n",
        "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    likelihood.train()\n",
        "\n",
        "    for i in range(50):  # Number of iterations\n",
        "        optimizer.zero_grad()  # Zero gradients from previous iteration\n",
        "        output = model(train_x)  # Forward pass\n",
        "        loss = -mll(output, train_y)  # Compute the negative marginal log likelihood      ###### REWEIGHT THESE ACCORDING TO OUR OWN WEIGHTS\n",
        "        loss.backward()  # Backpropagate the gradient\n",
        "        optimizer.step()  # Step the optimizer\n",
        "\n",
        "        print(f'Iter {i+1}/{50} - Loss: {loss.item()}')\n",
        "\n",
        "train()\n"
      ],
      "metadata": {
        "id": "V2ipcKzS2BIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Weighted GP inference\n",
        "\n",
        "import torch\n",
        "import gpytorch\n",
        "\n",
        "class WeightedKernel(gpytorch.kernels.Kernel):\n",
        "    def __init__(self, base_kernel: gpytorch.kernels.Kernel, weight_function):\n",
        "        \"\"\"\n",
        "        Initializes the WeightedKernel.\n",
        "\n",
        "        Args:\n",
        "            base_kernel (gpytorch.kernels.Kernel): The base kernel to apply weights to.\n",
        "            weight_function (callable): A function that takes in inputs and returns weights.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.base_kernel = base_kernel\n",
        "        self.weight_function = weight_function\n",
        "\n",
        "    def forward(self, x1, x2, diag=False, **params):\n",
        "        \"\"\"\n",
        "        Computes the kernel matrix between x1 and x2, scaling by weights.\n",
        "\n",
        "        Args:\n",
        "            x1, x2 (Tensor): Input tensors.\n",
        "            diag (bool): Whether to return the diagonal of the kernel matrix.\n",
        "            **params: Additional parameters for the base kernel.\n",
        "        \"\"\"\n",
        "        # Compute base kernel matrix\n",
        "        base_covar = self.base_kernel(x1, x2, diag=diag, **params)\n",
        "\n",
        "        # Compute weights for each pair of inputs\n",
        "        weights1 = self.weight_function(x1)\n",
        "        weights2 = self.weight_function(x2) if x2 is not x1 else weights1\n",
        "\n",
        "        # Apply weights to the covariance\n",
        "        # This is a simple element-wise multiplication for demonstration; adjust as necessary\n",
        "        weighted_covar = base_covar * weights1 * weights2\n",
        "\n",
        "        return weighted_covar\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1cDgHQfAgW9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of defining a weight function\n",
        "def my_weight_function(x):\n",
        "    # Simple example: use the sum of input features as the weight\n",
        "    return x.sum(dim=-1)\n",
        "\n",
        "# Initialize the base kernel, e.g., an RBF kernel\n",
        "base_kernel = gpytorch.kernels.RBFKernel()\n",
        "\n",
        "# Initialize the weighted kernel\n",
        "weighted_kernel = WeightedKernel(base_kernel=base_kernel, weight_function=my_weight_function)\n",
        "\n",
        "# Use weighted_kernel in your GP model as usual\n"
      ],
      "metadata": {
        "id": "gLHACGxVh3bT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import gpytorch\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Define the GP Model\n",
        "class WeightedGPModel(gpytorch.models.ExactGP):\n",
        "    def __init__(self, train_x, train_y, likelihood, base_kernel, weight_function):\n",
        "        super(WeightedGPModel, self).__init__(train_x, train_y, likelihood)\n",
        "        self.mean_module = gpytorch.means.ConstantMean()\n",
        "        self.covar_module = WeightedKernel(base_kernel=base_kernel, weight_function=weight_function)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean_x = self.mean_module(x)\n",
        "        covar_x = self.covar_module(x, x)\n",
        "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
        "\n",
        "# Example usage of WeightedGPModel\n",
        "# Define a simple weight function\n",
        "def my_weight_function(x):\n",
        "    # Use the sum of input features as the weight for demonstration\n",
        "    return torch.exp(-x.sum(dim=-1))\n",
        "\n",
        "# Training data\n",
        "train_x = torch.linspace(0, 1, 100)\n",
        "train_y = torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2\n",
        "\n",
        "# Initialize the base kernel and likelihood\n",
        "base_kernel = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
        "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
        "\n",
        "# Initialize model\n",
        "model = WeightedGPModel(train_x, train_y, likelihood, base_kernel, my_weight_function)\n",
        "\n",
        "# This is for illustration purposes. In practice, you would need to train the model.\n",
        "# Here's a simple training loop:\n",
        "model.train()\n",
        "likelihood.train()\n",
        "\n",
        "# Use the Adam optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
        "\n",
        "# \"Loss\" for GPs - the marginal log likelihood\n",
        "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
        "\n",
        "training_iter = 50\n",
        "for i in range(training_iter):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(train_x)\n",
        "    loss = -mll(output, train_y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "id": "_V7Hu7EbiUDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gpytorch\n",
        "\n",
        "class ConditionalWeightedKernel(gpytorch.kernels.Kernel):\n",
        "    def __init__(self, base_kernel: gpytorch.kernels.Kernel, weight_function, apply_weights_condition):\n",
        "        super().__init__()\n",
        "        self.base_kernel = base_kernel\n",
        "        self.weight_function = weight_function\n",
        "        self.apply_weights_condition = apply_weights_condition\n",
        "\n",
        "    def forward(self, x1, x2, diag=False, **params):\n",
        "        base_covar = self.base_kernel(x1, x2, diag=diag, **params)\n",
        "\n",
        "        # Determine which inputs satisfy the condition to apply weights\n",
        "        apply_weights1 = self.apply_weights_condition(x1)\n",
        "        apply_weights2 = self.apply_weights_condition(x2) if x2 is not x1 else apply_weights1\n",
        "\n",
        "        # Only apply weights to the inputs that satisfy the condition\n",
        "        if apply_weights1.any() and apply_weights2.any():\n",
        "            weights1 = self.weight_function(x1)\n",
        "            weights2 = self.weight_function(x2) if x2 is not x1 else weights1\n",
        "            weighted_covar = base_covar * weights1 * weights2\n",
        "        else:\n",
        "            weighted_covar = base_covar\n",
        "\n",
        "        return weighted_covar\n",
        "\n",
        "# Example GP model using the ConditionalWeightedKernel\n",
        "class MyGPModel(gpytorch.models.ExactGP):\n",
        "    def __init__(self, train_x, train_y, likelihood, base_kernel, weight_function, apply_weights_condition):\n",
        "        super(MyGPModel, self).__init__(train_x, train_y, likelihood)\n",
        "        self.mean_module = gpytorch.means.ConstantMean()\n",
        "        self.covar_module = ConditionalWeightedKernel(\n",
        "            base_kernel=base_kernel,\n",
        "            weight_function=weight_function,\n",
        "            apply_weights_condition=apply_weights_condition\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean_x = self.mean_module(x)\n",
        "        covar_x = self.covar_module(x, x)\n",
        "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
        "\n",
        "# Define your weight function and condition\n",
        "def my_weight_function(x):\n",
        "    return x.sum(dim=-1)\n",
        "\n",
        "def apply_weights_condition(x):\n",
        "    # Example condition: apply weights if the sum of features is greater than a threshold\n",
        "    return x.sum(dim=-1) > 5\n",
        "\n",
        "# Initialize your GP model\n",
        "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
        "base_kernel = gpytorch.kernels.RBFKernel()\n",
        "\n",
        "gp_model = MyGPModel(\n",
        "    train_x=torch.rand(10, 3),\n",
        "    train_y=torch.rand(10),\n",
        "    likelihood=likelihood,\n",
        "    base_kernel=base_kernel,\n",
        "    weight_function=my_weight_function,\n",
        "    apply_weights_condition=apply_weights_condition\n",
        ")\n"
      ],
      "metadata": {
        "id": "M0S_Yva3ijzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gpytorch\n",
        "\n",
        "class SelectiveWeightedKernel(gpytorch.kernels.Kernel):\n",
        "    def __init__(self, base_kernel: gpytorch.kernels.Kernel, weights=None, weight_function=None):\n",
        "        \"\"\"\n",
        "        Initializes the SelectiveWeightedKernel.\n",
        "\n",
        "        Args:\n",
        "            base_kernel (gpytorch.kernels.Kernel): The base kernel.\n",
        "            weights (Tensor, optional): A tensor of precomputed weights. If provided, weight_function is ignored.\n",
        "            weight_function (callable, optional): A function to compute weights dynamically based on inputs.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.base_kernel = base_kernel\n",
        "        self.weights = weights\n",
        "        self.weight_function = weight_function\n",
        "\n",
        "    def forward(self, x1, x2, diag=False, **params):\n",
        "        \"\"\"\n",
        "        Computes the kernel matrix between x1 and x2, selectively scaling by weights.\n",
        "        \"\"\"\n",
        "        # Compute base kernel matrix\n",
        "        base_covar = self.base_kernel(x1, x2, diag=diag, **params)\n",
        "\n",
        "        # Determine how to apply weights\n",
        "        if self.weights is not None:\n",
        "            # If precomputed weights are provided, use them\n",
        "            weights1 = self.weights\n",
        "            weights2 = self.weights\n",
        "        elif self.weight_function is not None:\n",
        "            # If a weight function is provided, compute weights dynamically\n",
        "            weights1 = self.weight_function(x1)\n",
        "            weights2 = self.weight_function(x2) if x2 is not x1 else weights1\n",
        "        else:\n",
        "            # If no weights are provided, do not modify the base covariance\n",
        "            return base_covar\n",
        "\n",
        "        # Apply weights to the covariance\n",
        "        weighted_covar = base_covar * weights1.unsqueeze(-1) * weights2.unsqueeze(-1).transpose(-2, -1)\n",
        "\n",
        "        return weighted_covar\n",
        "\n",
        "# Example usage within a GP Model\n",
        "class MyGPModel(gpytorch.models.ExactGP):\n",
        "    def __init__(self, train_x, train_y, likelihood, base_kernel, weights=None, weight_function=None):\n",
        "        super(MyGPModel, self).__init__(train_x, train_y, likelihood)\n",
        "        self.mean_module = gpytorch.means.ConstantMean()\n",
        "        self.covar_module = SelectiveWeightedKernel(base_kernel=base_kernel, weights=weights, weight_function=weight_function)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean_x = self.mean_module(x)\n",
        "        covar_x = self.covar_module(x, x)\n",
        "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n"
      ],
      "metadata": {
        "id": "6P_rhZZLjDAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gpytorch\n",
        "\n",
        "class WeightedKernel(gpytorch.kernels.Kernel):\n",
        "    def __init__(self, base_kernel: gpytorch.kernels.Kernel, weights):\n",
        "        \"\"\"\n",
        "        Initializes the WeightedKernel with explicit weights.\n",
        "\n",
        "        Args:\n",
        "            base_kernel (gpytorch.kernels.Kernel): The base kernel.\n",
        "            weights (Tensor): A tensor of precomputed weights.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.base_kernel = base_kernel\n",
        "        self.register_buffer('weights', weights)\n",
        "\n",
        "    def forward(self, x1, x2, diag=False, **params):\n",
        "        \"\"\"\n",
        "        Computes the kernel matrix between x1 and x2, scaling by precomputed weights.\n",
        "        \"\"\"\n",
        "        # Compute base kernel matrix\n",
        "        base_covar = self.base_kernel(x1, x2, diag=diag, **params)\n",
        "\n",
        "        # Apply weights to the covariance\n",
        "        # Assuming weights is a 1D tensor of the same length as x1 and x2's first dimension\n",
        "        weighted_covar = base_covar * self.weights.unsqueeze(-1) * self.weights.unsqueeze(-1).transpose(-2, -1)\n",
        "\n",
        "        return weighted_covar\n",
        "\n",
        "class MyGPModel(gpytorch.models.ExactGP):\n",
        "    def __init__(self, train_x, train_y, likelihood, base_kernel, weights):\n",
        "        super(MyGPModel, self).__init__(train_x, train_y, likelihood)\n",
        "        self.mean_module = gpytorch.means.ConstantMean()\n",
        "        self.covar_module = WeightedKernel(base_kernel=base_kernel, weights=weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean_x = self.mean_module(x)\n",
        "        covar_x = self.covar_module(x, x)\n",
        "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
        "\n",
        "# Example usage\n",
        "train_x = torch.rand(10, 2)  # Example training data\n",
        "train_y = torch.sin(train_x[:,0])  # Example target data\n",
        "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
        "base_kernel = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
        "\n",
        "# Example weights, one weight per training data point\n",
        "weights = torch.rand(train_x.size(0))\n",
        "\n",
        "# Initialize model\n",
        "model = MyGPModel(train_x, train_y, likelihood, base_kernel, weights)\n"
      ],
      "metadata": {
        "id": "9BmfN5EWjeDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gpytorch\n",
        "\n",
        "# Modified SelectiveWeightedKernel to accept and use precomputed weights directly\n",
        "class WeightedKernel(gpytorch.kernels.Kernel):\n",
        "    def __init__(self, base_kernel: gpytorch.kernels.Kernel, weights):\n",
        "        \"\"\"\n",
        "        Initializes the WeightedKernel with precomputed weights.\n",
        "\n",
        "        Args:\n",
        "            base_kernel (gpytorch.kernels.Kernel): The base kernel.\n",
        "            weights (Tensor): A tensor of precomputed weights.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.base_kernel = base_kernel\n",
        "        self.register_buffer(\"weights\", weights)  # Registering weights as a buffer to ensure proper device handling\n",
        "\n",
        "    def forward(self, x1, x2, diag=False, **params):\n",
        "        \"\"\"\n",
        "        Computes the kernel matrix between x1 and x2, scaling by weights.\n",
        "        \"\"\"\n",
        "        base_covar = self.base_kernel(x1, x2, diag=diag, **params)\n",
        "        # Apply weights - ensure weights are correctly broadcasted\n",
        "        weighted_covar = base_covar * self.weights.unsqueeze(-1) * self.weights.unsqueeze(-1).transpose(-2, -1)\n",
        "        return weighted_covar\n",
        "\n",
        "# Example GP model using the weighted kernel\n",
        "class WeightedGPModel(gpytorch.models.ExactGP):\n",
        "    def __init__(self, train_x, train_y, likelihood, base_kernel, weights):\n",
        "        super(WeightedGPModel, self).__init__(train_x, train_y, likelihood)\n",
        "        self.mean_module = gpytorch.means.ConstantMean()\n",
        "        self.covar_module = WeightedKernel(base_kernel=base_kernel, weights=weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean_x = self.mean_module(x)\n",
        "        covar_x = self.covar_module(x, x)\n",
        "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
        "\n",
        "\n",
        "\n",
        "def recondition_gp_model(model, new_train_x, new_train_y, new_weights):\n",
        "    \"\"\"\n",
        "    Update the GP model with new training data and weights.\n",
        "\n",
        "    Args:\n",
        "        model (WeightedGPModel): The GP model to update.\n",
        "        new_train_x (Tensor): New training inputs.\n",
        "        new_train_y (Tensor): New training outputs.\n",
        "        new_weights (Tensor): New weights for the training inputs.\n",
        "    \"\"\"\n",
        "    # Update training data\n",
        "    model.set_train_data(inputs=new_train_x, targets=new_train_y, strict=False)\n",
        "\n",
        "    # Update weights\n",
        "    model.covar_module.weights = new_weights\n",
        "\n",
        "    # Note: Depending on the use case, you might need to reset parameters or perform other updates.\n"
      ],
      "metadata": {
        "id": "Luf3yxACj0QE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gpytorch\n",
        "\n",
        "class WeightedGPModel(gpytorch.models.ExactGP):\n",
        "    def __init__(self, train_x, train_y, likelihood, base_kernel, weights):\n",
        "        super(WeightedGPModel, self).__init__(train_x, train_y, likelihood)\n",
        "        self.mean_module = gpytorch.means.ConstantMean()\n",
        "        self.base_kernel = base_kernel\n",
        "        self.weights = torch.nn.Parameter(weights, requires_grad=True)  # Make weights a learnable parameter\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply weights to the base kernel output\n",
        "        weighted_covar = self.base_kernel(x) * self.weights.unsqueeze(-1)\n",
        "        return gpytorch.distributions.MultivariateNormal(self.mean_module(x), weighted_covar)\n",
        "\n",
        "# Example of using the model\n",
        "N, D = 10, 3  # Example dimensions for the training data\n",
        "train_x = torch.randn(N, D)\n",
        "train_y = torch.randn(N)\n",
        "weights = torch.randn(N)  # Initial weights\n",
        "\n",
        "# Initialize the GP model components\n",
        "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
        "base_kernel = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
        "\n",
        "# Initialize the GP model with explicit weights\n",
        "model = WeightedGPModel(train_x, train_y, likelihood, base_kernel, weights)\n",
        "\n",
        "# Example: Perform a forward pass (inference)\n",
        "model.eval()\n",
        "likelihood.eval()\n",
        "with torch.no_grad():\n",
        "    observed_pred = likelihood(model(train_x))\n",
        "\n",
        "# The model is differentiable with respect to the weights, allowing for gradient updates\n"
      ],
      "metadata": {
        "id": "Dw7aNkkckIg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gpytorch\n",
        "\n",
        "# Simplified SelectiveWeightedKernel that accepts explicit weights\n",
        "class SelectiveWeightedKernel(gpytorch.kernels.ScaleKernel):\n",
        "    def __init__(self, base_kernel, weights=None):\n",
        "        super(SelectiveWeightedKernel, self).__init__(base_kernel)\n",
        "        self.weights = weights\n",
        "\n",
        "    def forward(self, x1, x2, diag=False, **params):\n",
        "        base_covar = super().forward(x1, x2, diag=diag, **params)\n",
        "        if self.weights is not None:\n",
        "            weighted_covar = base_covar * self.weights\n",
        "        else:\n",
        "            weighted_covar = base_covar\n",
        "        return weighted_covar\n",
        "\n",
        "# Example GP Model\n",
        "class MyGPModel(gpytorch.models.ExactGP):\n",
        "    def __init__(self, train_x, train_y, likelihood, base_kernel, weights=None):\n",
        "        super(MyGPModel, self).__init__(train_x, train_y, likelihood)\n",
        "        self.mean_module = gpytorch.means.ConstantMean()\n",
        "        self.covar_module = SelectiveWeightedKernel(base_kernel=base_kernel, weights=weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean_x = self.mean_module(x)\n",
        "        covar_x = self.covar_module(x, x)\n",
        "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
        "\n",
        "# Updating the model with new training data and potentially new weights\n",
        "def update_model_with_new_data(model, new_train_x, new_train_y, new_weights=None):\n",
        "    model.set_train_data(inputs=new_train_x, targets=new_train_y, strict=False)\n",
        "    model.covar_module.weights = new_weights  # Update weights if provided\n"
      ],
      "metadata": {
        "id": "6Kv5J9VZkJJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_model_with_cumulative_data(model, old_train_x, old_train_y, new_train_x, new_train_y, new_weights=None):\n",
        "    # Concatenate old training data with new training data\n",
        "    combined_train_x = torch.cat([old_train_x, new_train_x], 0)\n",
        "    combined_train_y = torch.cat([old_train_y, new_train_y], 0)\n",
        "\n",
        "    # Update the model with the combined dataset\n",
        "    model.set_train_data(inputs=combined_train_x, targets=combined_train_y, strict=False)\n",
        "\n",
        "    # Optionally, update weights if provided\n",
        "    if new_weights is not None:\n",
        "        # Assuming new_weights is adjusted to match the combined dataset\n",
        "        model.covar_module.weights = new_weights\n"
      ],
      "metadata": {
        "id": "HqEObCcLkxci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you use the `set_train_data` method in GPyTorch to update a Gaussian Process (GP) model with new training data, it effectively replaces the existing training data with the new data you provide. This means the model \"forgets\" the old training data in the sense that it is no longer used directly in the training process or in the calculation of the GP posterior. The model will then use only the new training data for future predictions and training iterations unless the old data is explicitly included in the update.\n",
        "\n",
        "Given this behavior, the choice between incremental and cumulative updates to your GP model should consider the following factors:\n",
        "\n",
        "1. **Data Relevance:** If all data points (old and new) are relevant to the problem you're modeling, a cumulative update approach (where you combine old and new data) might be more appropriate. This ensures that the model learns from the entire dataset, potentially leading to more accurate predictions.\n",
        "\n",
        "2. **Computational Resources:** Training GP models on large datasets can be computationally expensive due to the inversion of the covariance matrix, which is a \\(O(N^3)\\) operation, where \\(N\\) is the number of training points. If your dataset grows large, you might need to consider sparse GP methods or other strategies to manage computational demands.\n",
        "\n",
        "3. **Concept Drift:** If the data's underlying distribution changes over time (concept drift), it might be beneficial to prioritize more recent data, allowing for incremental updates or using methods that can weight the data based on its recency or relevance.\n",
        "\n",
        "4. **Memory Constraints:** For applications with limited memory, retaining all historical data might not be feasible. Incremental updates or strategies to summarize the old data efficiently could be necessary.\n",
        "\n",
        "### Suggestion\n",
        "\n",
        "- **Balanced Approach:** Consider a balanced approach where you periodically update the model with a mix of old and new data that captures the essential characteristics of the problem space. This could involve techniques like data summarization, where you keep a representative subset of the old data, or using inducing points in sparse Gaussian Processes to manage computational complexity while retaining the model's ability to learn from a broad dataset.\n",
        "\n",
        "- **Explore Sparse GPs:** If dealing with very large datasets, explore using sparse Gaussian Processes provided by GPyTorch, like `InducingPointKernel`, which allows the GP to scale more effectively to large datasets by approximating the full GP model.\n",
        "\n",
        "- **Monitor Model Performance:** Regularly evaluate the model's performance on a validation set to determine if the strategy you've chosen is effective or if adjustments are needed based on changing data distributions or performance criteria.\n",
        "\n",
        "When using `set_train_data`, it's essential to be mindful of how the changes in training data impact the model's knowledge and predictions, adjusting your data management strategy to fit the needs of your specific application."
      ],
      "metadata": {
        "id": "QYWw3waFlpcr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When considering the best approach to updating your Gaussian Process (GP) model in GPyTorch with new training data, the decision largely depends on the specific characteristics of your problem, including the size of your dataset, the nature of your data (stationary vs. non-stationary), computational resources, and the specific goals of your analysis or application.\n",
        "\n",
        "### Suggestions on Updating GP Models with New Data\n",
        "\n",
        "1. **Cumulative Update (Including Old Data):**\n",
        "   - If your dataset is not too large and the computational resources allow, it's generally beneficial to include all available data in your model. This approach ensures that the model has the most comprehensive understanding of the data generating process, which is particularly important if the data is stationary and the underlying process does not change over time.\n",
        "\n",
        "2. **Incremental Update (Without Keeping Old Data):**\n",
        "   - For cases where data is non-stationary, or if you're dealing with constraints on memory or computational resources, an incremental update might be more appropriate. Here, you might use a sliding window approach or a forgetting mechanism to prioritize more recent data.\n",
        "\n",
        "### GPyTorch's `set_train_data` and Its Effects\n",
        "\n",
        "When you use `set_train_data` in GPyTorch, it updates the model's training data with the provided inputs and targets. This method effectively replaces the existing training data with the new data, meaning the model \"forgets\" the old training data unless it is included in the new dataset you provide. This behavior is designed to give you flexibility in managing the model's training data, but it also means that if you wish to retain the influence of the old data, you must explicitly include it in the dataset you set with `set_train_data`.\n",
        "\n",
        "### Differentiability Through `set_train_data`\n",
        "\n",
        "The operation of setting new training data itself (`set_train_data`) is not part of the computational graph that PyTorch uses for automatic differentiation. The method updates the model's training data attributes but does not directly affect the gradients of the model's parameters with respect to its inputs. The differentiation and learning processes in GPyTorch (or any other PyTorch model) concern the model's parameters (e.g., kernel hyperparameters, likelihood parameters) rather than the training data itself.\n",
        "\n",
        "However, after updating the training data, the subsequent operations (like inference and optimization of the model's hyperparameters based on the new data) are fully differentiable. This means you can optimize the model's parameters based on the new data, and gradients will propagate through the model's computations that use these parameters.\n",
        "\n",
        "### Decision Making\n",
        "\n",
        "- If your application can afford the computational cost and memory usage, and if retaining all data is beneficial for model performance, opt for the cumulative update approach.\n",
        "- If you're dealing with large datasets, non-stationary processes, or limited resources, consider using incremental updates or developing a strategy to selectively include the most relevant data.\n",
        "- Regardless of the approach, it's essential to carefully manage how the training data is updated to ensure the model remains accurate and relevant to the task at hand.\n",
        "\n",
        "In summary, your strategy should align with the specific requirements and constraints of your application, balancing the need for model accuracy and freshness of the data against computational and memory limitations."
      ],
      "metadata": {
        "id": "8EDywkgrmJhv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vikV-FdHmKz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When deciding whether to keep old training data or replace it with new data in a Gaussian Process (GP) model (like those created with GPyTorch), the best approach depends on the specific goals and constraints of your application. Here are some considerations to guide your decision:\n",
        "\n",
        "### Keeping Old Data vs. Replacing with New Data\n",
        "- **Memory and Computational Constraints:** Keeping all the data can quickly become computationally expensive as GPs typically have a computational complexity of \\(O(n^3)\\) for training, where \\(n\\) is the number of training points. If computational resources are limited, you might opt for strategies that allow for incremental updates or subsampling.\n",
        "- **Relevance of Data:** If the process you're modeling is stationary, keeping all the data might improve the model's accuracy. However, if the process is non-stationary (i.e., the underlying distribution changes over time), older data might become less relevant, and a strategy that emphasizes newer data could be preferable.\n",
        "- **Online Learning:** For scenarios where data arrives sequentially, and you need to update the model in real-time or near-real-time, incremental updates might be necessary.\n",
        "\n",
        "### Using `set_train_data` in GPyTorch\n",
        "- **Forgetting Old Data:** When you use `set_train_data` in GPyTorch, it updates the model's training data. If you don't include the old data in the new dataset you pass to `set_train_data`, the old data is effectively \"forgotten\" from the model's perspective.\n",
        "- **Differentiability:** GPyTorch's design allows for gradient-based optimization of model hyperparameters (and potentially other parameters if you structure your model to support it). However, the operation of setting new training data itself is not something you'd typically differentiate through since it's a data management operation rather than a parameter optimization step.\n",
        "\n",
        "### Differentiating Through the Training Data Selection Process\n",
        "If your goal is to differentiate the outcome of a GP model's posterior inference with respect to the process of selecting training data (e.g., based on some policy), you're venturing into the realm of differentiable programming where the selection process itself needs to be formulated in a way that gradients can flow through it.\n",
        "\n",
        "One approach to achieve this could involve:\n",
        "- **Parameterizing Your Data Selection Policy:** For example, using a neural network to score potential data points based on their utility, where the network's weights are the parameters you optimize.\n",
        "- **Incorporating the Selection Process into Your Model's Computational Graph:** Ensure that the selection of training data is a differentiable operation. This might involve soft selections or relaxations of discrete choices to continuous ones, enabling gradients to propagate back through the selection process.\n",
        "\n",
        "This setup allows you to use gradient-based optimization not just for the GP model's hyperparameters but also for the parameters governing how training data is selected, aligning the data selection process with your overall optimization objectives.\n",
        "\n",
        "### Practical Steps\n",
        "To implement a system where you can differentiate through both the data selection and GP inference processes, consider:\n",
        "- Designing a differentiable selection mechanism.\n",
        "- Integrating this mechanism with your GP model in a unified computational graph.\n",
        "- Using gradient-based optimization to simultaneously optimize data selection parameters and GP hyperparameters, based on your chosen objective function (e.g., predictive performance on a validation set or some task-specific loss).\n",
        "\n",
        "This approach requires careful formulation to ensure that the entire process, from data selection through GP inference, is differentiable and aligns with your modeling goals."
      ],
      "metadata": {
        "id": "-XARhWJTmyO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing a fully differentiable process that includes both data selection and Gaussian Process (GP) inference within a framework like GPyTorch involves advanced concepts and requires a custom setup. Below is a conceptual outline of how one might approach this, using a differentiable selection mechanism for the training data and integrating it with GP inference.\n",
        "\n",
        "### Step 1: Define a Differentiable Data Selection Mechanism\n",
        "\n",
        "Let's assume we have a dataset from which we want to select training data based on some criteria (e.g., points that are expected to provide the most information about the model). One approach is to use a neural network that assigns scores to each data point, where higher scores indicate higher utility. This network's parameters can be optimized based on the outcome of the GP model's predictions.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import gpytorch\n",
        "\n",
        "class DataSelector(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(DataSelector, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, 1)  # Simple linear layer to score data points\n",
        "\n",
        "    def forward(self, X):\n",
        "        scores = self.fc(X)\n",
        "        return torch.sigmoid(scores)  # Use sigmoid to get scores between 0 and 1\n",
        "```\n",
        "\n",
        "### Step 2: Integrate Data Selection with GP Model\n",
        "\n",
        "Next, integrate the selection process with your GP model. This involves using the scores from the `DataSelector` to weigh the contributions of different data points, potentially as part of the kernel function or in determining which points to actively use for training.\n",
        "\n",
        "```python\n",
        "class CustomGPModel(gpytorch.models.ExactGP):\n",
        "    def __init__(self, train_x, train_y, likelihood):\n",
        "        super(CustomGPModel, self).__init__(train_x, train_y, likelihood)\n",
        "        self.mean_module = gpytorch.means.ConstantMean()\n",
        "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean_x = self.mean_module(x)\n",
        "        covar_x = self.covar_module(x)\n",
        "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
        "```\n",
        "\n",
        "### Step 3: Optimize Data Selection and GP Model Jointly\n",
        "\n",
        "You would then define an optimization loop where both the parameters of the `DataSelector` and the GP model (including its hyperparameters) are updated based on the performance of the GP model's predictions.\n",
        "\n",
        "```python\n",
        "# Initialize components\n",
        "data_selector = DataSelector(input_dim=X.shape[1])\n",
        "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
        "gp_model = CustomGPModel(train_x=torch.empty((0, X.shape[1])), train_y=torch.empty(0), likelihood=likelihood)\n",
        "\n",
        "# Optimizer for both data selector and GP model parameters\n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': data_selector.parameters()},\n",
        "    {'params': gp_model.parameters()},\n",
        "    {'params': likelihood.parameters()}\n",
        "], lr=0.01)\n",
        "\n",
        "# Example training loop\n",
        "for i in range(training_iterations):\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Select data based on current data selector\n",
        "    scores = data_selector(X)\n",
        "    selected = scores > 0.5  # For example, select data points with scores above 0.5\n",
        "    selected_x = X[selected.squeeze()]\n",
        "    selected_y = Y[selected.squeeze()]\n",
        "    \n",
        "    # Update GP model with selected data\n",
        "    gp_model.set_train_data(inputs=selected_x, targets=selected_y, strict=False)\n",
        "    \n",
        "    # Forward pass through GP model\n",
        "    gp_output = gp_model(selected_x)\n",
        "    \n",
        "    # Compute your loss function (e.g., negative log likelihood)\n",
        "    loss = -gp_model.likelihood(gp_output, selected_y).log_prob(selected_y)\n",
        "    \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "```\n",
        "\n",
        "This example is quite simplified and conceptual. The key challenges in implementing such a system include ensuring the entire process is differentiable (e.g., making the selection process differentiable and handling the discrete nature of data selection) and efficiently managing computational resources (since GPs can be computationally intensive, especially with large datasets).\n",
        "\n",
        "This setup allows the model to learn both which data points are most informative (through the `DataSelector`) and the GP model parameters simultaneously, potentially leading to more efficient learning when acquiring new data in an active learning setting or when trying to optimize the utility of the training dataset."
      ],
      "metadata": {
        "id": "BOPAp8cLm1RT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import gpytorch\n",
        "\n",
        "class DataSelector(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(DataSelector, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, 1)  # Simple linear layer to score data points\n",
        "\n",
        "    def forward(self, X):\n",
        "        scores = self.fc(X)\n",
        "        return torch.sigmoid(scores)  # Use sigmoid to get scores between 0 and 1\n"
      ],
      "metadata": {
        "id": "ghRQyI4dm3rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomGPModel(gpytorch.models.ExactGP):\n",
        "    def __init__(self, train_x, train_y, likelihood):\n",
        "        super(CustomGPModel, self).__init__(train_x, train_y, likelihood)\n",
        "        self.mean_module = gpytorch.means.ConstantMean()\n",
        "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean_x = self.mean_module(x)\n",
        "        covar_x = self.covar_module(x)\n",
        "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n"
      ],
      "metadata": {
        "id": "LBhCloxZm6PI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize components\n",
        "data_selector = DataSelector(input_dim=X.shape[1])\n",
        "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
        "gp_model = CustomGPModel(train_x=torch.empty((0, X.shape[1])), train_y=torch.empty(0), likelihood=likelihood)\n",
        "\n",
        "# Optimizer for both data selector and GP model parameters\n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': data_selector.parameters()},\n",
        "    {'params': gp_model.parameters()},\n",
        "    {'params': likelihood.parameters()}\n",
        "], lr=0.01)\n",
        "\n",
        "# Example training loop\n",
        "for i in range(training_iterations):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Select data based on current data selector\n",
        "    scores = data_selector(X)\n",
        "    selected = scores > 0.5  # For example, select data points with scores above 0.5\n",
        "    selected_x = X[selected.squeeze()]\n",
        "    selected_y = Y[selected.squeeze()]\n",
        "\n",
        "    # Update GP model with selected data\n",
        "    gp_model.set_train_data(inputs=selected_x, targets=selected_y, strict=False)\n",
        "\n",
        "    # Forward pass through GP model\n",
        "    gp_output = gp_model(selected_x)\n",
        "\n",
        "    # Compute your loss function (e.g., negative log likelihood)\n",
        "    loss = -gp_model.likelihood(gp_output, selected_y).log_prob(selected_y)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "id": "yF0igMMDm8f5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WeightedKernel(gpytorch.kernels.Kernel):\n",
        "    def __init__(self, base_kernel: gpytorch.kernels.Kernel, weights):\n",
        "        \"\"\"\n",
        "        Initializes the WeightedKernel with explicit weights.\n",
        "\n",
        "        Args:\n",
        "            base_kernel (gpytorch.kernels.Kernel): The base kernel.\n",
        "            weights (Tensor): A tensor of precomputed weights.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.base_kernel = base_kernel\n",
        "        self.register_buffer('weights', weights)\n",
        "\n",
        "    def forward(self, x1, x2, diag=False, **params):\n",
        "        \"\"\"\n",
        "        Computes the kernel matrix between x1 and x2, scaling by precomputed weights.\n",
        "        \"\"\"\n",
        "        # Compute base kernel matrix\n",
        "        base_covar = self.base_kernel(x1, x2, diag=diag, **params)\n",
        "\n",
        "        # Apply weights to the covariance\n",
        "        # Assuming weights is a 1D tensor of the same length as x1 and x2's first dimension\n",
        "        weighted_covar = base_covar * self.weights.unsqueeze(-1) * self.weights.unsqueeze(-1).transpose(-2, -1)\n",
        "\n",
        "        return weighted_covar"
      ],
      "metadata": {
        "id": "e8Yt_GYH9c-u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}