{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas\n",
        "import numpy"
      ],
      "metadata": {
        "id": "kh9wxat3_hxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### NEW FUNCTION\n",
        "\n",
        "def approx_ber(logits, tau, device): #h is n-dim; output is an approx Bernoulli vector with mean h\n",
        "    gumbel_dist = torch.distributions.gumbel.Gumbel(torch.tensor(0.0), torch.tensor(1.0))\n",
        "    gumbels = gumbel_dist.sample(logits.size()).to(logits.device)                   ### Can use torch.clamp(x, min=1, max=3) here - torch.clamp is autodiffable - but we will not face the inf/nan issue as torch.softmax handles it by subtacting maximum value from all the values.\n",
        "    y_soft = torch.softmax((logits + gumbels) / tau, dim=1)\n",
        "    y = y_soft[:,1]\n",
        "    return y\n",
        "\n",
        "\n",
        "\n",
        "def Model_pred(X_loader, model, device):\n",
        "    prediction_list = torch.empty((0, 1), dtype=torch.float32, device=device)\n",
        "    for (x_batch, label_batch) in X_loader:\n",
        "        prediction = model(x_batch)\n",
        "        prediction_list = torch.cat((prediction_list,prediction),0)\n",
        "\n",
        "\n",
        "    predicted_class = torch.argmax(prediction_list)       ## what is need of this??\n",
        "    predicted_class = prediction_list >= 0.5\n",
        "    return predicted_class\n",
        "\n",
        "\n",
        "def Recall(ENN_logits, predicted_class, tau, device):\n",
        "    Y_vec = approx_ber(ENN_logits, tau, device)\n",
        "\n",
        "    Y_vec = torch.unsqueeze(Y_vec, 1)\n",
        "\n",
        "    x = torch.sum(torch.mul(Y_vec, predicted_class))\n",
        "    y = torch.sum(Y_vec)\n",
        "    return x/y\n",
        "\n",
        "def Recall_True(dataloader_test, model, device):\n",
        "    label_list  = torch.empty((0), dtype=torch.float32, device=device)\n",
        "    prediction_list = torch.empty((0, 1), dtype=torch.float32, device=device)\n",
        "\n",
        "    for (x_batch, label_batch) in dataloader_test:\n",
        "        label_list = torch.cat((label_list,label_batch),0)\n",
        "        prediction = model(x_batch)\n",
        "        prediction_list = torch.cat((prediction_list,prediction),0)\n",
        "\n",
        "    #predicted_class = torch.argmax(prediction_list)                             ### why is this needed??\n",
        "    predicted_class = prediction_list >= 0.5\n",
        "    predicted_class = torch.squeeze(predicted_class, 1)\n",
        "\n",
        "    x = torch.sum(torch.mul(label_list, predicted_class))\n",
        "    y = torch.sum(label_list)\n",
        "\n",
        "    return x/y\n",
        "\n",
        "def var_recall_estimator(fnet, dataloader_test, Predictor, device, para):\n",
        "    tau = para['tau']\n",
        "    z_dim = para['z_dim']\n",
        "    N_iter =  para['N_iter']\n",
        "    if_print =  para['if_print']\n",
        "    predicted_class = Model_pred(dataloader_test, Predictor, device)\n",
        "\n",
        "    res  = torch.empty((0), dtype=torch.float32, device=device)\n",
        "    res_square  = torch.empty((0), dtype=torch.float32, device=device)\n",
        "\n",
        "\n",
        "    for i in range(N_iter):\n",
        "        z_pool = torch.randn(z_dim, device=device)\n",
        "        ENN_logits = torch.empty((0,2), dtype=torch.float32, device=device)\n",
        "        for (x_batch, label_batch) in dataloader_test:\n",
        "            fnet_logits = fnet(x_batch, z_pool)\n",
        "            #fnet_logits_probs = torch.nn.functional.softmax(fnet_logits, dim=1) ---- no need of this as logits can work themselves\n",
        "            ENN_logits = torch.cat((ENN_logits,fnet_logits),dim=0)\n",
        "        recall_est = Recall(ENN_logits, predicted_class, tau, device)\n",
        "        res = torch.cat((res,(recall_est).view(1)),0)\n",
        "        res_square = torch.cat((res_square,(recall_est ** 2).view(1)),0)\n",
        "\n",
        "    var = torch.mean(res_square) - (torch.mean(res)) ** 2\n",
        "    if if_print == 1:\n",
        "        print('recall list', res)\n",
        "        print(\"var of recall:\",var)\n",
        "        print(\"mean of recall\",  torch.mean(res))\n",
        "    return var\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "#res = 0\n",
        "#n = 5\n",
        "#h = torch.tensor([0.15 for i in range(n)])\n",
        "#c = torch.tensor([1, 0, 1, 0, 1]) #fix classifier\n",
        "\n",
        "\n",
        "#tau = 0.1\n",
        "#gamma = 0.5\n",
        "#epsilon = 0.7\n",
        "\n",
        "\n",
        "##ignore the below\n",
        "##var_recall_estimator(fnet, dataloader_test, Predictor)\n",
        "#derivative of fnet_parmaeters w.r.t NN (sampling policy) parameters is known - now we need derivative of var recall w.r.t fnet_parameters\n"
      ],
      "metadata": {
        "id": "_u7HTxf4Gukw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas\n",
        "import numpy\n",
        "from torch.nn.functional import cosine_similarity"
      ],
      "metadata": {
        "id": "EoTCq8tNI_BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Reinforce estimator for the gradient\n",
        "\n",
        "#derivative of E(recall estimator) w.r.t\n",
        "\n",
        "N=1000\n",
        "N_iter = 1000\n",
        "predicted_class = torch.randint(0, 2, (N,))\n",
        "#print(predicted_class)\n",
        "random_logits = torch.randn(N, 2, requires_grad=True)\n",
        "#print(random_logits)"
      ],
      "metadata": {
        "id": "EwRwkvJ0Sgiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gumbel_dist = torch.distributions.gumbel.Gumbel(torch.tensor(0.0), torch.tensor(1.0))\n",
        "\n",
        "soft_recall_vector  = torch.empty((0), dtype=torch.float32)\n",
        "hard_derivative_recall_vector =  torch.empty((0,N), dtype=torch.float32)\n",
        "tau = 0.1"
      ],
      "metadata": {
        "id": "qRej55XmShwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(N_iter):\n",
        "  gumbels = gumbel_dist.sample(random_logits.size())\n",
        "  #print(\"gumbels:\",gumbels)\n",
        "  logits_perturbed = random_logits + gumbels\n",
        "  y_soft = torch.softmax(logits_perturbed / tau, dim=1)\n",
        "  y_soft_final = y_soft[:,1]\n",
        "  #print(\"y_soft_final:\", y_soft_final)\n",
        "  y_hard = torch.argmax(logits_perturbed,dim =1)\n",
        "  #print(\"y_hard:\",y_hard)\n",
        "\n",
        "  #y_soft_final = torch.unsqueeze(y_soft_final, 1)\n",
        "  #y_hard = torch.unsqueeze(y_hard, 1)\n",
        "\n",
        "  Recall_soft = torch.sum(torch.mul(y_soft_final, predicted_class))/torch.sum(y_soft_final)\n",
        "  #print(\"Recall_soft:\", Recall_soft)\n",
        "  soft_recall_vector  = torch.cat((soft_recall_vector,(Recall_soft).view(1)),0)\n",
        "  #print(\"soft_recall_vector:\", soft_recall_vector)\n",
        "\n",
        "  Recall_hard_numerator = torch.sum(torch.mul(y_hard, predicted_class))\n",
        "  #print(\"Recall_hard_numerator:\", Recall_hard_numerator)\n",
        "  Recall_hard_denominator = torch.sum(y_hard)\n",
        "  #print(\"Recall_hard_denominator:\",Recall_hard_denominator)\n",
        "  hard_derivative_recall =  torch.empty((0), dtype=torch.float32)\n",
        "\n",
        "  for j in range(N):\n",
        "       hard_derivative_recall_one =  ((Recall_hard_numerator - y_hard[j]*predicted_class[j]+predicted_class[j])/(Recall_hard_denominator-y_hard[j]+1))-((Recall_hard_numerator - y_hard[j]*predicted_class[j])/(Recall_hard_denominator-y_hard[j]))\n",
        "       #print(\"hard_derivative_recall_one:\", hard_derivative_recall_one)\n",
        "       hard_derivative_recall  = torch.cat((hard_derivative_recall,(hard_derivative_recall_one).view(1)),0)\n",
        "       #print(\"hard_derivative_recall:\", hard_derivative_recall)\n",
        "       hard_derivative_recall_unseq = hard_derivative_recall.unsqueeze(0)\n",
        "  hard_derivative_recall_vector =   torch.cat((hard_derivative_recall_vector,hard_derivative_recall_unseq),0)\n",
        "  #print(\"hard_derivative_recall_vector:\", hard_derivative_recall_vector)\n"
      ],
      "metadata": {
        "id": "fsV7AUMLSoMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soft_racall_final = torch.mean(soft_recall_vector)\n",
        "soft_racall_final.backward()\n",
        "soft_racall_final_gradient = random_logits.grad[:,1]\n"
      ],
      "metadata": {
        "id": "zultegRHKH7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(hard_derivative_recall_vector)\n",
        "hard_recall_gradient_vector = hard_derivative_recall_vector.mean(dim=0)\n",
        "probabilities = torch.softmax(random_logits, dim=1)\n",
        "probabilities_multiplied = torch.prod(probabilities, dim=1)\n",
        "hard_recall_gradient_vector_success = probabilities_multiplied * hard_recall_gradient_vector\n",
        "print(hard_recall_gradient_vector_success)\n",
        "print(soft_racall_final_gradient)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FpofvAuXL-j",
        "outputId": "b4e31af5-3130-472d-dd25-8bd44fb052c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1.6756e-04, -1.1284e-04, -2.5324e-04,  2.2069e-04,  2.3845e-04,\n",
            "        -1.6027e-04, -2.4354e-04,  1.4240e-04,  7.1172e-05, -2.3407e-04,\n",
            "        -1.9863e-04,  2.2903e-04, -2.3454e-04,  2.2174e-04,  1.5670e-04,\n",
            "        -2.5284e-04, -1.8328e-04,  2.3686e-04,  2.4502e-04, -2.4016e-04,\n",
            "        -3.8414e-05,  1.7981e-04,  2.4753e-04, -2.5192e-04, -1.1610e-04,\n",
            "         2.1551e-04, -1.5469e-04,  1.9199e-04, -4.4106e-05,  4.6276e-05,\n",
            "         9.7621e-05,  9.3777e-05, -7.4688e-05, -2.4580e-04,  8.7760e-05,\n",
            "         2.0467e-04, -2.0690e-04, -2.0262e-04,  1.6611e-04,  1.3942e-04,\n",
            "        -1.3089e-04,  1.9627e-04, -1.5432e-04,  2.4667e-04, -5.6775e-05,\n",
            "        -2.4505e-04,  2.3247e-04, -3.8994e-05,  2.1263e-04, -2.2859e-04,\n",
            "        -2.1723e-04, -1.2389e-04, -2.5105e-04, -8.6189e-05,  1.2173e-04,\n",
            "        -1.5979e-04, -1.9355e-04, -1.3136e-04,  1.7705e-04, -1.6525e-04,\n",
            "        -7.3844e-05,  1.5740e-04,  1.3976e-04,  7.4804e-05,  2.4186e-04,\n",
            "         1.6908e-04, -1.9646e-04, -2.4296e-04, -2.5358e-04,  1.6356e-04,\n",
            "        -1.8549e-04, -1.2163e-04,  2.9295e-05,  2.1496e-04, -5.3642e-05,\n",
            "         1.6345e-04, -7.0187e-05, -1.5566e-04, -2.5353e-04,  1.2059e-04,\n",
            "        -1.1402e-04,  1.1675e-04,  1.2464e-04,  2.0551e-04, -2.1709e-04,\n",
            "         2.0972e-04,  1.6332e-04, -2.5354e-04,  1.4703e-04,  9.6392e-05,\n",
            "        -1.5018e-04,  2.3493e-04, -2.3021e-04,  2.3811e-04,  2.4419e-04,\n",
            "         1.1484e-04, -2.3026e-04,  2.0938e-04, -5.0203e-05,  2.2826e-04,\n",
            "         2.1641e-04, -2.4818e-04, -2.0276e-04, -2.5232e-04, -2.5004e-04,\n",
            "        -2.5127e-04, -2.5367e-04,  1.4534e-04,  1.3595e-04,  1.5276e-04,\n",
            "         1.8826e-04, -2.2598e-04,  2.2851e-04,  2.3506e-04,  2.2718e-04,\n",
            "        -2.2287e-04,  1.9961e-04, -2.0194e-04, -2.5183e-04,  2.1780e-04,\n",
            "         4.5216e-05,  2.2381e-04,  2.3609e-04,  2.4324e-04,  1.7640e-04,\n",
            "         1.9359e-04,  2.2841e-04, -2.5055e-04, -3.9040e-05, -2.4209e-04,\n",
            "        -2.1342e-04,  1.7551e-04,  1.5671e-04, -1.3053e-04, -1.3033e-04,\n",
            "        -1.2510e-04, -2.4929e-04, -2.2558e-04, -2.4481e-04,  1.3861e-04,\n",
            "         1.9447e-04, -5.7403e-05,  1.4996e-04,  2.4152e-04,  2.4417e-04,\n",
            "         1.8926e-04, -2.4669e-04, -2.1385e-04,  2.2952e-04, -5.0916e-05,\n",
            "        -2.0317e-04, -1.3770e-04,  1.8466e-04, -2.0034e-04, -2.1924e-04,\n",
            "        -1.6634e-04,  1.7415e-04, -2.2704e-04,  2.4743e-04,  2.4523e-04,\n",
            "         2.1757e-04, -1.0399e-04,  2.3394e-04,  1.6034e-04,  1.5679e-04,\n",
            "        -2.2423e-04, -2.2714e-04, -2.4821e-04, -1.9545e-04, -8.3919e-05,\n",
            "         1.3943e-04,  1.9515e-04, -2.4308e-04,  1.8856e-04, -1.0032e-04,\n",
            "         1.3322e-04, -2.2544e-04, -2.4917e-04,  2.0641e-04,  2.4662e-04,\n",
            "        -1.3360e-04,  2.4266e-04, -3.8276e-05, -2.4179e-04, -1.9713e-04,\n",
            "         1.9736e-04,  1.4810e-04, -2.5170e-04, -1.9547e-04,  4.9005e-05,\n",
            "         1.1600e-04,  2.4669e-04,  2.4539e-04,  6.0026e-05,  1.4085e-04,\n",
            "        -1.6913e-04,  2.2287e-04,  2.1678e-04, -1.9836e-04, -1.1130e-04,\n",
            "        -1.7984e-04,  1.8721e-04, -2.0906e-04,  4.1930e-05, -2.4171e-04,\n",
            "        -2.4392e-04,  2.4297e-04, -1.0848e-04,  2.3952e-04,  2.3988e-04,\n",
            "        -2.4737e-04,  2.4727e-04,  2.4404e-04,  1.4027e-04, -1.9291e-04,\n",
            "         2.3507e-04, -2.4737e-04,  2.2196e-04, -2.0082e-04,  7.8213e-05,\n",
            "        -2.1045e-04, -2.2807e-04, -2.5011e-04,  1.5369e-04, -1.3262e-04,\n",
            "         1.9212e-04, -2.0230e-04,  1.9345e-04, -2.0521e-04, -1.1357e-04,\n",
            "         2.1604e-04, -9.5341e-05, -1.9498e-04,  1.4371e-04, -1.3278e-04,\n",
            "         1.0672e-04,  1.5850e-04, -1.8433e-04, -2.5293e-04, -2.5368e-04,\n",
            "         2.2838e-04, -2.4703e-04, -2.5202e-04,  2.3237e-04, -2.3744e-04,\n",
            "         1.3139e-04,  4.1417e-05,  2.2992e-04,  7.2813e-05, -2.4472e-04,\n",
            "         2.0261e-04, -2.5349e-04,  9.6785e-05, -1.1525e-04, -2.5104e-04,\n",
            "         1.4603e-04, -2.4788e-04,  2.0552e-04,  1.9892e-04, -2.2745e-04,\n",
            "         2.1299e-04, -2.3751e-04, -1.8297e-04, -2.3593e-04,  2.2134e-04,\n",
            "        -2.3421e-04,  2.2098e-04,  2.1871e-04,  1.0077e-04, -1.5092e-04,\n",
            "         1.7928e-04, -2.5081e-04,  3.1723e-05, -2.4986e-04, -2.0183e-04,\n",
            "        -1.2793e-04,  2.3094e-04, -2.2078e-04,  2.3904e-04,  2.4753e-04,\n",
            "         1.6581e-04,  2.3743e-04,  3.7095e-05,  2.4749e-04,  8.0273e-05,\n",
            "        -2.1229e-04,  2.0720e-04, -2.5205e-04, -2.5365e-04,  2.4704e-04,\n",
            "        -1.6844e-04, -2.0084e-04,  2.4753e-04,  2.4379e-04, -2.2222e-04,\n",
            "         7.4584e-05,  2.4548e-04, -7.1043e-05, -3.9616e-05,  2.4505e-04,\n",
            "        -1.8344e-04,  1.5528e-04, -1.3577e-04, -2.4746e-04,  1.9043e-04,\n",
            "         1.9532e-05, -2.3590e-04, -2.5235e-04, -9.4424e-05, -2.5179e-04,\n",
            "        -1.5603e-04, -2.5361e-04, -2.3732e-04,  2.4042e-04, -1.4981e-04,\n",
            "        -8.5839e-05, -1.9924e-04,  2.1305e-04,  2.0590e-04,  1.2322e-04,\n",
            "        -5.9658e-05,  7.1506e-05,  2.4091e-04,  2.2009e-04, -1.2793e-04,\n",
            "        -2.1003e-04, -2.4581e-04, -1.8998e-04, -1.7072e-04, -2.5342e-04,\n",
            "         1.6216e-04,  9.1230e-05,  1.1825e-04,  2.3141e-04,  2.4756e-04,\n",
            "         2.3378e-04, -1.9936e-04, -2.2112e-04, -2.5366e-04,  8.6894e-05,\n",
            "         1.8990e-04, -2.4823e-04,  1.7029e-04, -2.1624e-04,  1.7092e-04,\n",
            "        -9.1226e-05, -1.4600e-04, -3.1195e-05, -2.2629e-04,  2.2844e-04,\n",
            "        -2.0910e-04, -2.3055e-04, -2.2580e-04, -9.5734e-05, -1.7351e-04,\n",
            "         2.0663e-04, -1.7069e-04, -2.5362e-04,  2.2692e-04, -5.9849e-05,\n",
            "         2.3905e-04, -1.6378e-04, -2.2905e-04, -2.5158e-04,  2.1053e-04,\n",
            "         2.4057e-04,  2.4464e-04, -9.3778e-05, -2.5367e-04, -2.3287e-04,\n",
            "         2.1424e-04, -2.0229e-04, -9.6604e-06,  2.4227e-04, -2.3443e-04,\n",
            "         1.5284e-04,  1.3235e-04,  1.6634e-04, -4.3375e-05,  2.3798e-04,\n",
            "        -2.2621e-04,  2.4650e-04, -5.7680e-05,  1.1061e-04, -2.4257e-04,\n",
            "        -5.5542e-05,  2.3547e-04,  1.8916e-04, -2.5341e-04,  2.4661e-04,\n",
            "        -2.5367e-04, -2.4914e-04, -8.5320e-05,  2.4442e-04,  2.1334e-04,\n",
            "         5.1263e-05,  1.6033e-04, -1.2141e-04,  2.4686e-04,  2.4610e-04,\n",
            "        -2.5367e-04,  1.1671e-04, -2.5087e-04, -2.3878e-04, -2.2735e-04,\n",
            "         2.1682e-04,  1.1002e-04,  3.1665e-05,  2.1549e-04, -1.2571e-04,\n",
            "        -1.5423e-04,  2.0362e-04,  2.2140e-04,  2.3885e-04, -1.6694e-04,\n",
            "         2.2648e-04, -1.7150e-04,  2.1175e-04,  1.5388e-04,  2.3495e-04,\n",
            "        -1.2534e-04, -3.4152e-05, -1.5704e-04, -6.1638e-05, -2.5362e-04,\n",
            "        -2.5205e-04,  2.2929e-04, -2.4681e-04,  4.9431e-05,  9.1000e-05,\n",
            "         2.2500e-04,  2.2083e-04, -2.5064e-04,  1.2319e-04,  2.3965e-04,\n",
            "        -2.4515e-04, -2.4994e-04,  6.0878e-05, -2.5370e-04, -1.4652e-04,\n",
            "         1.3487e-04,  8.8271e-05,  8.6658e-05, -2.1029e-04, -2.5364e-04,\n",
            "         8.6567e-05,  2.4687e-04,  2.3630e-04, -2.5071e-04,  1.6051e-04,\n",
            "         8.8765e-05, -2.3985e-04,  1.9345e-04, -2.5297e-04,  2.2504e-04,\n",
            "         2.3996e-04,  1.1519e-04,  1.1607e-04,  1.8104e-04,  1.7393e-04,\n",
            "        -1.2301e-04,  1.9722e-04,  2.1294e-04,  2.3892e-04, -8.5009e-05,\n",
            "         2.4524e-04,  1.1095e-04, -1.7930e-04, -2.4302e-04, -2.5319e-04,\n",
            "         2.4511e-04, -1.9633e-04,  2.2912e-04, -2.1536e-04,  4.9158e-05,\n",
            "         2.4208e-04,  1.0352e-04,  2.4719e-04, -2.0255e-04, -2.2749e-04,\n",
            "        -1.5037e-04,  2.3275e-04,  2.4724e-04, -2.4688e-04, -2.5214e-04,\n",
            "         9.5877e-05, -1.9256e-04, -2.3274e-04, -2.1541e-04,  1.1824e-04,\n",
            "        -1.6187e-04,  1.1703e-04, -8.8154e-05,  2.2529e-04, -1.8828e-04,\n",
            "         2.4721e-04, -1.8833e-04, -1.7548e-04, -2.5335e-04,  2.4687e-04,\n",
            "         5.8951e-05,  1.0840e-04, -1.5775e-04, -1.6697e-04,  1.9859e-04,\n",
            "         2.4757e-04, -2.2515e-04, -9.2172e-05, -1.7007e-04, -2.0055e-04,\n",
            "        -2.5217e-04,  1.2963e-04,  1.8200e-04, -2.3996e-04, -2.4008e-04,\n",
            "         2.4748e-04,  1.1379e-04, -1.3941e-04, -1.1600e-04,  2.4633e-04,\n",
            "         2.0474e-04, -2.0369e-04,  1.3570e-04,  1.5050e-04,  1.3171e-04,\n",
            "        -1.3921e-04,  5.7988e-05, -1.6153e-04,  6.8252e-05,  2.2185e-04,\n",
            "        -2.2360e-04, -2.4407e-04,  2.1599e-04, -2.5279e-04, -2.1125e-04,\n",
            "         2.2758e-05, -2.3849e-04, -3.3311e-05, -2.4700e-04, -1.2257e-04,\n",
            "        -2.3185e-04,  2.0051e-04,  2.2850e-04, -1.1916e-04,  1.3951e-04,\n",
            "        -1.4902e-04,  8.5816e-05,  1.5221e-04, -1.7847e-04,  2.3931e-04,\n",
            "        -2.3910e-04,  2.4648e-04, -2.5164e-04,  2.3208e-04,  2.0420e-04,\n",
            "         5.9642e-05, -4.4684e-05,  2.0395e-04,  2.4130e-04, -2.0515e-04,\n",
            "         1.3239e-04,  2.1049e-04,  1.4904e-04,  1.4405e-04, -1.1728e-04,\n",
            "        -4.5208e-05,  8.6347e-05, -1.0931e-04, -2.2689e-04,  5.2657e-05,\n",
            "        -1.8447e-04,  6.5480e-05, -1.8826e-04,  1.5062e-04, -1.1341e-04,\n",
            "         2.2689e-04,  1.7587e-04, -2.5063e-04,  1.2027e-04,  2.4402e-04,\n",
            "         1.4644e-04, -2.4846e-04, -2.2929e-04, -1.7510e-04,  1.3488e-04,\n",
            "         1.4870e-04,  1.9403e-04,  1.1529e-04, -2.0487e-04, -4.3144e-05,\n",
            "         2.0977e-04,  2.0049e-04,  1.5228e-04, -1.1621e-04, -2.5279e-04,\n",
            "        -2.3541e-04, -2.3380e-04,  1.1970e-04, -2.5254e-04,  1.1413e-04,\n",
            "        -1.0088e-04,  2.3593e-04, -2.1425e-04,  2.4530e-04, -7.0798e-05,\n",
            "        -1.9388e-04, -1.6802e-04,  2.2262e-04, -2.3601e-04,  1.6667e-04,\n",
            "         2.0821e-04, -3.9539e-05, -7.0814e-05,  1.7269e-04,  9.2705e-05,\n",
            "         1.8618e-04,  1.9075e-04,  2.1585e-04, -2.0664e-04,  1.2667e-04,\n",
            "         2.4048e-04,  4.9423e-05, -2.5335e-04, -2.4988e-04, -1.0953e-04,\n",
            "         2.0697e-04,  9.8480e-05,  1.3675e-04, -1.9152e-04,  4.0890e-05,\n",
            "         2.4660e-04, -2.3845e-04, -1.2203e-04, -2.2228e-04,  4.7273e-05,\n",
            "         1.0892e-04,  1.8019e-04, -2.4244e-05, -1.2539e-04,  2.2953e-04,\n",
            "         1.2125e-04,  2.1899e-04, -1.8473e-04, -2.3807e-04, -2.0400e-04,\n",
            "        -1.4237e-04, -1.2884e-04, -1.2848e-04, -1.3848e-04, -3.7754e-05,\n",
            "         2.0903e-04,  2.4187e-04, -2.4438e-04,  2.3919e-04,  2.1201e-04,\n",
            "         2.2857e-04,  2.4225e-04,  2.3969e-04,  2.0599e-04, -2.0431e-04,\n",
            "         2.1018e-04, -2.4910e-04,  2.4613e-04, -2.5351e-04,  2.4691e-04,\n",
            "        -1.7157e-04,  9.6274e-05, -1.4409e-04, -2.4523e-04,  2.2886e-04,\n",
            "         3.3496e-05,  1.8322e-04,  1.8087e-04,  2.0446e-04, -2.3414e-04,\n",
            "         6.6629e-05, -1.9687e-04, -2.4493e-04,  6.3668e-05,  2.2640e-04,\n",
            "         2.2682e-04,  2.4322e-04, -1.9074e-04,  1.7741e-04,  2.2301e-04,\n",
            "         1.8455e-04, -2.5281e-04,  2.0867e-04, -2.2258e-05, -2.5341e-04,\n",
            "        -4.6217e-05, -2.3220e-04, -1.3579e-04,  5.0595e-05,  2.4071e-04,\n",
            "        -2.4865e-04,  1.0071e-04, -1.9209e-04,  2.0813e-04,  1.1499e-04,\n",
            "        -2.4228e-04,  7.3533e-05, -1.3784e-04,  1.6976e-04,  6.3872e-05,\n",
            "        -3.5727e-05, -4.6264e-05,  1.0092e-04,  2.4342e-04, -3.6326e-05,\n",
            "        -2.4244e-04,  1.6349e-04, -2.4289e-04, -1.9039e-04, -1.7734e-04,\n",
            "         1.7164e-04,  1.5963e-04, -1.5963e-04, -2.0405e-04,  2.4737e-04,\n",
            "        -7.8499e-05,  1.5987e-04, -1.6964e-04, -2.5357e-04, -2.4914e-04,\n",
            "        -2.5133e-04,  1.2863e-04, -7.8497e-05,  2.0977e-04, -2.2434e-04,\n",
            "        -2.4779e-04,  2.1286e-04,  2.3583e-04, -2.5232e-04,  2.4568e-04,\n",
            "         1.5694e-04, -2.4572e-04, -2.4315e-04, -7.1215e-05, -1.6404e-04,\n",
            "         2.2218e-04, -1.6502e-04,  2.0783e-04,  2.3886e-05,  1.6402e-04,\n",
            "         2.4641e-04, -1.7615e-04,  9.8700e-05, -2.3919e-04,  4.5202e-05,\n",
            "        -1.2414e-04, -9.6469e-05, -2.3226e-04,  2.3619e-04,  2.4669e-04,\n",
            "        -1.9198e-04, -1.7727e-04, -2.1937e-04, -1.6150e-05,  2.4735e-04,\n",
            "        -1.1156e-04, -2.1230e-04,  1.2902e-04, -2.5369e-04, -1.5180e-04,\n",
            "        -1.6353e-04,  1.7041e-04, -2.5366e-04, -2.5355e-04, -2.2399e-04,\n",
            "         1.6004e-04, -2.1551e-04, -2.5337e-04,  2.4636e-04, -1.8392e-04,\n",
            "        -2.2083e-04,  1.9310e-04,  1.9929e-04,  5.9237e-05, -8.6663e-05,\n",
            "        -1.6691e-04, -2.1193e-04,  1.5909e-04,  5.7028e-05,  2.1254e-04,\n",
            "         4.2311e-05, -2.4084e-04, -1.6175e-04, -2.5206e-04, -2.2313e-04,\n",
            "         2.4533e-04, -8.1898e-05,  2.0777e-04, -2.0881e-04, -2.4238e-04,\n",
            "         2.2475e-04,  2.3738e-04, -9.1711e-05,  1.4501e-04, -2.2681e-04,\n",
            "         2.4268e-04, -2.5262e-04,  2.1695e-04,  2.0056e-04,  2.4198e-04,\n",
            "         1.5808e-04, -1.9754e-04,  2.4707e-04, -8.0429e-05, -2.4788e-04,\n",
            "        -2.3666e-04, -2.3322e-04,  2.3522e-04,  1.3641e-04, -2.3355e-05,\n",
            "         2.0090e-04, -5.7743e-05,  2.1856e-04, -2.2887e-04,  1.4148e-04,\n",
            "         2.2470e-04,  1.3288e-04, -1.8091e-04,  1.2079e-04, -1.6791e-04,\n",
            "        -2.5243e-04, -2.5290e-04, -1.6974e-04,  1.6640e-04, -2.3835e-04,\n",
            "         2.2846e-04, -1.8029e-04, -1.9805e-04, -2.4432e-04, -2.5369e-04,\n",
            "         2.2281e-04,  6.8252e-05,  1.7470e-04,  1.7102e-04, -1.0118e-04,\n",
            "        -1.9696e-04,  1.7749e-04, -2.4139e-04,  4.9509e-05, -5.2908e-05,\n",
            "        -2.3691e-04,  1.4491e-04,  1.9675e-04, -1.2620e-04, -2.3981e-04,\n",
            "         1.1718e-04,  1.0916e-04, -4.3184e-05, -2.3834e-04,  1.0272e-04,\n",
            "        -9.5307e-05,  7.3409e-05, -8.6823e-05, -1.2636e-04, -2.5291e-04,\n",
            "        -1.3599e-04,  2.3449e-04, -1.5709e-04,  2.1657e-04,  1.2923e-04,\n",
            "        -2.4174e-04,  2.3436e-04, -1.8705e-04,  2.4739e-04,  1.8687e-04,\n",
            "        -2.5130e-04, -3.0525e-05, -2.4746e-04,  2.1098e-04,  1.1766e-04,\n",
            "         2.4003e-04, -8.6338e-05,  1.5023e-04, -2.4868e-04, -2.5305e-04,\n",
            "         7.1001e-05, -2.4352e-04,  2.1139e-04,  2.3206e-04,  2.4093e-04,\n",
            "         2.4367e-04,  1.4885e-04, -2.1201e-04,  2.4757e-04,  2.4659e-04,\n",
            "         6.8724e-05, -8.3422e-05,  8.8674e-05,  2.4000e-04,  1.6813e-04,\n",
            "         9.7431e-05,  1.5520e-04, -2.3607e-04,  2.4405e-04,  1.4322e-04,\n",
            "        -2.2658e-04, -2.5347e-04,  1.4383e-04, -2.3569e-04, -2.5250e-04,\n",
            "         2.4333e-04, -1.4915e-04,  2.0699e-04, -1.7724e-04,  2.4705e-04,\n",
            "        -3.9210e-05,  1.5260e-05,  2.2062e-04,  7.7912e-05,  1.6369e-04,\n",
            "        -1.5556e-04, -8.3049e-05,  1.2070e-04,  1.8548e-04,  5.8059e-05,\n",
            "         1.5877e-04, -1.3242e-04, -1.5384e-04, -2.4188e-04,  2.4736e-04,\n",
            "         3.8092e-05, -2.4440e-04,  2.0559e-04, -1.7997e-04,  1.7552e-04,\n",
            "        -2.1847e-04, -1.8037e-04,  1.5611e-04, -1.4392e-04,  2.0162e-04,\n",
            "         2.1455e-04, -9.5046e-05,  2.0420e-04, -2.4295e-04, -2.5099e-04,\n",
            "        -2.5336e-04, -1.9793e-04, -2.3522e-04,  2.0560e-04,  2.3689e-04,\n",
            "         1.4976e-04, -2.5302e-04,  2.1966e-04, -2.5200e-04, -2.5312e-04,\n",
            "         2.4740e-04, -1.3397e-04,  1.0055e-04, -2.4912e-04,  2.4612e-04,\n",
            "        -2.5309e-04, -1.9490e-04, -1.9237e-04,  2.0204e-04, -2.0330e-04,\n",
            "         1.9447e-04,  2.0582e-04, -2.5347e-04, -7.2459e-05, -1.1393e-04,\n",
            "        -2.3779e-04, -1.4282e-04, -2.1354e-04,  2.4655e-04,  1.2889e-04,\n",
            "        -1.0142e-04,  2.4116e-04,  1.2134e-04, -2.4890e-04,  7.4215e-05,\n",
            "         1.1390e-04,  2.4752e-04, -3.4303e-05, -2.3773e-04,  2.4549e-04,\n",
            "        -2.3096e-04, -2.3858e-04, -2.3120e-04,  1.4221e-04, -1.3911e-04,\n",
            "         2.0673e-04, -1.7746e-04,  2.3632e-04,  1.4562e-04, -2.2943e-04,\n",
            "        -1.9589e-04,  2.4737e-04,  2.4063e-04,  2.4706e-04, -1.3691e-04,\n",
            "        -2.3833e-04, -2.5327e-04,  1.3821e-04,  2.0701e-04,  2.3483e-04],\n",
            "       grad_fn=<MulBackward0>)\n",
            "tensor([ 1.4402e-04, -1.1863e-04, -2.4215e-04,  2.5737e-04,  2.8779e-04,\n",
            "        -1.5484e-04, -2.2327e-04,  1.3757e-04,  8.1937e-05, -2.5371e-04,\n",
            "        -2.0593e-04,  2.3113e-04, -2.3780e-04,  1.9572e-04,  1.5617e-04,\n",
            "        -2.4116e-04, -1.9497e-04,  2.4038e-04,  2.4088e-04, -2.4909e-04,\n",
            "        -5.3922e-05,  1.8854e-04,  2.5167e-04, -2.6599e-04, -1.2899e-04,\n",
            "         1.9713e-04, -1.5283e-04,  2.0694e-04, -4.5427e-05,  5.1900e-05,\n",
            "         1.0042e-04,  1.1307e-04, -6.4625e-05, -2.3515e-04,  9.2160e-05,\n",
            "         2.0418e-04, -2.3355e-04, -2.1047e-04,  1.7367e-04,  1.6307e-04,\n",
            "        -1.3743e-04,  1.9774e-04, -1.8574e-04,  2.5824e-04, -5.9745e-05,\n",
            "        -2.6718e-04,  2.5321e-04, -2.3806e-05,  2.1014e-04, -2.1733e-04,\n",
            "        -2.0636e-04, -1.1333e-04, -2.2842e-04, -8.5637e-05,  9.0525e-05,\n",
            "        -1.5971e-04, -2.1715e-04, -1.1320e-04,  1.6352e-04, -1.6675e-04,\n",
            "        -7.9761e-05,  1.7760e-04,  1.3773e-04,  9.3149e-05,  2.0270e-04,\n",
            "         1.6839e-04, -2.2166e-04, -2.2960e-04, -2.1767e-04,  1.8373e-04,\n",
            "        -1.7635e-04, -1.3510e-04,  3.0715e-05,  2.3469e-04, -4.1299e-05,\n",
            "         1.6911e-04, -5.9773e-05, -1.5706e-04, -2.6286e-04,  1.2984e-04,\n",
            "        -9.4156e-05,  1.3417e-04,  1.2347e-04,  2.0190e-04, -2.0514e-04,\n",
            "         2.0942e-04,  1.8880e-04, -2.6176e-04,  1.4467e-04,  8.6956e-05,\n",
            "        -1.3764e-04,  2.6342e-04, -2.4972e-04,  2.8251e-04,  2.2465e-04,\n",
            "         1.1681e-04, -2.2905e-04,  2.0735e-04, -6.5293e-05,  2.1160e-04,\n",
            "         2.0256e-04, -2.4503e-04, -1.8728e-04, -2.5935e-04, -2.6545e-04,\n",
            "        -2.8019e-04, -2.6485e-04,  1.1843e-04,  1.5713e-04,  1.6307e-04,\n",
            "         2.0173e-04, -2.0395e-04,  2.0205e-04,  2.2549e-04,  2.1276e-04,\n",
            "        -2.0983e-04,  2.0240e-04, -1.6570e-04, -2.5096e-04,  2.4162e-04,\n",
            "         5.8195e-05,  2.2544e-04,  2.0277e-04,  2.1531e-04,  1.9524e-04,\n",
            "         1.9073e-04,  2.2201e-04, -2.6537e-04, -4.2659e-05, -2.4726e-04,\n",
            "        -2.3947e-04,  1.3290e-04,  1.4336e-04, -1.1791e-04, -1.2025e-04,\n",
            "        -1.3405e-04, -2.6171e-04, -2.5142e-04, -2.4220e-04,  1.4072e-04,\n",
            "         1.8226e-04, -5.5196e-05,  1.4266e-04,  2.0463e-04,  2.5631e-04,\n",
            "         1.6979e-04, -2.0794e-04, -1.9836e-04,  2.2373e-04, -3.6859e-05,\n",
            "        -2.0790e-04, -1.4529e-04,  1.9431e-04, -1.7657e-04, -2.1567e-04,\n",
            "        -1.7839e-04,  1.5380e-04, -2.3314e-04,  2.2894e-04,  2.2965e-04,\n",
            "         2.0356e-04, -9.2964e-05,  2.0437e-04,  1.5684e-04,  1.6626e-04,\n",
            "        -2.1295e-04, -2.1622e-04, -2.5752e-04, -1.8341e-04, -9.0213e-05,\n",
            "         1.5250e-04,  1.9895e-04, -2.4728e-04,  1.9464e-04, -7.2245e-05,\n",
            "         1.5704e-04, -2.3183e-04, -2.3763e-04,  2.3167e-04,  2.4234e-04,\n",
            "        -1.1424e-04,  2.4639e-04, -4.1482e-05, -2.5601e-04, -2.0460e-04,\n",
            "         1.5600e-04,  1.5791e-04, -2.7249e-04, -1.6871e-04,  4.7794e-05,\n",
            "         1.2927e-04,  2.4383e-04,  2.4370e-04,  7.1919e-05,  1.1988e-04,\n",
            "        -1.9516e-04,  2.5232e-04,  2.2253e-04, -2.0447e-04, -1.2704e-04,\n",
            "        -1.5569e-04,  2.0217e-04, -1.6103e-04,  3.3882e-05, -2.1874e-04,\n",
            "        -2.4486e-04,  2.2409e-04, -1.1979e-04,  2.6511e-04,  2.2918e-04,\n",
            "        -2.6871e-04,  2.2657e-04,  2.6139e-04,  1.4078e-04, -2.1390e-04,\n",
            "         2.1490e-04, -2.3457e-04,  1.9409e-04, -1.8479e-04,  9.3916e-05,\n",
            "        -1.9260e-04, -2.2486e-04, -2.5218e-04,  1.2607e-04, -1.1244e-04,\n",
            "         2.1209e-04, -2.2738e-04,  1.9757e-04, -1.8284e-04, -1.1861e-04,\n",
            "         1.9147e-04, -1.0304e-04, -1.9531e-04,  1.4587e-04, -1.6229e-04,\n",
            "         8.9759e-05,  1.5415e-04, -1.8605e-04, -2.6140e-04, -2.8973e-04,\n",
            "         2.4055e-04, -2.6788e-04, -2.7708e-04,  2.7250e-04, -2.4367e-04,\n",
            "         1.0994e-04,  4.3577e-05,  2.2712e-04,  6.1731e-05, -2.4967e-04,\n",
            "         2.0739e-04, -2.4315e-04,  8.5865e-05, -1.1587e-04, -2.5238e-04,\n",
            "         1.5746e-04, -2.4345e-04,  1.9824e-04,  2.0383e-04, -2.3786e-04,\n",
            "         2.1581e-04, -2.5650e-04, -1.6422e-04, -2.5117e-04,  1.7557e-04,\n",
            "        -2.3959e-04,  1.8840e-04,  2.1084e-04,  9.6058e-05, -1.4109e-04,\n",
            "         1.7512e-04, -2.4134e-04,  2.8376e-05, -2.3797e-04, -2.2577e-04,\n",
            "        -1.6378e-04,  2.3313e-04, -1.8042e-04,  2.4532e-04,  2.9729e-04,\n",
            "         1.7427e-04,  2.5323e-04,  3.9642e-05,  2.2587e-04,  7.6127e-05,\n",
            "        -2.2568e-04,  1.9098e-04, -2.1906e-04, -2.2720e-04,  2.4360e-04,\n",
            "        -1.4464e-04, -1.8526e-04,  2.5527e-04,  2.1943e-04, -2.2159e-04,\n",
            "         9.6632e-05,  2.0683e-04, -7.4093e-05, -4.1396e-05,  2.4084e-04,\n",
            "        -1.9293e-04,  1.4976e-04, -1.3808e-04, -2.4107e-04,  1.6756e-04,\n",
            "         1.9249e-05, -2.3922e-04, -2.4678e-04, -9.4661e-05, -2.3993e-04,\n",
            "        -1.4166e-04, -2.4677e-04, -2.3837e-04,  2.4314e-04, -1.9477e-04,\n",
            "        -9.8036e-05, -1.9086e-04,  2.2932e-04,  1.9136e-04,  1.1745e-04,\n",
            "        -7.4746e-05,  5.0783e-05,  2.3472e-04,  2.0294e-04, -1.2590e-04,\n",
            "        -2.2211e-04, -2.4491e-04, -2.0188e-04, -1.7076e-04, -2.5848e-04,\n",
            "         1.7127e-04,  8.6018e-05,  1.4791e-04,  2.2264e-04,  2.8592e-04,\n",
            "         2.3943e-04, -2.0058e-04, -1.9265e-04, -2.4604e-04,  8.4408e-05,\n",
            "         1.9439e-04, -2.5888e-04,  1.6660e-04, -1.9961e-04,  1.6024e-04,\n",
            "        -7.9713e-05, -1.6178e-04, -3.4420e-05, -2.1263e-04,  2.1887e-04,\n",
            "        -2.0942e-04, -2.3042e-04, -2.0248e-04, -8.1835e-05, -1.9639e-04,\n",
            "         2.3456e-04, -1.6845e-04, -2.5155e-04,  2.1604e-04, -5.5588e-05,\n",
            "         1.9914e-04, -1.5593e-04, -2.1303e-04, -2.5186e-04,  2.3770e-04,\n",
            "         2.3604e-04,  2.2185e-04, -1.0489e-04, -2.4305e-04, -2.5097e-04,\n",
            "         2.2382e-04, -1.6974e-04, -1.3863e-05,  2.4764e-04, -2.5205e-04,\n",
            "         1.6337e-04,  1.4208e-04,  1.9225e-04, -4.4355e-05,  2.5077e-04,\n",
            "        -2.1307e-04,  2.4951e-04, -8.1698e-05,  9.9592e-05, -2.4749e-04,\n",
            "        -6.3938e-05,  2.3855e-04,  1.8682e-04, -2.3559e-04,  2.5460e-04,\n",
            "        -2.6770e-04, -2.7641e-04, -7.6312e-05,  2.2574e-04,  2.0257e-04,\n",
            "         4.6700e-05,  1.8982e-04, -1.3108e-04,  2.4790e-04,  2.5059e-04,\n",
            "        -2.7086e-04,  1.4043e-04, -2.4186e-04, -2.4155e-04, -2.1410e-04,\n",
            "         2.0842e-04,  9.3858e-05,  2.7732e-05,  2.5617e-04, -1.3107e-04,\n",
            "        -1.5611e-04,  1.9693e-04,  2.8083e-04,  2.6469e-04, -1.7716e-04,\n",
            "         2.0572e-04, -1.5460e-04,  2.1335e-04,  1.2478e-04,  2.0716e-04,\n",
            "        -1.2324e-04, -4.0288e-05, -1.4067e-04, -7.2011e-05, -2.6035e-04,\n",
            "        -2.3676e-04,  2.3016e-04, -2.2779e-04,  4.9515e-05,  8.8673e-05,\n",
            "         2.2954e-04,  2.1265e-04, -2.6038e-04,  1.3650e-04,  2.3424e-04,\n",
            "        -2.5351e-04, -2.5387e-04,  6.6177e-05, -2.4142e-04, -1.4771e-04,\n",
            "         1.4950e-04,  8.6514e-05,  9.4908e-05, -1.7769e-04, -2.3033e-04,\n",
            "         9.6931e-05,  2.4035e-04,  2.2098e-04, -2.2194e-04,  1.5050e-04,\n",
            "         1.0842e-04, -2.4951e-04,  1.7340e-04, -2.2147e-04,  2.2726e-04,\n",
            "         2.0895e-04,  1.1606e-04,  1.0925e-04,  1.9165e-04,  1.9736e-04,\n",
            "        -1.3540e-04,  1.9243e-04,  1.9864e-04,  2.5110e-04, -7.0059e-05,\n",
            "         2.2438e-04,  1.2074e-04, -1.7174e-04, -2.3997e-04, -2.7765e-04,\n",
            "         2.2554e-04, -2.1056e-04,  2.1909e-04, -2.0487e-04,  4.3153e-05,\n",
            "         2.6809e-04,  1.1266e-04,  2.3818e-04, -1.8769e-04, -1.8412e-04,\n",
            "        -1.4591e-04,  2.1829e-04,  2.5098e-04, -2.0490e-04, -2.2484e-04,\n",
            "         9.2669e-05, -1.8345e-04, -2.8747e-04, -1.9105e-04,  1.2820e-04,\n",
            "        -1.9230e-04,  1.0587e-04, -7.7035e-05,  2.7056e-04, -1.7483e-04,\n",
            "         2.7828e-04, -1.8327e-04, -1.9008e-04, -2.4240e-04,  2.2753e-04,\n",
            "         6.5267e-05,  1.1095e-04, -1.6791e-04, -1.5894e-04,  1.7082e-04,\n",
            "         2.6468e-04, -2.3743e-04, -1.2302e-04, -1.5237e-04, -1.6865e-04,\n",
            "        -2.2934e-04,  1.1765e-04,  1.7209e-04, -2.2094e-04, -2.4594e-04,\n",
            "         2.3792e-04,  1.0288e-04, -1.3045e-04, -1.0533e-04,  2.4910e-04,\n",
            "         2.0438e-04, -1.7401e-04,  1.5702e-04,  1.4677e-04,  1.2909e-04,\n",
            "        -1.4077e-04,  5.8186e-05, -1.4830e-04,  6.0738e-05,  2.2740e-04,\n",
            "        -1.9036e-04, -2.4620e-04,  2.2075e-04, -2.6736e-04, -2.0618e-04,\n",
            "         2.2636e-05, -2.5183e-04, -2.7439e-05, -2.4159e-04, -1.0355e-04,\n",
            "        -2.0249e-04,  2.0601e-04,  2.5140e-04, -1.2701e-04,  1.5107e-04,\n",
            "        -1.3855e-04,  9.7434e-05,  1.5584e-04, -1.6462e-04,  2.2808e-04,\n",
            "        -2.1975e-04,  2.7258e-04, -2.5244e-04,  2.2136e-04,  1.8511e-04,\n",
            "         6.4666e-05, -2.8824e-05,  2.0779e-04,  2.3843e-04, -1.8999e-04,\n",
            "         1.3969e-04,  2.2288e-04,  1.5321e-04,  1.2899e-04, -1.1405e-04,\n",
            "        -4.3683e-05,  8.3829e-05, -1.2828e-04, -2.0724e-04,  6.5599e-05,\n",
            "        -1.8524e-04,  7.0785e-05, -2.0823e-04,  1.5785e-04, -9.4397e-05,\n",
            "         2.5626e-04,  1.3402e-04, -2.2862e-04,  1.1618e-04,  2.4311e-04,\n",
            "         1.6065e-04, -2.4491e-04, -2.2419e-04, -1.5943e-04,  1.5078e-04,\n",
            "         1.4601e-04,  2.2220e-04,  1.0317e-04, -1.7146e-04, -4.5592e-05,\n",
            "         2.3306e-04,  2.1999e-04,  1.4879e-04, -1.1251e-04, -2.2150e-04,\n",
            "        -2.4703e-04, -2.4884e-04,  8.8152e-05, -2.5485e-04,  1.2586e-04,\n",
            "        -1.1275e-04,  2.4878e-04, -2.0568e-04,  2.3618e-04, -5.7355e-05,\n",
            "        -2.1815e-04, -1.9321e-04,  2.4134e-04, -2.6107e-04,  1.3070e-04,\n",
            "         1.8556e-04, -5.1525e-05, -7.1742e-05,  1.6958e-04,  1.0780e-04,\n",
            "         2.1459e-04,  1.9443e-04,  2.3407e-04, -1.9125e-04,  1.2680e-04,\n",
            "         2.1263e-04,  4.2143e-05, -2.7754e-04, -2.5304e-04, -1.0132e-04,\n",
            "         1.7845e-04,  1.0162e-04,  1.4261e-04, -1.7799e-04,  5.4035e-05,\n",
            "         2.6996e-04, -2.5070e-04, -1.4596e-04, -2.1233e-04,  3.6734e-05,\n",
            "         1.0226e-04,  1.7318e-04, -3.2410e-05, -1.1849e-04,  2.2022e-04,\n",
            "         1.1466e-04,  2.2222e-04, -1.5768e-04, -2.0429e-04, -2.2849e-04,\n",
            "        -1.4789e-04, -1.2897e-04, -1.3560e-04, -1.6762e-04, -5.2103e-05,\n",
            "         2.1262e-04,  2.1632e-04, -2.3655e-04,  2.4462e-04,  1.9219e-04,\n",
            "         2.3055e-04,  2.1881e-04,  2.2097e-04,  2.4438e-04, -1.9594e-04,\n",
            "         2.2027e-04, -2.1562e-04,  2.5842e-04, -2.7973e-04,  2.7596e-04,\n",
            "        -1.7311e-04,  9.5159e-05, -1.5032e-04, -2.4644e-04,  2.4039e-04,\n",
            "         3.3726e-05,  2.0434e-04,  1.6856e-04,  2.0817e-04, -1.8002e-04,\n",
            "         6.4366e-05, -2.1062e-04, -2.4965e-04,  5.0545e-05,  2.3990e-04,\n",
            "         2.2442e-04,  2.2972e-04, -1.6826e-04,  1.7009e-04,  2.1328e-04,\n",
            "         2.1649e-04, -2.2432e-04,  2.0541e-04, -1.8514e-05, -2.6136e-04,\n",
            "        -5.8202e-05, -2.1880e-04, -1.4587e-04,  5.0070e-05,  2.5316e-04,\n",
            "        -3.0288e-04,  1.0835e-04, -2.1398e-04,  1.9195e-04,  1.0682e-04,\n",
            "        -2.3492e-04,  7.4808e-05, -1.6236e-04,  1.5967e-04,  5.8545e-05,\n",
            "        -3.6665e-05, -4.3981e-05,  1.1829e-04,  2.5866e-04, -2.1465e-05,\n",
            "        -2.3145e-04,  1.7924e-04, -2.4453e-04, -1.9922e-04, -2.0438e-04,\n",
            "         1.3909e-04,  1.7838e-04, -1.4055e-04, -2.1584e-04,  2.4254e-04,\n",
            "        -6.2071e-05,  1.9092e-04, -1.6628e-04, -2.7351e-04, -2.3639e-04,\n",
            "        -1.9445e-04,  1.0740e-04, -6.8765e-05,  2.3025e-04, -2.3010e-04,\n",
            "        -2.3522e-04,  2.1092e-04,  2.3722e-04, -2.5980e-04,  2.3487e-04,\n",
            "         1.4619e-04, -2.5551e-04, -2.3404e-04, -7.1596e-05, -1.8944e-04,\n",
            "         2.1690e-04, -1.4452e-04,  2.1340e-04,  2.3000e-05,  1.4412e-04,\n",
            "         2.6456e-04, -2.0672e-04,  1.1283e-04, -2.1323e-04,  4.6340e-05,\n",
            "        -9.9648e-05, -1.1445e-04, -2.3453e-04,  2.4341e-04,  2.1614e-04,\n",
            "        -2.4098e-04, -1.7585e-04, -2.3695e-04, -2.0969e-05,  2.2924e-04,\n",
            "        -9.6167e-05, -2.2625e-04,  1.3653e-04, -2.8703e-04, -1.7144e-04,\n",
            "        -1.9613e-04,  1.3485e-04, -2.5004e-04, -2.9361e-04, -2.1433e-04,\n",
            "         1.5137e-04, -2.3496e-04, -2.6108e-04,  2.6551e-04, -1.7438e-04,\n",
            "        -2.1007e-04,  1.7660e-04,  1.8874e-04,  6.4405e-05, -9.1201e-05,\n",
            "        -1.7012e-04, -2.1104e-04,  1.5303e-04,  5.9922e-05,  2.1834e-04,\n",
            "         3.7814e-05, -2.3877e-04, -1.8721e-04, -2.7563e-04, -1.8842e-04,\n",
            "         2.3124e-04, -7.1282e-05,  2.0242e-04, -2.1845e-04, -2.4375e-04,\n",
            "         2.1149e-04,  2.7650e-04, -9.0289e-05,  1.7104e-04, -2.2667e-04,\n",
            "         2.3401e-04, -2.3591e-04,  2.0567e-04,  1.9379e-04,  2.3735e-04,\n",
            "         1.5719e-04, -1.9716e-04,  2.6765e-04, -7.4799e-05, -2.5765e-04,\n",
            "        -2.3077e-04, -2.1576e-04,  2.6055e-04,  1.1927e-04, -1.6139e-05,\n",
            "         1.9506e-04, -5.6705e-05,  2.1653e-04, -2.1715e-04,  1.5028e-04,\n",
            "         2.3046e-04,  1.4409e-04, -1.8554e-04,  1.2220e-04, -1.6816e-04,\n",
            "        -2.4867e-04, -2.7333e-04, -1.4533e-04,  1.5014e-04, -2.5037e-04,\n",
            "         2.1624e-04, -1.7966e-04, -2.2521e-04, -2.7341e-04, -2.6550e-04,\n",
            "         2.1364e-04,  7.6959e-05,  1.8450e-04,  1.7032e-04, -8.3182e-05,\n",
            "        -1.9197e-04,  1.6447e-04, -2.4313e-04,  6.6180e-05, -6.4755e-05,\n",
            "        -2.6137e-04,  1.6263e-04,  2.0324e-04, -1.3129e-04, -2.1383e-04,\n",
            "         1.1967e-04,  1.1044e-04, -4.6828e-05, -2.1302e-04,  1.0433e-04,\n",
            "        -9.5491e-05,  8.9053e-05, -7.3511e-05, -1.1909e-04, -2.2087e-04,\n",
            "        -1.2696e-04,  2.5960e-04, -1.5215e-04,  2.3293e-04,  1.3849e-04,\n",
            "        -2.4577e-04,  2.2798e-04, -1.7620e-04,  2.4971e-04,  1.9388e-04,\n",
            "        -2.5987e-04, -2.8157e-05, -2.4586e-04,  1.9938e-04,  1.2158e-04,\n",
            "         2.4621e-04, -7.4967e-05,  1.5263e-04, -2.7464e-04, -2.6903e-04,\n",
            "         7.0322e-05, -2.4673e-04,  2.3112e-04,  2.3535e-04,  2.3705e-04,\n",
            "         2.2773e-04,  1.4543e-04, -2.0803e-04,  2.4996e-04,  2.2961e-04,\n",
            "         5.9454e-05, -7.7343e-05,  8.9844e-05,  2.1379e-04,  1.9039e-04,\n",
            "         9.2890e-05,  1.6013e-04, -2.4261e-04,  2.2856e-04,  1.5070e-04,\n",
            "        -2.2085e-04, -2.4963e-04,  1.5556e-04, -2.5086e-04, -2.2818e-04,\n",
            "         2.6134e-04, -1.0336e-04,  1.8625e-04, -1.8714e-04,  2.5124e-04,\n",
            "        -3.3653e-05,  2.1748e-05,  2.2702e-04,  7.9668e-05,  1.7301e-04,\n",
            "        -1.5380e-04, -7.5480e-05,  1.2426e-04,  1.7178e-04,  5.7904e-05,\n",
            "         1.6480e-04, -1.0680e-04, -1.5622e-04, -1.8636e-04,  2.4877e-04,\n",
            "         3.4271e-05, -2.4172e-04,  2.0984e-04, -1.9065e-04,  1.6796e-04,\n",
            "        -2.1361e-04, -1.8817e-04,  1.7592e-04, -1.2271e-04,  2.0679e-04,\n",
            "         1.9618e-04, -9.0756e-05,  2.0460e-04, -2.6165e-04, -2.6170e-04,\n",
            "        -2.6447e-04, -1.7954e-04, -2.5105e-04,  2.0997e-04,  2.5017e-04,\n",
            "         1.3640e-04, -2.2641e-04,  2.3114e-04, -2.6609e-04, -2.3189e-04,\n",
            "         2.4883e-04, -1.2996e-04,  9.4027e-05, -2.6665e-04,  2.2953e-04,\n",
            "        -2.3562e-04, -1.9184e-04, -2.0394e-04,  1.9992e-04, -1.9719e-04,\n",
            "         1.8965e-04,  2.0638e-04, -2.4882e-04, -8.1488e-05, -1.0394e-04,\n",
            "        -2.3933e-04, -1.3122e-04, -2.3019e-04,  2.7795e-04,  1.3545e-04,\n",
            "        -1.0651e-04,  2.3304e-04,  9.6132e-05, -2.3288e-04,  7.6474e-05,\n",
            "         1.3151e-04,  2.7127e-04, -3.7376e-05, -2.1026e-04,  2.4558e-04,\n",
            "        -2.4058e-04, -2.2697e-04, -2.3211e-04,  1.3559e-04, -1.4565e-04,\n",
            "         2.2085e-04, -1.9729e-04,  2.1840e-04,  1.4126e-04, -2.1473e-04,\n",
            "        -1.7591e-04,  2.4334e-04,  2.2874e-04,  2.5186e-04, -1.4661e-04,\n",
            "        -2.3875e-04, -2.2884e-04,  1.1271e-04,  1.9565e-04,  2.1939e-04])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cos_sim = cosine_similarity(soft_racall_final_gradient.unsqueeze(0), hard_recall_gradient_vector_success.unsqueeze(0))\n",
        "\n",
        "cos_sim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MGpw9rfYeA1",
        "outputId": "6f96364d-f66a-4b79-8f0f-e2150d3c9ca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.9962], grad_fn=<SumBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDUHY2pG5Ws8"
      },
      "outputs": [],
      "source": [
        "####OLD FUNCTION\n",
        "\n",
        "\n",
        "def approx_ber(h, tau, device): #h is n-dim; output is an approx Bernoulli vector with mean h\n",
        "    n = len(h)\n",
        "    u = torch.rand((2, n), device=device)\n",
        "    G = -torch.log(-torch.log(u))\n",
        "    x1 = torch.exp((torch.log(h) + G[0])/tau)\n",
        "    x2 = torch.exp((torch.log(torch.add(1,-h)) + G[1])/tau)\n",
        "    x_sum = torch.add(x1,x2)\n",
        "    x = torch.div(x1,x_sum)\n",
        "    return x\n",
        "\n",
        " #m = torch.distributions.gumbel.Gumbel(torch.zeros_like(scores), torch.ones_like(scores))\n",
        " #       g = m.sample()\n",
        " #       scores = scores + g\n",
        " #scores = scores + torch.log(khot_mask)\n",
        " #           onehot_approx = torch.nn.functional.softmax(scores / self.tau, dim=1)\n",
        "\n",
        "def Model_pred(X_loader, model, device):\n",
        "    prediction_list = torch.empty((0, 1), dtype=torch.float32, device=device)\n",
        "    for (x_batch, label_batch) in X_loader:\n",
        "        prediction = model(x_batch)\n",
        "        prediction_list = torch.cat((prediction_list,prediction),0)\n",
        "\n",
        "\n",
        "    predicted_class = torch.argmax(prediction_list)\n",
        "    predicted_class = prediction_list >= 0.5\n",
        "    return predicted_class\n",
        "\n",
        "\n",
        "def Recall(h, predicted_class, tau, device):\n",
        "    Y_vec = approx_ber(h, tau, device)\n",
        "    n = len(h)\n",
        "\n",
        "    Y_vec = torch.unsqueeze(Y_vec, 1)\n",
        "\n",
        "    x = torch.sum(torch.mul(Y_vec, predicted_class))\n",
        "    y = torch.sum(Y_vec)\n",
        "    return x/y\n",
        "\n",
        "def Recall_True(dataloader_test, model, device):\n",
        "    label_list  = torch.empty((0), dtype=torch.float32, device=device)\n",
        "    prediction_list = torch.empty((0, 1), dtype=torch.float32, device=device)\n",
        "\n",
        "    for (x_batch, label_batch) in dataloader_test:\n",
        "        label_list = torch.cat((label_list,label_batch),0)\n",
        "        prediction = model(x_batch)\n",
        "        prediction_list = torch.cat((prediction_list,prediction),0)\n",
        "\n",
        "    predicted_class = torch.argmax(prediction_list)\n",
        "    predicted_class = prediction_list >= 0.5\n",
        "    predicted_class = torch.squeeze(predicted_class, 1)\n",
        "\n",
        "    x = torch.sum(torch.mul(label_list, predicted_class))\n",
        "    y = torch.sum(label_list)\n",
        "\n",
        "    return x/y\n",
        "\n",
        "def var_recall_estimator(fnet, dataloader_test, Predictor, device, para):\n",
        "    tau = para['tau']\n",
        "    z_dim = para['z_dim']\n",
        "    N_iter =  para['N_iter']\n",
        "    if_print =  para['if_print']\n",
        "    predicted_class = Model_pred(dataloader_test, Predictor, device)\n",
        "\n",
        "    res  = torch.empty((0), dtype=torch.float32, device=device)\n",
        "    res_square  = torch.empty((0), dtype=torch.float32, device=device)\n",
        "\n",
        "\n",
        "    for i in range(N_iter):\n",
        "        z_pool = torch.randn(z_dim, device=device)\n",
        "        ENN_output_list = torch.empty((0), dtype=torch.float32, device=device)\n",
        "        for (x_batch, label_batch) in dataloader_test:\n",
        "            fnet_logits = fnet(x_batch, z_pool)\n",
        "            fnet_logits_probs = torch.nn.functional.softmax(fnet_logits, dim=1)\n",
        "            ENN_output_list = torch.cat((ENN_output_list,fnet_logits_probs[:,1]),0)\n",
        "        recall_est = Recall(ENN_output_list, predicted_class, tau, device)\n",
        "        res = torch.cat((res,(recall_est).view(1)),0)\n",
        "        res_square = torch.cat((res_square,(recall_est ** 2).view(1)),0)\n",
        "\n",
        "    var = torch.mean(res_square) - (torch.mean(res)) ** 2\n",
        "    if if_print == 1:\n",
        "        print('recall list', res)\n",
        "        print(\"var of recall:\",var)\n",
        "        print(\"mean of recall\",  torch.mean(res))\n",
        "    return var\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "#res = 0\n",
        "#n = 5\n",
        "#h = torch.tensor([0.15 for i in range(n)])\n",
        "#c = torch.tensor([1, 0, 1, 0, 1]) #fix classifier\n",
        "\n",
        "\n",
        "#tau = 0.1\n",
        "#gamma = 0.5\n",
        "#epsilon = 0.7\n",
        "\n",
        "\n",
        "##ignore the below\n",
        "##var_recall_estimator(fnet, dataloader_test, Predictor)\n",
        "#derivative of fnet_parmaeters w.r.t NN (sampling policy) parameters is known - now we need derivative of var recall w.r.t fnet_parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ILck0k13uMRC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}