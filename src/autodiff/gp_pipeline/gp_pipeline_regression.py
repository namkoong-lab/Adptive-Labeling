# -*- coding: utf-8 -*-
"""GP_pipeline_regression_true_pool_label_true_GP_params.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jonUWpD_3A9GDxI1N4oM5LYdgB0DoMSa
"""

import argparse
import typing

import torch
import gpytorch
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torch.distributions as distributions
import numpy as np
from dataclasses import dataclass
import time
from datetime import datetime
import matplotlib.pyplot as plt
import math

import k_subset_sampling
from nn_feature_weights import NN_feature_weights
from sample_normal import sample_multivariate_normal
from gaussian_process_cholesky_advanced import RBFKernelAdvanced, GaussianProcessCholeskyAdvanced
from var_l_2_loss import var_l2_loss_estimator, l2_loss, var_l2_loss_custom_gp_estimator
from custom_gp_cholesky import GaussianProcessCholesky, RBFKernel

# Define a configuration class for dataset-related parameters
@dataclass
class DatasetConfig:
    #large_dataset: bool
    csv_file_train: str
    csv_file_test: str
    csv_file_pool: str
    y_column: str  # Assuming same column name across above 3 sets


@dataclass
class ModelConfig:
    access_to_true_pool_y: bool
    hyperparameter_tune: bool
    batch_size_query: int
    temp_k_subset: float
    hidden_sizes_weight_NN: list
    meta_opt_lr: float
    meta_opt_weight_decay: float


@dataclass
class TrainConfig:
    n_train_iter: int
    N_iter: int


@dataclass
class GPConfig:
    length_scale: float
    output_scale: float
    noise: float
    paramater_tune_lr: float
    parameter_tune_weight_decay: float
    parameter_tune_nepochs: int

def experiment(dataset_config: DatasetConfig, model_config: ModelConfig, train_config: TrainConfig, gp_config: GPConfig, Predictor, device, if_print = 0):


    # Predictor here has already been pretrained


    # ------ see how to define a global seed --------- and separate controllable seeds for reproducibility
    #torch.manual_seed(40)



    #if dataset_config.large_dataset:

    #   dataset_train = TabularDataset(device, csv_file=dataset_config.csv_file_train, y_column=dataset_config.y_column)
    #   dataloader_train = DataLoader(dataset_train, batch_size=model_config.batch_size_train, shuffle=True)     # gives batch for training features and labels  (both in float 32)

    #   dataset_test = TabularDataset(device, csv_file=dataset_config.csv_file_test, y_column=dataset_config.y_column)
    #   dataloader_test = DataLoader(dataset_test, batch_size=model_config.batch_size_test, shuffle=False)       # gives batch for test features and label    (both in float 32)

    #   dataset_pool = TabularDataset(device, csv_file=dataset_config.csv_file_pool, y_column=dataset_config.y_column)
    #   pool_size = len(dataset_pool)
    #   dataloader_pool = DataLoader(dataset_pool, batch_size=pool_size, shuffle=False)       # gives all the pool features and label   (both in float 32) - needed for input in NN_weights

    #   dataset_pool_train = TabularDatasetPool(device, csv_file=dataset_config.csv_file_pool, y_column=dataset_config.y_column)
    #   dataloader_pool_train = DataLoader(dataset_pool_train, batch_size=model_config.batch_size_train, shuffle=True)       # gives batch of the pool features and label   (both in float 32) - needed for updating the posterior of ENN - as we will do batchwise update


    #else:

    init_train_data_frame = pd.read_csv(dataset_config.csv_file_train)
    pool_data_frame = pd.read_csv(dataset_config.csv_file_pool)
    test_data_frame = pd.read_csv(dataset_config.csv_file_test)
    init_train_x = torch.tensor(init_train_data_frame.drop(dataset_config.y_column, axis=1).values, dtype=torch.float32).to(device)
    init_train_y = torch.tensor(init_train_data_frame[dataset_config.y_column].values, dtype=torch.float32).to(device)
    pool_x = torch.tensor(pool_data_frame.drop(dataset_config.y_column, axis=1).values, dtype=torch.float32).to(device)
    pool_y = torch.tensor(pool_data_frame[dataset_config.y_column].values, dtype=torch.float32).to(device)
    test_x = torch.tensor(test_data_frame.drop(dataset_config.y_column, axis=1).values, dtype=torch.float32).to(device)
    test_y = torch.tensor(test_data_frame[dataset_config.y_column].values, dtype=torch.float32).to(device)
    pool_size = pool_x.size(0)




    #input_feature_size = init_train_x.size(1)
    #NN_weights = NN_feature_weights(input_feature_size, model_config.hidden_sizes_weight_NN, 1).to(device)
    #meta_opt = optim.Adam(NN_weights.parameters(), lr=model_config.meta_opt_lr, weight_decay=model_config.meta_opt_weight_decay)


    # Convert the size to a tensor and calculate the reciprocal
    reciprocal_size_value =  math.log(1.0 / pool_size)
    NN_weights = torch.full([pool_size], reciprocal_size_value, requires_grad=True).to(device)
    meta_opt = optim.Adam([NN_weights], lr=model_config.meta_opt_lr, weight_decay=model_config.meta_opt_weight_decay)

    SubsetOperator = k_subset_sampling.SubsetOperator(model_config.batch_size_query, device, model_config.temp_k_subset, False).to(device)

    #seed for this
    SubsetOperatortest = k_subset_sampling.SubsetOperator(model_config.batch_size_query, device, model_config.temp_k_subset, True).to(device)

    #if dataset_config.large_dataset:
    #  train_smaller_dataset(init_train_x, init_train_y, pool_x, pool_y, test_x, test_y, device, model_config, train_config, gp_config, NN_weights, meta_opt, SubsetOperator, Predictor, if_print = if_print)
    #  test_smaller_dataset(init_train_x, init_train_y, pool_x, pool_y, test_x, test_y, device, model_config, train_config, gp_config, NN_weights, meta_opt, SubsetOperator, Predictor, if_print = if_print)

    #else:

    if ModelConfig.hyperparameter_tune:
      gp_model = GaussianProcessCholeskyAdvanced(length_scale_init=gp_config.length_scale, variance_init=gp_config.output_scale, noise_var_init=gp_config.noise).to(device)
      optimizer = torch.optim.Adam(gp_model.parameters(), lr=gp_model.paramater_tune_lr, weight_decay = gp_model.paramater_tune_weight_decay)

      gp_model.train()  # Set the model to training mode
      for epoch in range(gp_model.paramater_tune_nepochs):
        optimizer.zero_grad()  # Clear previous gradients
        loss = gp_model.nll(x_train, y_train)  # Compute the loss (NLL)
        loss.backward()  # Compute gradients
        optimizer.step()  # Update parameters
        if (epoch + 1) % 10 == 0:
            print_model_parameters(gp_model)
            print(f'Epoch {epoch+1}, Loss: {loss.item()}')

    else:
        kernel = RBFKernel(length_scale=gp_config.length_scale, output_scale = gp_config.output_scale).to(device)
        gp_model = GaussianProcessCholesky(kernel=kernel).to(device)






    train_smaller_dataset(gp_model, init_train_x, init_train_y, pool_x, pool_y, test_x, test_y, device, model_config, train_config, gp_config, NN_weights, meta_opt, SubsetOperator, Predictor, if_print = if_print)
    test_smaller_dataset(gp_model, init_train_x, init_train_y, pool_x, pool_y, test_x, test_y, device, model_config, train_config, gp_config, NN_weights, meta_opt, SubsetOperatortest, Predictor, if_print = if_print)

def train_samller_dataset(gp_model, init_train_x, init_train_y, pool_x, pool_y, test_x, test_y, device, model_config, train_config, gp_config, NN_weights, meta_opt, SubsetOperator, Predictor, if_print = if_print):

  for i in range(train_config.n_train_iter):    # Should we do this multiple times or not
    start_time = time.time()

    meta_opt.zero_grad()

    #pool_weights = NN_weights(pool_x)   #pool_weights has shape [pool_size,1]
    #pool_weights_t = pool_weights.t()  #convert pool_weights to shape [1, pool_size]

    NN_weights_unsqueezed = NN_weights.unsqueeze(0)
    soft_k_vector = SubsetOperator(NN_weights_unsqueezed)
    #soft_k_vector = SubsetOperator(pool_weights_t)     #soft_k_vector has shape  [1,pool_size]
    soft_k_vector_squeeze = soft_k_vector.squeeze()

    print(soft_k_vector_squeeze)


    #input_feature_size = init_train_x.size(1)
    #init_train_batch_size = init_train_x.size(0)


    if model_config.access_to_true_pool_y:
      y_gp = torch.cat([init_train_y,pool_y], dim=0)
    else:
      w_dumi = torch.ones(init_train_batch_size).to(device)
      mu1, cov1 = gp_model(init_train_x, init_train_y, w_dumi, pool_x, gp_config.noise)
      cov_final = cov1 +  noise_var * torch.eye(pool_x.size(0), device=pool_x.device)
      pool_y_dumi = sample_multivariate_normal(mu1, cov_final, 1)
      print(pool_y_dumi)
      y_gp = torch.cat([init_train_y,pool_y_dumi], dim=0)

    x_gp = torch.cat([init_train_x,pool_x], dim=0)
    w_train = torch.ones(init_train_batch_size, requires_grad = True).to(device)
    w_gp = torch.cat([w_train,soft_k_vector_squeeze])



    mu2, cov2 = gp_model(x_gp, y_gp, w_gp, test_x, gp_config.noise)
    var_square_loss = var_l2_loss_custom_gp_estimator(mu2, cov2, gp_config.noise, test_x, Predictor, device, train_config.N_iter)
    print("var_square_loss:", var_square_loss)

    var_square_loss.backward()
    meta_opt.step()

    l_2_loss_actual = l2_loss(test_x, test_y, Predictor, None)
    print("l_2_loss_actual:", l_2_loss_actual)

def test_smaller_dataset(gp_model, init_train_x, init_train_y, pool_x, pool_y, test_x, test_y, device, model_config, train_config, gp_config, NN_weights, meta_opt, SubsetOperatortest, Predictor, if_print = if_print):


    NN_weights_unsqueezed = NN_weights.unsqueeze(0)
    soft_k_vector = SubsetOperatortest(NN_weights_unsqueezed)
    #soft_k_vector = SubsetOperator(pool_weights_t)     #soft_k_vector has shape  [1,pool_size]
    soft_k_vector_squeeze = soft_k_vector.squeeze()

    print(soft_k_vector_squeeze)

    y_gp = torch.cat([init_train_y,pool_y], dim=0)
    x_gp = torch.cat([init_train_x,pool_x], dim=0)
    w_train = torch.ones(init_train_batch_size, requires_grad = True).to(device)
    w_gp = torch.cat([w_train,soft_k_vector_squeeze])
    mu2, cov2 = gp_model(x_gp, y_gp, w_gp, test_x, gp_config.noise)

    var_square_loss = var_l2_loss_custom_gp_estimator(mu2, cov2, gp_config.noise, test_x, Predictor, device, train_config.N_iter)
    print("var_square_loss:", var_square_loss)


    l_2_loss_actual = l2_loss(test_x, test_y, Predictor, None)
    print("l_2_loss_actual:", l_2_loss_actual)