{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1594b1e",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa872f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFor real data -\\nSelect 10 features - use the old file  — code we have - we need to change the code somewhat\\nChange the setting to a continuous one - use the \\nForm the clusters - how many? - 51 clusters\\nReduce the dataset size -  use the old file — on fly \\n\\nTrain points - 100 - 500\\nPool points - 1 cluster - 250 points |||||  50 clusters 5 points each \\nNumber of clusters - 1+50\\nBatch size to be acquired  - 5\\nHorizons - 5 horizons\\n\\nMarginal distribution of x is not same as given dataset\\nTest points - 1 cluster - 20 points\\n                     25 clusters - 20 points\\n                       25 clusters  - 2 points\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from line_profiler import LineProfiler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.stats import percentileofscore\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotnine as gg\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "#from dataloader import TabularDataset\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import higher\n",
    "\n",
    "from torch import nn\n",
    "# from acme.utils.loggers.terminal import TerminalLogger\n",
    "import dataclasses\n",
    "#import chex\n",
    "#import haiku as hk\n",
    "#import jax\n",
    "#import jax.numpy as jnp\n",
    "#import optax\n",
    "import pandas as pd\n",
    "#import warnings\n",
    "import gpytorch\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "#warnings.filterwarnings('ignore')\n",
    "#import pipeline_var_l2_loss\n",
    "import seaborn as sns\n",
    "#from dataloader import TabularDataset\n",
    "#from var_l2_loss_estimator import *\n",
    "#from ENN import basenet_with_learnable_epinet_and_ensemble_prior\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#wandb.init()\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import torch.nn.init as init\n",
    "from datetime import datetime\n",
    " \n",
    "#data from https://drive.google.com/drive/u/1/folders/1WuXIzpYLrLNH6pn9zBMx6oRCCQPRz0F1\n",
    "\n",
    "directory = '/shared/share_mala/data/eicu_train_test/'\n",
    "train_csv = 'eicu_train_final.csv'\n",
    "test_csv = 'eicu_test_final.csv'\n",
    "\n",
    "df_train = pd.read_csv(directory + train_csv)\n",
    "df_test = pd.read_csv(directory + test_csv)\n",
    "df = pd.concat([df_train, df_test], axis = 0)\n",
    "X_col = list(df.columns)[:-1]\n",
    "Y = 'EVENT_LABEL'\n",
    "\n",
    "\n",
    "#df = df.groupby('EVENT_LABEL', group_keys=False).apply(lambda x: x.sample(2000))\n",
    "Y_data = np.array(df[Y])\n",
    "\n",
    "'''\n",
    "For real data -\n",
    "Select 10 features - use the old file  — code we have - we need to change the code somewhat\n",
    "Change the setting to a continuous one - use the \n",
    "Form the clusters - how many? - 51 clusters\n",
    "Reduce the dataset size -  use the old file — on fly \n",
    "\n",
    "Train points - 100 - 500\n",
    "Pool points - 1 cluster - 250 points |||||  50 clusters 5 points each \n",
    "Number of clusters - 1+50\n",
    "Batch size to be acquired  - 5\n",
    "Horizons - 5 horizons\n",
    "\n",
    "Marginal distribution of x is not same as given dataset\n",
    "Test points - 1 cluster - 20 points\n",
    "                     25 clusters - 20 points\n",
    "                       25 clusters  - 2 points\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19bcf88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(\n",
    "n_estimators=100,  # Number of trees in the forest\n",
    "criterion='gini',  # Function to measure the quality of a split. Can also be 'entropy'.\n",
    "max_depth=None,    # Maximum depth of the tree. If None, nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "min_samples_split=2,  # Minimum number of samples required to split an internal node\n",
    "min_samples_leaf=1,   # Minimum number of samples required to be at a leaf node\n",
    "bootstrap=True,       # Whether bootstrap samples are used when building trees\n",
    "oob_score=False,      # Whether to use out-of-bag samples to estimate the generalization score\n",
    "random_state=None,    # Controls both the randomness of the bootstrapping and the sampling of features to consider when looking for the best split\n",
    "verbose=0,            # Controls the verbosity of the process\n",
    "class_weight=None,    # Weights associated with classes. Can be 'balanced'.\n",
    ")\n",
    "    \n",
    "def get_feat_importance(model, X_col): # get most important features, input is a randomforest classifier/regressor\n",
    "    importances = model.feature_importances_\n",
    "    forest_importances = pd.Series(importances, index = X_col)\n",
    "    forest_importances = forest_importances.nlargest(10)  #only keep most important 10 feature\n",
    "    feature_importances = list(forest_importances.index)\n",
    "    return feature_importances #return the columns   \n",
    "\n",
    "\n",
    "Y_train = df['EVENT_LABEL']\n",
    "X_col = list(df.columns)\n",
    "X_col.remove('EVENT_LABEL')\n",
    "X_train = df[X_col]\n",
    "\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "X_col = list(X_train.columns)\n",
    "X_col_important = get_feat_importance(model, X_col)\n",
    "# X_col_important.append('EVENT_LABEL')\n",
    "\n",
    "X_train_imp = df[X_col_important]\n",
    "\n",
    "Y_pred = model.predict_proba(X_train)\n",
    "Y_pred = Y_pred[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6a88cffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/ym2865/.conda/envs/yuanzhe_new/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(n_clusters=51, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(n_clusters=51, random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KMeans(n_clusters=51, random_state=1)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize data\n",
    "n_cluster = 51\n",
    "data = np.array(X_train_imp)\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(data)\n",
    "kmeans = KMeans(n_clusters = n_cluster, random_state = 1)\n",
    "kmeans.fit(data_normalized)\n",
    "\n",
    "# y_kmeans = kmeans.predict(data_normalized)\n",
    "\n",
    "# # Visualize the clusters\n",
    "# plt.scatter(data[:, 0], data[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "\n",
    "# # Plot the centroids of the clusters\n",
    "# centers = kmeans.cluster_centers_\n",
    "# plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75)\n",
    "\n",
    "# plt.xlabel('Feature 1')\n",
    "# plt.ylabel('Feature 2')\n",
    "# plt.title('KMeans Clustering')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c74bef64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element 0 occurs 983 times.\n",
      "Element 1 occurs 3891 times.\n",
      "Element 2 occurs 840 times.\n",
      "Element 3 occurs 337 times.\n",
      "Element 4 occurs 1168 times.\n",
      "Element 5 occurs 498 times.\n",
      "Element 6 occurs 3068 times.\n",
      "Element 7 occurs 1697 times.\n",
      "Element 8 occurs 1381 times.\n",
      "Element 9 occurs 523 times.\n",
      "Element 10 occurs 1921 times.\n",
      "Element 11 occurs 1421 times.\n",
      "Element 12 occurs 446 times.\n",
      "Element 13 occurs 3547 times.\n",
      "Element 14 occurs 1262 times.\n",
      "Element 15 occurs 502 times.\n",
      "Element 16 occurs 388 times.\n",
      "Element 17 occurs 626 times.\n",
      "Element 18 occurs 1660 times.\n",
      "Element 19 occurs 1646 times.\n",
      "Element 20 occurs 303 times.\n",
      "Element 21 occurs 7 times.\n",
      "Element 22 occurs 661 times.\n",
      "Element 23 occurs 3308 times.\n",
      "Element 24 occurs 54 times.\n",
      "Element 25 occurs 668 times.\n",
      "Element 26 occurs 1255 times.\n",
      "Element 27 occurs 179 times.\n",
      "Element 28 occurs 2840 times.\n",
      "Element 29 occurs 140 times.\n",
      "Element 30 occurs 86 times.\n",
      "Element 31 occurs 2981 times.\n",
      "Element 32 occurs 218 times.\n",
      "Element 33 occurs 1821 times.\n",
      "Element 34 occurs 1985 times.\n",
      "Element 35 occurs 309 times.\n",
      "Element 36 occurs 479 times.\n",
      "Element 37 occurs 719 times.\n",
      "Element 38 occurs 1122 times.\n",
      "Element 39 occurs 314 times.\n",
      "Element 40 occurs 152 times.\n",
      "Element 41 occurs 3199 times.\n",
      "Element 42 occurs 529 times.\n",
      "Element 43 occurs 433 times.\n",
      "Element 44 occurs 1116 times.\n",
      "Element 45 occurs 1301 times.\n",
      "Element 46 occurs 227 times.\n",
      "Element 47 occurs 343 times.\n",
      "Element 48 occurs 331 times.\n",
      "Element 49 occurs 31 times.\n",
      "Element 50 occurs 133 times.\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "y_kmeans = kmeans.predict(data_normalized)\n",
    "# Get unique elements and their counts\n",
    "unique_elements, counts = np.unique(y_kmeans, return_counts=True)\n",
    "\n",
    "# Zip the unique elements and their counts together for easy iteration\n",
    "frequency = dict(zip(unique_elements, counts))\n",
    "\n",
    "# Print frequency of each element\n",
    "for element, count in frequency.items():\n",
    "    print(f\"Element {element} occurs {count} times.\")\n",
    "    \n",
    "print(np.sum(np.array(list(frequency.values())) >= 800))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fce84b",
   "metadata": {},
   "source": [
    "# Select points from clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "83cbf5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# < 800 choose 1 test point\n",
    "# > 800 10 test points\n",
    "# cluster 1 as training cluster 100 points in training\n",
    "# 250 points from 1 as pool data\n",
    "# 5 points from each of the rest for pool\n",
    "\n",
    "big_cluster_indices = [i for i in range(n_cluster) if frequency[i] >= 800]\n",
    "small_cluster_indices = [i for i in range(n_cluster) if frequency[i] < 800]\n",
    "biggest_cluster = 1\n",
    "big_cluster_indices.remove(biggest_cluster)\n",
    "n = df.shape[0]\n",
    "labels = kmeans.labels_\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "\n",
    "\n",
    "def close_point_cluster(data, labels, centers, num_closest_points, desired_cluster_index):\n",
    "    # Index of the desired cluster center\n",
    "    X = data[labels == desired_cluster_index]\n",
    "\n",
    "    desired_cluster_index = desired_cluster_index  # Change this to the desired cluster index\n",
    "\n",
    "    # Calculate distances from each point to the desired cluster center\n",
    "    distances_to_desired_center = np.linalg.norm(X - centers[desired_cluster_index], axis=1)\n",
    "\n",
    "    # Sort the distances and select the indices of the 10 closest points\n",
    "    closest_indices = np.argsort(distances_to_desired_center)[:num_closest_points]\n",
    "\n",
    "\n",
    "    return closest_indices\n",
    "\n",
    "def list_remove(arr, index_to_remove, version = 0):\n",
    "    if version == 0:\n",
    "        new_arr = np.concatenate((arr[:index_to_remove], arr[index_to_remove+1:]))\n",
    "    else:\n",
    "        new_arr = [arr[i] for i in range(len(arr)) if i not in index_to_remove]\n",
    "    return new_arr\n",
    "\n",
    "training_pool_indices = close_point_cluster(data_normalized, labels, centers, num_closest_points = 350, desired_cluster_index = biggest_cluster)\n",
    "shuffled_integers = np.random.permutation(np.arange(0, len(training_pool_indices)))\n",
    "# Select the first n integers from the shuffled array\n",
    "n = 100  # Number of random integers to generate\n",
    "random_integers = shuffled_integers[:n]\n",
    "\n",
    "train_indices = training_pool_indices[random_integers]\n",
    "pool_indices = list_remove(training_pool_indices, random_integers, 1)\n",
    "\n",
    "\n",
    "test_pool_indices_small = [close_point_cluster(data_normalized, labels, centers, num_closest_points = 6, desired_cluster_index = i)  for i in small_cluster_indices]\n",
    "random_indices_list = np.random.randint(low=0, high=6, size=n_cluster)\n",
    "pool_indices_small = [test_pool_indices_small[i][random_indices_list[i]] for i in range(len(small_cluster_indices))]\n",
    "test_indices_small = [list_remove(test_pool_indices_small[i],random_indices_list[i]) for i in range(len(small_cluster_indices))]\n",
    "\n",
    "test_pool_indices_big = [close_point_cluster(data_normalized, labels, centers, num_closest_points = 15, desired_cluster_index = i)  for i in big_cluster_indices]\n",
    "#random_indices_list_big = np.random.randint(low=0, high=15, size=(len(big_cluster_indices),5))\n",
    "random_indices_list_big = []\n",
    "for i in range(len(big_cluster_indices)):\n",
    "    shuffled_integers = np.random.permutation(np.arange(0, 15))\n",
    "    # Select the first n integers from the shuffled array\n",
    "    n = 5  # Number of random integers to generate\n",
    "    random_integers = shuffled_integers[:n]\n",
    "    random_indices_list_big.append(random_integers)\n",
    "\n",
    "\n",
    "pool_indices_big = [test_pool_indices_big[i][random_indices_list_big[i]] for i in range(len(big_cluster_indices))]\n",
    "test_indices_big = [list_remove(test_pool_indices_big[i],random_indices_list_big[i], version = 1) for i in range(len(big_cluster_indices))]\n",
    "\n",
    " \n",
    "#train_indices\n",
    "pool_indices_all = np.concatenate((pool_indices, pool_indices_small, np.array(pool_indices_big).flatten()))\n",
    "test_indices_all = np.concatenate((np.array(test_indices).flatten(), np.array(test_indices_small).flatten(), np.array(test_indices_big).flatten()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "49127079",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/shared/share_mala/data/eicu_train_test/0518_data/'\n",
    "\n",
    "index_list = [train_indices, pool_indices_all, test_indices_all]\n",
    "name_list = ['train','pool','test']\n",
    "\n",
    "for i in range(3):\n",
    "    pd.DataFrame(data_normalized[index_list[i]]).to_csv(data_dir + name_list[i] + '_x.csv')\n",
    "    pd.DataFrame(Y_pred[index_list[i]]).to_csv(data_dir + name_list[i] + '_y.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yuanzhe_new",
   "language": "python",
   "name": "yuanzhe_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
