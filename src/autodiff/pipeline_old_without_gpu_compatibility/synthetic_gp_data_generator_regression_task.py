# -*- coding: utf-8 -*-
"""Synthetic_GP_Data_generator_Regression_task.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LQS2JSMyawNpEj2LJ_qxMpbjaSkNECvB
"""

pip install gpytorch

import torch
import gpytorch

# Define parameters for the model
mean_constant = 0.0  # Mean of the GP
length_scale = 1.0   # Length scale of the RBF kernel
noise_std = 0.1      # Standard deviation of the noise

# Initialize multidimensional training data
num_samples = 2
input_dim = 1
train_x = torch.rand((num_samples, input_dim))  # Random inputs in multi-dimensions
train_y = torch.zeros(num_samples)  # Placeholder for training targets

class CustomizableGPModel(gpytorch.models.ExactGP):
    def __init__(self, train_x, train_y, likelihood, mean_constant, length_scale, noise_std):
        super(CustomizableGPModel, self).__init__(train_x, train_y, likelihood)
        self.mean_module = gpytorch.means.ConstantMean()
        self.mean_module.constant = mean_constant
        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())
        self.covar_module.base_kernel.lengthscale = length_scale
        self.likelihood = likelihood
        self.likelihood.noise_covar.noise = noise_std**2

    def forward(self, x):
        return gpytorch.distributions.MultivariateNormal(self.mean_module(x), self.covar_module(x))

# Define the likelihood
likelihood = gpytorch.likelihoods.GaussianLikelihood()

# Create the GP model with specified parameters
model = CustomizableGPModel(train_x, train_y, likelihood, mean_constant, length_scale, noise_std)

# Sample from the prior for training data
model.eval()
likelihood.eval()
with torch.no_grad():
    prior_dist = likelihood(model(train_x))
    train_y = prior_dist.sample()  # Synthetic training targets

# Update the model with the training data
model.set_train_data(inputs=train_x, targets=train_y, strict=False)       ####### CAN ALSO USE TRAINING OVER NLL HERE########

# Compute the posterior on the training data
with torch.no_grad(), gpytorch.settings.fast_pred_var():
    posterior_train = model(train_x)
    posterior_mean = posterior_train.mean
    posterior_var = posterior_train.variance

# Define multidimensional test data points
num_test_samples = 100
test_x = torch.rand((num_test_samples, input_dim))

with torch.no_grad(), gpytorch.settings.fast_pred_var():
    posterior_test_latent = model(test_x)
    test_f = posterior_test_latent.sample()
    posterior_test = likelihood(test_f)
    test_y = posterior_test.sample()

#f_preds = model(test_x)
#y_preds = likelihood(model(test_x))

#f_mean = f_preds.mean
#f_var = f_preds.variance
#f_covar = f_preds.covariance_matrix
#f_samples = f_preds.sample(sample_shape=torch.Size(1000,))
#likelihood.noise.item()
#posterior_test.variance