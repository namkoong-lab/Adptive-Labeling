# -*- coding: utf-8 -*-
"""GP_pipeline_regression_true_pool_label_true_GP_params.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jonUWpD_3A9GDxI1N4oM5LYdgB0DoMSa
https://github.com/namkoong-lab/adaptive_sampling/blob/c1b7fe65bffeff2f731fa0030ad170456c26d316/src/baselines/RL_scripts/gp_pipeline_regression_modified.py
"""

import argparse
import typing

import torch
import gpytorch
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torch.distributions as distributions
import numpy as np
from dataclasses import dataclass
import time
from datetime import datetime
import matplotlib.pyplot as plt
import pandas as pd
import math
import torch.nn as nn
from torch import Tensor
import numpy as np
import wandb
import matplotlib.pyplot as plt

# import k_subset_sampling
#from nn_feature_weights import NN_feature_weights
# from sample_normal import sample_multivariate_normal
# from gaussian_process_cholesky_advanced import RBFKernelAdvanced, GaussianProcessCholeskyAdvanced
from variance_l_2_loss import var_l2_loss_estimator, l2_loss
from polyadic_sampler import CustomizableGPModel
# from custom_gp_cholesky import GaussianProcessCholesky, RBFKernel

import gymnasium as gym
from gymnasium import spaces

from tqdm import tqdm

# Define a configuration class for dataset-related parameters
@dataclass
class DatasetConfig:
    def __init__(self, direct_tensors_bool: bool, csv_file_train=None, csv_file_test=None, csv_file_pool=None, y_column=None):
        self.direct_tensors_bool = direct_tensors_bool
        self.csv_file_train = csv_file_train
        self.csv_file_test = csv_file_test
        self.csv_file_pool = csv_file_pool
        self.y_column = y_column      # Assuming same column name across above 3 sets


@dataclass
class ModelConfig:
    access_to_true_pool_y: bool
    hyperparameter_tune: bool
    batch_size_query: int
    temp_k_subset: float
    meta_opt_lr: float
    meta_opt_weight_decay: float


@dataclass
class TrainConfig:
    n_train_iter: int
    n_samples: int
    G_samples: int


@dataclass
class GPConfig:
    length_scale: float
    noise_var: float
    output_scale: float

    
def print_model_parameters(model):
    for name, param in model.named_parameters():
        print(f"{name}: {param.data}")    



class GP_experiment():
    """
    experiment for training GP (UQ)
    """

    def __init__(self,model,train_x,train_y):

        self.model = model
        self.train_x = train_x
        self.train_y = train_y

    def step(self,x,y):
        """GP training Step"""
        # features,labels = batch.tensors
        new_train_x = torch.cat([self.train_x,x],dim=0)
        new_train_y = torch.cat([self.train_y,y],dim=0)
        self.model.set_train_data(inputs=new_train_x, targets=new_train_y, strict=False)

class toy_GP_ENV(gym.Env):

    #T = 1
    # in_dim = 20
    # out_dim = 1
    # num_epochs = 10 #number of epochs for each batch in MLP experiment

    def __init__(self,train_x,train_y,test_x,pool_x,pool_y,model,model_config, train_config, gp_config,Predictor,batch_size,seed_policy = 0, seed_model=0):

        super().__init__()
        #self.batch_size = batch_size
        self.train_x = train_x
        self.train_y = train_y
        self.test_x = test_x
        self.pool_x = pool_x
        self.pool_y = pool_y
        self.model = model
        self.Predictor = Predictor
        self.n_samples = train_config.n_samples
        self.T = 1
        # self.learning_rate = learning_rate
        self.t = 0
        #self.seed_model = seed_model
        # num_dataset = len(dataset)
        self.experiment = GP_experiment(self.model,self.train_x,self.train_y)
        # self.init_model = copy.deepcopy(self.experiment.model)
        #self.action_space = spaces.MultiBinary(num_dataset) #gym settings currently not needed
        #self.observation_space = spaces.Box(low=-1, high=1, shape=(num_dataset,)) #gym settings currently not needed
        print("INITIALIZED")

    def reset(self, seed=None, options=None):
        return None

    def _get_obs(self):
        return None

    def _get_info(self):
        return None
 
    def step(self, action):
        """environment step"""
        x = self.pool_x[action]
        y = self.pool_y[action]
        self.experiment.step(x,y)
        #observation = self._get_obs() # not needed
        mean, loss = var_l2_loss_estimator(self.experiment.model, self.test_x, self.Predictor, (self.test_x).device, self.n_samples)

        terminated = False # check if it should terminate (we currently just have 1 step)
        self.t += 1
        if self.t >= self.T:
            terminated = True

        #truncated = False
        #terminated = True
        #info = self._get_info()

        return mean, loss, terminated

    def render(self):
        pass

    def close(self):
        pass


def experiment(dataset_config: DatasetConfig, model_config: ModelConfig, train_config: TrainConfig, gp_config: GPConfig, direct_tensor_files, Predictor, device, if_print = 0):
    
    if dataset_config.direct_tensors_bool:
        assert direct_tensor_files != None, "direct_tensors_were_not_provided"
        init_train_x, init_train_y, pool_x, pool_y, test_x, test_y, pool_sample_idx, test_sample_idx = direct_tensor_files
    
    else: 
        init_train_data_frame = pd.read_csv(dataset_config.csv_file_train)
        pool_data_frame = pd.read_csv(dataset_config.csv_file_pool)
        test_data_frame = pd.read_csv(dataset_config.csv_file_test)
        init_train_x = torch.tensor(init_train_data_frame.drop(dataset_config.y_column, axis=1).values, dtype=torch.float32).to(device)
        init_train_y = torch.tensor(init_train_data_frame[dataset_config.y_column].values, dtype=torch.float32).to(device)
        pool_x = torch.tensor(pool_data_frame.drop(dataset_config.y_column, axis=1).values, dtype=torch.float32).to(device)
        pool_y = torch.tensor(pool_data_frame[dataset_config.y_column].values, dtype=torch.float32).to(device)
        test_x = torch.tensor(test_data_frame.drop(dataset_config.y_column, axis=1).values, dtype=torch.float32).to(device)
        test_y = torch.tensor(test_data_frame[dataset_config.y_column].values, dtype=torch.float32).to(device)
        pool_sample_idx = None 
        test_sample_idx = None
    
    pool_size = pool_x.size(0)

    mean_module = gpytorch.means.ConstantMean()
    base_kernel = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())
    likelihood = gpytorch.likelihoods.GaussianLikelihood()


    length_scale = gp_config.length_scale
    noise_var = gp_config.noise_var
    output_scale = gp_config.output_scale

    mean_module.constant = 0.0
    base_kernel.base_kernel.lengthscale = length_scale
    base_kernel.outputscale = output_scale
    likelihood.noise_covar.noise = noise_var


    gp_model = CustomizableGPModel(init_train_x, init_train_y, mean_module, base_kernel, likelihood).to(device)

    # Sample from the prior for training data
    gp_model.eval()
    likelihood.eval()

    var_square_loss, policy = policy_gradient_train(gp_model, init_train_x, init_train_y, pool_x, pool_y, test_x, test_y, model_config, train_config, gp_config, Predictor)
    
    return var_square_loss

def policy_gradient_train(gp_model, init_train_x, init_train_y, pool_x, pool_y, test_x, test_y, model_config, train_config, gp_config, Predictor):

    pool_size = pool_x.size(0)
    reciprocal_size_value =  math.log(1.0 / pool_size)
    policy = torch.full([pool_size], reciprocal_size_value, requires_grad=True)

    batch_size_query = model_config.batch_size_query

    env = toy_GP_ENV(init_train_x,init_train_y,test_x,pool_x,pool_y,gp_model,model_config, train_config, gp_config,Predictor,batch_size_query)

    optimizer = torch.optim.Adam([policy], lr=model_config.meta_opt_lr, weight_decay = model_config.meta_opt_weight_decay)
    
    loss_pool = []

    steps = 0

    for episode in tqdm(range(train_config.n_train_iter)):
        #state = env.reset() # reset env, state currenly not needed
        #env.render()

        for t in range(1):
            w = policy.squeeze()
            prob = F.softmax(w, dim=0)   
            
            loss_temp = []
            for j in range(train_config.G_samples):
                batch_ind = torch.multinomial(prob, batch_size_query, replacement=False)
                log_pr = (torch.log(prob[batch_ind])).sum()
                for i in range(batch_size_query):
                    log_pr = log_pr- torch.log(1 - prob[batch_ind[:i]].sum())
                action = batch_ind

                mean, loss, done = env.step(action) # env step, uq update
                loss_temp.append(log_pr*loss)
                env.reset()

            avg_loss = torch.stack(loss_temp).mean()

            optimizer.zero_grad()
            avg_loss.backward()
            optimizer.step()
                          
            env.render()

            loss_pool.append(avg_loss.detach().cpu().numpy())

            steps += 1

            if done:
                break

    data_series = pd.Series(loss_pool)
    # rolling_mean = data_series
    rolling_mean = data_series.rolling(window=200).mean()
    plt.plot(rolling_mean)
    plt.savefig('pg_test_gpr.jpg')

    return loss_pool[-1], policy





def experiment(dataset_config: DatasetConfig, model_config: ModelConfig, train_config: TrainConfig, gp_config: GPConfig, direct_tensor_files, Predictor, device, if_print = 0):
    
    if dataset_config.direct_tensors_bool:
        assert direct_tensor_files != None, "direct_tensors_were_not_provided"
        init_train_x, init_train_y, pool_x, pool_y, test_x, test_y, pool_sample_idx, test_sample_idx = direct_tensor_files
    
    else: 
        init_train_data_frame = pd.read_csv(dataset_config.csv_file_train)
        pool_data_frame = pd.read_csv(dataset_config.csv_file_pool)
        test_data_frame = pd.read_csv(dataset_config.csv_file_test)
        init_train_x = torch.tensor(init_train_data_frame.drop(dataset_config.y_column, axis=1).values, dtype=torch.float32).to(device)
        init_train_y = torch.tensor(init_train_data_frame[dataset_config.y_column].values, dtype=torch.float32).to(device)
        pool_x = torch.tensor(pool_data_frame.drop(dataset_config.y_column, axis=1).values, dtype=torch.float32).to(device)
        pool_y = torch.tensor(pool_data_frame[dataset_config.y_column].values, dtype=torch.float32).to(device)
        test_x = torch.tensor(test_data_frame.drop(dataset_config.y_column, axis=1).values, dtype=torch.float32).to(device)
        test_y = torch.tensor(test_data_frame[dataset_config.y_column].values, dtype=torch.float32).to(device)
        pool_sample_idx = None 
        test_sample_idx = None
    
    pool_size = pool_x.size(0)

    mean_module = gpytorch.means.ConstantMean()
    base_kernel = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())
    likelihood = gpytorch.likelihoods.GaussianLikelihood()


    length_scale = gp_config.length_scale
    noise_var = gp_config.noise_var
    output_scale = gp_config.output_scale

    mean_module.constant = 0.0
    base_kernel.base_kernel.lengthscale = length_scale
    base_kernel.outputscale = output_scale
    likelihood.noise_covar.noise = noise_var


    gp_model = CustomizableGPModel(init_train_x, init_train_y, mean_module, base_kernel, likelihood).to(device)

    # Sample from the prior for training data
    gp_model.eval()
    likelihood.eval()

    var_square_loss, policy = policy_gradient_train(gp_model, init_train_x, init_train_y, pool_x, pool_y, test_x, test_y, model_config, train_config, gp_config, Predictor)
    
    return var_square_loss, policy
