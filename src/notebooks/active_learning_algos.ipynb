{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import typing as tp"
      ],
      "metadata": {
        "id": "AERFhCplSKsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIs96l10Ru9y"
      },
      "outputs": [],
      "source": [
        "    \"\"\"Calculates a priority score based on logits, labels, and a random key.\n",
        "\n",
        "    Args:\n",
        "      logits: An array of shape [A, B, C] where B is the batch size of data, C\n",
        "        is the number of outputs per data (for classification, this is equal to\n",
        "        number of classes), and A is the number of random samples for each data.\n",
        "      labels: An array of shape [B, 1] where B is the batch size of data.\n",
        "      key: A random key.\n",
        "\n",
        "    Returns:\n",
        "      A priority score per example of shape [B,].\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "def uniform_per_example(logits, labels):\n",
        "    \"\"\"Returns uniformly random scores per example.\"\"\"\n",
        "    del logits  # logits are not used in this function\n",
        "    labels = labels.squeeze()\n",
        "    return torch.rand(labels.shape)\n",
        "\n",
        "\n",
        "def variance_per_example(logits):\n",
        "    \"\"\"Calculates variance per example.\"\"\"\n",
        "    _, data_size, _ = logits.shape\n",
        "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    variances = torch.var(probs, dim=0, unbiased=False).sum(dim=-1)\n",
        "    assert variances.shape == (data_size,)\n",
        "    return variances\n",
        "\n",
        "\n",
        "\n",
        "def nll_per_example(logits, labels):\n",
        "    \"\"\"Calculates negative log-likelihood (NLL) per example.\"\"\"\n",
        "    _, data_size, _ = logits.shape\n",
        "    sample_probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    probs = torch.mean(sample_probs, dim=0)\n",
        "\n",
        "    # Penalize with log loss\n",
        "    labels = labels.to(torch.int64)  # Ensure labels are integers\n",
        "    labels = labels.squeeze()\n",
        "    true_probs = probs[torch.arange(data_size), labels]\n",
        "    losses = -torch.log(true_probs)\n",
        "    assert losses.shape == (data_size,)\n",
        "    return losses\n",
        "\n",
        "\n",
        "\n",
        "def joint_nll_per_example(logits, labels):\n",
        "    \"\"\"Calculates joint negative log-likelihood (NLL) per example.\"\"\"\n",
        "    num_enn_samples, data_size, _ = logits.shape\n",
        "    sample_probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "    # Penalize with log loss\n",
        "    labels = labels.to(torch.int64)  # Ensure labels are integers\n",
        "    labels = labels.squeeze()\n",
        "    true_probs = sample_probs[:, torch.arange(data_size), labels]\n",
        "    tau = 10\n",
        "    repeated_lls = tau * torch.log(true_probs)\n",
        "    assert repeated_lls.shape == (num_enn_samples, data_size)\n",
        "\n",
        "    # Take average of joint lls over num_enn_samples\n",
        "    joint_lls = torch.mean(repeated_lls, dim=0)\n",
        "    assert joint_lls.shape == (data_size,)\n",
        "\n",
        "    return -1 * joint_lls\n",
        "\n",
        "\n",
        "\n",
        "def entropy_per_example(logits):\n",
        "    \"\"\"Calculates entropy per example.\"\"\"\n",
        "    _, data_size, num_classes = logits.shape\n",
        "    sample_probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    probs = torch.mean(sample_probs, dim=0)\n",
        "    assert probs.shape == (data_size, num_classes)\n",
        "\n",
        "    entropies = -1 * torch.sum(probs * torch.log(probs), dim=1)\n",
        "    assert entropies.shape == (data_size,)\n",
        "\n",
        "    return entropies\n",
        "\n",
        "\n",
        "\n",
        "def margin_per_example(logits):\n",
        "    \"\"\"Calculates margin between top and second probabilities per example.\"\"\"\n",
        "    _, data_size, num_classes = logits.shape\n",
        "    sample_probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    probs = torch.mean(sample_probs, dim=0)\n",
        "    assert probs.shape == (data_size, num_classes)\n",
        "\n",
        "    sorted_probs, _ = torch.sort(probs, descending=True)\n",
        "    margins = sorted_probs[:, 0] - sorted_probs[:, 1]\n",
        "    assert margins.shape == (data_size,)\n",
        "\n",
        "    # Return the *negative* margin\n",
        "    return -margins\n",
        "\n",
        "\n",
        "\n",
        "def bald_per_example(logits):\n",
        "    \"\"\"Calculates BALD mutual information per example.\"\"\"\n",
        "    num_enn_samples, data_size, num_classes = logits.shape\n",
        "    sample_probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "    # Function to compute entropy\n",
        "    def compute_entropy(p):\n",
        "        return -1 * torch.sum(p * torch.log(p), dim=1)\n",
        "\n",
        "    # Compute entropy for average probabilities\n",
        "    mean_probs = torch.mean(sample_probs, dim=0)\n",
        "    assert mean_probs.shape == (data_size, num_classes)\n",
        "    mean_entropy = compute_entropy(mean_probs)\n",
        "    assert mean_entropy.shape == (data_size,)\n",
        "\n",
        "    # Compute entropy for each sample probabilities\n",
        "    sample_entropies = torch.stack([compute_entropy(p) for p in sample_probs])\n",
        "    assert sample_entropies.shape == (num_enn_samples, data_size)\n",
        "\n",
        "    models_disagreement = mean_entropy - torch.mean(sample_entropies, dim=0)\n",
        "    assert models_disagreement.shape == (data_size,)\n",
        "    return models_disagreement\n",
        "\n",
        "\n",
        "\n",
        "def var_ratios_per_example(logits):\n",
        "    \"\"\"Calculates the highest probability per example.\"\"\"\n",
        "    _, data_size, num_classes = logits.shape\n",
        "    sample_probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    probs = torch.mean(sample_probs, dim=0)\n",
        "    assert probs.shape == (data_size, num_classes)\n",
        "\n",
        "    max_probs = torch.max(probs, dim=1).values\n",
        "    variation_ratio = 1 - max_probs\n",
        "    assert len(variation_ratio) == data_size\n",
        "\n",
        "    return variation_ratio\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def make_ucb_per_example(ucb_factor: float = 1., class_values: tp.Optional[torch.Tensor] = None):\n",
        "    \"\"\"Creates a UCB-style priority metric.\"\"\"\n",
        "\n",
        "    def compute_ucb(logits, labels, key=None):\n",
        "        del labels, key\n",
        "        _, data_size, num_classes = logits.shape\n",
        "\n",
        "        # Either use class values or default to just the first class\n",
        "        scale_values = class_values\n",
        "        if scale_values is None:\n",
        "            scale_values = torch.zeros(num_classes)\n",
        "            scale_values[0] = 1\n",
        "\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        value = torch.einsum('zbc,c->zb', probs, scale_values)\n",
        "        mean_values = torch.mean(value, dim=0)\n",
        "        std_values = torch.std(value, dim=0, unbiased=False)\n",
        "        ucb_value = mean_values + ucb_factor * std_values\n",
        "        assert ucb_value.shape == (data_size,)\n",
        "        return ucb_value\n",
        "\n",
        "    return compute_ucb\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def make_scaled_mean_per_example(class_values: tp.Optional[torch.Tensor] = None):\n",
        "    \"\"\"Creates a priority metric based on mean probs scaled by class_values.\"\"\"\n",
        "\n",
        "    def compute_scaled_mean(logits, labels, key=None):\n",
        "        del labels, key\n",
        "        _, data_size, num_classes = logits.shape\n",
        "\n",
        "        # Either use class values or default to just the first class\n",
        "        scale_values = class_values\n",
        "        if scale_values is None:\n",
        "            scale_values = torch.zeros(num_classes)\n",
        "            scale_values[0] = 1\n",
        "\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        values = torch.einsum('zbc,c->zb', probs, scale_values)\n",
        "        mean_values = torch.mean(values, dim=0)\n",
        "        assert mean_values.shape == (data_size,)\n",
        "        return mean_values\n",
        "\n",
        "    return compute_scaled_mean\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def make_scaled_std_per_example(class_values: tp.Optional[torch.Tensor] = None):\n",
        "    \"\"\"Creates a priority metric based on std of probs scaled by class_values.\"\"\"\n",
        "\n",
        "    def compute_scaled_std(logits, labels, key=None):\n",
        "        del labels, key\n",
        "        _, data_size, num_classes = logits.shape\n",
        "\n",
        "        # Either use class values or default to just the first class\n",
        "        scale_values = class_values\n",
        "        if scale_values is None:\n",
        "            scale_values = torch.zeros(num_classes)\n",
        "            scale_values[0] = 1\n",
        "\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        values = torch.einsum('zbc,c->zb', probs, scale_values)\n",
        "        std_values = torch.std(values, axis=0, unbiased=False)\n",
        "        assert std_values.shape == (data_size,)\n",
        "        return std_values\n",
        "\n",
        "    return compute_scaled_std\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = [x:y]\n",
        "acquisition_size = 10\n",
        "\n",
        "for i in range(n_iter):\n",
        "  z = torch.randn(z_dim)\n",
        "  logits = enn(x,z)\n",
        "\n",
        "labels = y\n",
        "if labels.ndim == 1:\n",
        "    labels = labels.unsqueeze(1)\n",
        "\n",
        "candidate_scores = per_example_priority(logits, labels)\n",
        "\n",
        "pool_size = len(y)\n",
        "acquisition_size = min(acquisition_size, pool_size)\n",
        "\n",
        "selected_idxs = torch.argsort(candidate_scores, descending=True)[:acquisition_size]\n",
        "acquired_data = {k: v[selected_idxs] for k, v in batch.items()}\n"
      ],
      "metadata": {
        "id": "PNmG4sMeR2Zt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}