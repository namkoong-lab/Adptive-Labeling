# -*- coding: utf-8 -*-
"""ENN_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1foUFAnKD2kw-WDMknI_XU8h89T3WTlKW
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.init as init

#METHODOLOGY 1
# An implementation that combines basenet, learnable epinet, prior epinet

# shape of x in [batch_size,x_dim], z is [z_dim]
# assuming input is always included and output is never included
# hidden layers and exposed layers same number of entries



class ensemble_base(nn.Module):
    def __init__(self, input_size, basenet_hidden_sizes, n_classes, num_ensembles, seed_ensemble, provide_seed=True):
        super(ensemble_base, self).__init__()


        self.n_classes = n_classes
        self.num_ensemble = num_ensembles
        #self.alpha = alpha


        # Create a list of all sizes (input + hidden + output)
        basenet_all_sizes = [input_size] + basenet_hidden_sizes + [n_classes]
        if provide_seed:
           torch.manual_seed(seed_ensemble)

        self.ensemble = nn.ModuleList()
        for _ in range(self.num_ensemble):
            layers_1 = []
            all_sizes_prior_1 = basenet_all_sizes
            for i in range(len(all_sizes_prior_1) - 1):
                layer_1 = nn.Linear(all_sizes_prior_1[i], all_sizes_prior_1[i + 1])


                # Initialize weights and biases here
                #init.xavier_uniform_(layer.weight)
                #init.zeros_(layer.bias)

                layers_1.append(layer_1)
                if i < len(all_sizes_prior_1) - 2:
                    layers_1.append(nn.ReLU())

            mlp_1 = nn.Sequential(*layers_1)
            self.ensemble.append(mlp_1)
        

        #torch.manual_seed(seed_prior_ensemble)
        #self.prior_ensemble = nn.ModuleList()
        #for _ in range(self.num_ensemble):
        #    layers = []
        #    all_sizes_prior = basenet_all_sizes
        #    for i in range(len(all_sizes_prior) - 1):
        #        layer = nn.Linear(all_sizes_prior[i], all_sizes_prior[i + 1])


        #       # Initialize weights and biases here
        #        init.xavier_uniform_(layer.weight)
        #        init.zeros_(layer.bias)

        #        layers.append(layer)
        #        if i < len(all_sizes_prior) - 2:
        #            layers.append(nn.ReLU())

        #    mlp = nn.Sequential(*layers)
        #    # Freeze the parameters of this MLP
        #    for param in mlp.parameters():
        #        param.requires_grad = False

        #    self.prior_ensemble.append(mlp)    

        





    def forward(self, x, index):
        # Check if the index is within the valid range
        if index < 0 or index >= self.num_ensemble:
            raise ValueError(f"Index {index} is out of the valid range for the ensemble size {self.num_ensemble}")

        # Calculate the main ensemble and prior ensemble outputs
        main_output = self.ensemble[index](x)
        #prior_output = self.prior_ensemble[index](x)

        # Return the combined output
        return main_output










class ensemble_prior(nn.Module):
    def __init__(self, input_size, basenet_hidden_sizes, n_classes, num_ensembles, seed_prior_ensemble):
        super(ensemble_prior, self).__init__()


        self.n_classes = n_classes
        self.num_ensemble = num_ensembles
        #self.alpha = alpha


        # Create a list of all sizes (input + hidden + output)
        basenet_all_sizes = [input_size] + basenet_hidden_sizes + [n_classes]

        #torch.manual_seed(seed_ensemble)
        #self.ensemble = nn.ModuleList()
        #for _ in range(self.num_ensemble):
        #    layers_1 = []
        #    all_sizes_prior_1 = basenet_all_sizes
        #    for i in range(len(all_sizes_prior_1) - 1):
        #        layer_1 = nn.Linear(all_sizes_prior_1[i], all_sizes_prior_1[i + 1])


                # Initialize weights and biases here
                #init.xavier_uniform_(layer.weight)
                #init.zeros_(layer.bias)

        #        layers_1.append(layer_1)
        #        if i < len(all_sizes_prior_1) - 2:
        #            layers_1.append(nn.ReLU())

        #    mlp_1 = nn.Sequential(*layers_1)
        #    self.ensemble.append(mlp_1)
        

        torch.manual_seed(seed_prior_ensemble)
        self.prior_ensemble = nn.ModuleList()
        for _ in range(self.num_ensemble):
            layers = []
            all_sizes_prior = basenet_all_sizes
            for i in range(len(all_sizes_prior) - 1):
                layer = nn.Linear(all_sizes_prior[i], all_sizes_prior[i + 1])


                # Initialize weights and biases here
                init.xavier_uniform_(layer.weight)
                init.zeros_(layer.bias)

                layers.append(layer)
                if i < len(all_sizes_prior) - 2:
                    layers.append(nn.ReLU())

            mlp = nn.Sequential(*layers)
            # Freeze the parameters of this MLP
            for param in mlp.parameters():
                param.requires_grad = False

            self.prior_ensemble.append(mlp)    

        





    def forward(self, x, index):
        # Check if the index is within the valid range
        if index < 0 or index >= self.num_ensemble:
            raise ValueError(f"Index {index} is out of the valid range for the ensemble size {self.num_ensemble}")

        # Calculate the main ensemble and prior ensemble outputs
        #main_output = self.ensemble[index](x)
        prior_output = self.prior_ensemble[index](x)

        # Return the combined output
        return prior_output



