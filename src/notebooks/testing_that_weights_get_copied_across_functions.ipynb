{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n"
      ],
      "metadata": {
        "id": "HACHZuBAjR5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(10, 5)\n",
        "        self.fc2 = nn.Linear(5, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "jqFv8KKljqQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data(batch_size):\n",
        "    inputs = torch.randn(batch_size, 10)\n",
        "    targets = torch.randn(batch_size, 1)\n",
        "    return inputs, targets"
      ],
      "metadata": {
        "id": "iWZ0iNGtmWHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step_2(model_2, inputs, targets):\n",
        "    print(\"Params inside function trainig\")\n",
        "\n",
        "    for name, param in model_2.named_parameters():\n",
        "      print(f\"{name}: {param.size()}\")\n",
        "      print(param)\n",
        "\n",
        "    optimizer_2 = optim.SGD(model_2.parameters(), lr=0.01)\n",
        "    print(\"Params after optimizer inside function trainig\")\n",
        "\n",
        "    for name, param in model_2.named_parameters():\n",
        "      print(f\"{name}: {param.size()}\")\n",
        "      print(param)\n",
        "\n",
        "    print(\"\\nParameters in Optimizer:\")\n",
        "    for group in optimizer_2.param_groups:\n",
        "      for param in group['params']:\n",
        "        print(param)\n",
        "\n",
        "\n",
        "    criterion_2 = nn.MSELoss()\n",
        "    model_2.train()\n",
        "    optimizer_2.zero_grad()\n",
        "    outputs = model_2(inputs)\n",
        "    loss = criterion_2(outputs, targets)\n",
        "    loss.backward()\n",
        "    optimizer_2.step()\n",
        "\n",
        "    #for var_name in optimizer_2.state_dict():\n",
        "     #   print(var_name, \"\\n\", optimizer_2.state_dict()[var_name])\n",
        "    print(\"Params after training inside function trainig\")\n",
        "\n",
        "    for name, param in model_2.named_parameters():\n",
        "      print(f\"{name}: {param.size()}\")\n",
        "      print(param)\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "7JHwcdMOoKz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Does weights get carried across functions if optimizers are defined within those functions**\n",
        "\n",
        "if you define a new optimizer within each function, the optimizer's internal state (such as momentums in SGD or running averages in Adam) will not be carried over between the functions. However, the weights of the model itself will be carried over, as they are stored within the model object.\n",
        "\n",
        "Here's the key distinction:\n",
        "\n",
        "Model Weights: The model's weights and biases are attributes of the model object. If you pass the same model instance across different functions and update its weights, the updated weights persist across function calls because they are tied to the model instance.\n",
        "\n",
        "Optimizer State: The optimizer maintains its own state, which includes information like learning rates, momentums, etc. If you define a new optimizer in each function, each new optimizer starts with its default initial state, not aware of the previous updates made by a different optimizer instance.\n",
        "\n",
        "If you want the optimizer's state to persist across different functions, you should create the optimizer outside these functions and pass it as an argument, similar to how you would handle the model object."
      ],
      "metadata": {
        "id": "Wixy0Unnxvop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def experiment():     #what are the arguments\n",
        "\n",
        "    # Create an instance of the network\n",
        "\n",
        "    # Print the model's parameters\n",
        "\n",
        "    model = SimpleNet()\n",
        "\n",
        "    print(\"Params before any training\")\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "      print(f\"{name}: {param.size()}\")\n",
        "      print(param)\n",
        "\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    print(\"Params after defining optimizer\")\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "      print(f\"{name}: {param.size()}\")\n",
        "      print(param)\n",
        "\n",
        "    print(\"\\nParameters in Optimizer:\")\n",
        "    for group in optimizer.param_groups:\n",
        "      for param in group['params']:\n",
        "        print(param)\n",
        "\n",
        "    batch_size = 32\n",
        "    inputs_1, targets_1 = generate_data(batch_size)\n",
        "\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs_1 = model(inputs_1)\n",
        "    loss_1 = criterion(outputs_1, targets_1)\n",
        "    loss_1.backward()\n",
        "    optimizer.step()\n",
        "    #for var_name in optimizer.state_dict():\n",
        "     #   print(var_name, \"\\n\", optimizer.state_dict()[var_name])\n",
        "\n",
        "    print(loss_1.item())\n",
        "\n",
        "\n",
        "    print(\"Params after first trainig\")\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "      print(f\"{name}: {param.size()}\")\n",
        "      print(param)\n",
        "\n",
        "    inputs_2, targets_2 = generate_data(batch_size)\n",
        "\n",
        "    for i in range(3):\n",
        "      print(\"Params inside for loop before trainig\")\n",
        "      for name, param in model.named_parameters():\n",
        "        print(f\"{name}: {param.size()}\")\n",
        "        print(param)\n",
        "      loss_2 = train_step_2(model, inputs_2, targets_2)\n",
        "      print(f\"Loss after stage 2: {loss_2}\")\n",
        "      print(\"Params inside for loop after trainig\")\n",
        "      for name, param in model.named_parameters():\n",
        "        print(f\"{name}: {param.size()}\")\n",
        "        print(param)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cYD4eIMAjmAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0n71aQxo7Ro",
        "outputId": "d2af4dea-c560-479c-b20f-79240a266d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params before any training\n",
            "fc1.weight: torch.Size([5, 10])\n",
            "Parameter containing:\n",
            "tensor([[-0.1150,  0.2627,  0.1874, -0.0236, -0.0262,  0.2522,  0.0796,  0.2644,\n",
            "         -0.0987,  0.2446],\n",
            "        [-0.2835, -0.1992,  0.2061, -0.1774, -0.0465, -0.1941,  0.0694,  0.1202,\n",
            "         -0.1306,  0.2277],\n",
            "        [-0.1852,  0.2010,  0.2014,  0.1824,  0.3081,  0.2519,  0.0382,  0.2854,\n",
            "         -0.0019, -0.2112],\n",
            "        [-0.2094,  0.0764,  0.1692,  0.2396,  0.0766,  0.0171,  0.0904, -0.1763,\n",
            "         -0.2968, -0.1658],\n",
            "        [-0.0653, -0.2377, -0.1779, -0.0834,  0.1997,  0.1286,  0.1425, -0.0991,\n",
            "         -0.0707, -0.2052]], requires_grad=True)\n",
            "fc1.bias: torch.Size([5])\n",
            "Parameter containing:\n",
            "tensor([-0.2835,  0.2960, -0.2915,  0.1949,  0.2395], requires_grad=True)\n",
            "fc2.weight: torch.Size([1, 5])\n",
            "Parameter containing:\n",
            "tensor([[ 0.1337, -0.1685, -0.2931, -0.1561,  0.3518]], requires_grad=True)\n",
            "fc2.bias: torch.Size([1])\n",
            "Parameter containing:\n",
            "tensor([-0.1959], requires_grad=True)\n",
            "Params after defining optimizer\n",
            "fc1.weight: torch.Size([5, 10])\n",
            "Parameter containing:\n",
            "tensor([[-0.1150,  0.2627,  0.1874, -0.0236, -0.0262,  0.2522,  0.0796,  0.2644,\n",
            "         -0.0987,  0.2446],\n",
            "        [-0.2835, -0.1992,  0.2061, -0.1774, -0.0465, -0.1941,  0.0694,  0.1202,\n",
            "         -0.1306,  0.2277],\n",
            "        [-0.1852,  0.2010,  0.2014,  0.1824,  0.3081,  0.2519,  0.0382,  0.2854,\n",
            "         -0.0019, -0.2112],\n",
            "        [-0.2094,  0.0764,  0.1692,  0.2396,  0.0766,  0.0171,  0.0904, -0.1763,\n",
            "         -0.2968, -0.1658],\n",
            "        [-0.0653, -0.2377, -0.1779, -0.0834,  0.1997,  0.1286,  0.1425, -0.0991,\n",
            "         -0.0707, -0.2052]], requires_grad=True)\n",
            "fc1.bias: torch.Size([5])\n",
            "Parameter containing:\n",
            "tensor([-0.2835,  0.2960, -0.2915,  0.1949,  0.2395], requires_grad=True)\n",
            "fc2.weight: torch.Size([1, 5])\n",
            "Parameter containing:\n",
            "tensor([[ 0.1337, -0.1685, -0.2931, -0.1561,  0.3518]], requires_grad=True)\n",
            "fc2.bias: torch.Size([1])\n",
            "Parameter containing:\n",
            "tensor([-0.1959], requires_grad=True)\n",
            "1.4606454372406006\n",
            "Params after first trainig\n",
            "fc1.weight: torch.Size([5, 10])\n",
            "Parameter containing:\n",
            "tensor([[-0.1154,  0.2634,  0.1877, -0.0237, -0.0259,  0.2530,  0.0797,  0.2645,\n",
            "         -0.0986,  0.2448],\n",
            "        [-0.2827, -0.1996,  0.2056, -0.1773, -0.0473, -0.1947,  0.0692,  0.1196,\n",
            "         -0.1296,  0.2277],\n",
            "        [-0.1843,  0.2005,  0.2013,  0.1817,  0.3061,  0.2527,  0.0374,  0.2843,\n",
            "         -0.0006, -0.2119],\n",
            "        [-0.2088,  0.0762,  0.1688,  0.2393,  0.0759,  0.0166,  0.0904, -0.1762,\n",
            "         -0.2959, -0.1665],\n",
            "        [-0.0662, -0.2383, -0.1788, -0.0831,  0.2015,  0.1293,  0.1424, -0.0992,\n",
            "         -0.0736, -0.2039]], requires_grad=True)\n",
            "fc1.bias: torch.Size([5])\n",
            "Parameter containing:\n",
            "tensor([-0.2829,  0.2954, -0.2923,  0.1941,  0.2406], requires_grad=True)\n",
            "fc2.weight: torch.Size([1, 5])\n",
            "Parameter containing:\n",
            "tensor([[ 0.1368, -0.1658, -0.2904, -0.1518,  0.3546]], requires_grad=True)\n",
            "fc2.bias: torch.Size([1])\n",
            "Parameter containing:\n",
            "tensor([-0.1907], requires_grad=True)\n",
            "Params inside for loop before trainig\n",
            "fc1.weight: torch.Size([5, 10])\n",
            "Parameter containing:\n",
            "tensor([[-0.1154,  0.2634,  0.1877, -0.0237, -0.0259,  0.2530,  0.0797,  0.2645,\n",
            "         -0.0986,  0.2448],\n",
            "        [-0.2827, -0.1996,  0.2056, -0.1773, -0.0473, -0.1947,  0.0692,  0.1196,\n",
            "         -0.1296,  0.2277],\n",
            "        [-0.1843,  0.2005,  0.2013,  0.1817,  0.3061,  0.2527,  0.0374,  0.2843,\n",
            "         -0.0006, -0.2119],\n",
            "        [-0.2088,  0.0762,  0.1688,  0.2393,  0.0759,  0.0166,  0.0904, -0.1762,\n",
            "         -0.2959, -0.1665],\n",
            "        [-0.0662, -0.2383, -0.1788, -0.0831,  0.2015,  0.1293,  0.1424, -0.0992,\n",
            "         -0.0736, -0.2039]], requires_grad=True)\n",
            "fc1.bias: torch.Size([5])\n",
            "Parameter containing:\n",
            "tensor([-0.2829,  0.2954, -0.2923,  0.1941,  0.2406], requires_grad=True)\n",
            "fc2.weight: torch.Size([1, 5])\n",
            "Parameter containing:\n",
            "tensor([[ 0.1368, -0.1658, -0.2904, -0.1518,  0.3546]], requires_grad=True)\n",
            "fc2.bias: torch.Size([1])\n",
            "Parameter containing:\n",
            "tensor([-0.1907], requires_grad=True)\n",
            "Params inside function trainig\n",
            "fc1.weight: torch.Size([5, 10])\n",
            "Parameter containing:\n",
            "tensor([[-0.1154,  0.2634,  0.1877, -0.0237, -0.0259,  0.2530,  0.0797,  0.2645,\n",
            "         -0.0986,  0.2448],\n",
            "        [-0.2827, -0.1996,  0.2056, -0.1773, -0.0473, -0.1947,  0.0692,  0.1196,\n",
            "         -0.1296,  0.2277],\n",
            "        [-0.1843,  0.2005,  0.2013,  0.1817,  0.3061,  0.2527,  0.0374,  0.2843,\n",
            "         -0.0006, -0.2119],\n",
            "        [-0.2088,  0.0762,  0.1688,  0.2393,  0.0759,  0.0166,  0.0904, -0.1762,\n",
            "         -0.2959, -0.1665],\n",
            "        [-0.0662, -0.2383, -0.1788, -0.0831,  0.2015,  0.1293,  0.1424, -0.0992,\n",
            "         -0.0736, -0.2039]], requires_grad=True)\n",
            "fc1.bias: torch.Size([5])\n",
            "Parameter containing:\n",
            "tensor([-0.2829,  0.2954, -0.2923,  0.1941,  0.2406], requires_grad=True)\n",
            "fc2.weight: torch.Size([1, 5])\n",
            "Parameter containing:\n",
            "tensor([[ 0.1368, -0.1658, -0.2904, -0.1518,  0.3546]], requires_grad=True)\n",
            "fc2.bias: torch.Size([1])\n",
            "Parameter containing:\n",
            "tensor([-0.1907], requires_grad=True)\n",
            "Params after optimizer inside function trainig\n",
            "fc1.weight: torch.Size([5, 10])\n",
            "Parameter containing:\n",
            "tensor([[-0.1154,  0.2634,  0.1877, -0.0237, -0.0259,  0.2530,  0.0797,  0.2645,\n",
            "         -0.0986,  0.2448],\n",
            "        [-0.2827, -0.1996,  0.2056, -0.1773, -0.0473, -0.1947,  0.0692,  0.1196,\n",
            "         -0.1296,  0.2277],\n",
            "        [-0.1843,  0.2005,  0.2013,  0.1817,  0.3061,  0.2527,  0.0374,  0.2843,\n",
            "         -0.0006, -0.2119],\n",
            "        [-0.2088,  0.0762,  0.1688,  0.2393,  0.0759,  0.0166,  0.0904, -0.1762,\n",
            "         -0.2959, -0.1665],\n",
            "        [-0.0662, -0.2383, -0.1788, -0.0831,  0.2015,  0.1293,  0.1424, -0.0992,\n",
            "         -0.0736, -0.2039]], requires_grad=True)\n",
            "fc1.bias: torch.Size([5])\n",
            "Parameter containing:\n",
            "tensor([-0.2829,  0.2954, -0.2923,  0.1941,  0.2406], requires_grad=True)\n",
            "fc2.weight: torch.Size([1, 5])\n",
            "Parameter containing:\n",
            "tensor([[ 0.1368, -0.1658, -0.2904, -0.1518,  0.3546]], requires_grad=True)\n",
            "fc2.bias: torch.Size([1])\n",
            "Parameter containing:\n",
            "tensor([-0.1907], requires_grad=True)\n",
            "\n",
            "Parameters in Optimizer:\n",
            "Parameter containing:\n",
            "tensor([[-0.1154,  0.2634,  0.1877, -0.0237, -0.0259,  0.2530,  0.0797,  0.2645,\n",
            "         -0.0986,  0.2448],\n",
            "        [-0.2827, -0.1996,  0.2056, -0.1773, -0.0473, -0.1947,  0.0692,  0.1196,\n",
            "         -0.1296,  0.2277],\n",
            "        [-0.1843,  0.2005,  0.2013,  0.1817,  0.3061,  0.2527,  0.0374,  0.2843,\n",
            "         -0.0006, -0.2119],\n",
            "        [-0.2088,  0.0762,  0.1688,  0.2393,  0.0759,  0.0166,  0.0904, -0.1762,\n",
            "         -0.2959, -0.1665],\n",
            "        [-0.0662, -0.2383, -0.1788, -0.0831,  0.2015,  0.1293,  0.1424, -0.0992,\n",
            "         -0.0736, -0.2039]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.2829,  0.2954, -0.2923,  0.1941,  0.2406], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.1368, -0.1658, -0.2904, -0.1518,  0.3546]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1907], requires_grad=True)\n",
            "Params after training inside function trainig\n",
            "fc1.weight: torch.Size([5, 10])\n",
            "Parameter containing:\n",
            "tensor([[-0.1162,  0.2634,  0.1875, -0.0235, -0.0256,  0.2533,  0.0796,  0.2646,\n",
            "         -0.0984,  0.2451],\n",
            "        [-0.2812, -0.2001,  0.2053, -0.1780, -0.0478, -0.1943,  0.0698,  0.1200,\n",
            "         -0.1301,  0.2273],\n",
            "        [-0.1823,  0.2006,  0.2017,  0.1809,  0.3051,  0.2525,  0.0379,  0.2843,\n",
            "         -0.0006, -0.2122],\n",
            "        [-0.2074,  0.0755,  0.1686,  0.2384,  0.0754,  0.0169,  0.0911, -0.1756,\n",
            "         -0.2960, -0.1666],\n",
            "        [-0.0687, -0.2372, -0.1785, -0.0823,  0.2023,  0.1287,  0.1412, -0.1003,\n",
            "         -0.0730, -0.2037]], requires_grad=True)\n",
            "fc1.bias: torch.Size([5])\n",
            "Parameter containing:\n",
            "tensor([-0.2826,  0.2949, -0.2931,  0.1935,  0.2417], requires_grad=True)\n",
            "fc2.weight: torch.Size([1, 5])\n",
            "Parameter containing:\n",
            "tensor([[ 0.1373, -0.1633, -0.2887, -0.1470,  0.3546]], requires_grad=True)\n",
            "fc2.bias: torch.Size([1])\n",
            "Parameter containing:\n",
            "tensor([-0.1859], requires_grad=True)\n",
            "Loss after stage 2: 0.9405815005302429\n",
            "Params inside for loop after trainig\n",
            "fc1.weight: torch.Size([5, 10])\n",
            "Parameter containing:\n",
            "tensor([[-0.1162,  0.2634,  0.1875, -0.0235, -0.0256,  0.2533,  0.0796,  0.2646,\n",
            "         -0.0984,  0.2451],\n",
            "        [-0.2812, -0.2001,  0.2053, -0.1780, -0.0478, -0.1943,  0.0698,  0.1200,\n",
            "         -0.1301,  0.2273],\n",
            "        [-0.1823,  0.2006,  0.2017,  0.1809,  0.3051,  0.2525,  0.0379,  0.2843,\n",
            "         -0.0006, -0.2122],\n",
            "        [-0.2074,  0.0755,  0.1686,  0.2384,  0.0754,  0.0169,  0.0911, -0.1756,\n",
            "         -0.2960, -0.1666],\n",
            "        [-0.0687, -0.2372, -0.1785, -0.0823,  0.2023,  0.1287,  0.1412, -0.1003,\n",
            "         -0.0730, -0.2037]], requires_grad=True)\n",
            "fc1.bias: torch.Size([5])\n",
            "Parameter containing:\n",
            "tensor([-0.2826,  0.2949, -0.2931,  0.1935,  0.2417], requires_grad=True)\n",
            "fc2.weight: torch.Size([1, 5])\n",
            "Parameter containing:\n",
            "tensor([[ 0.1373, -0.1633, -0.2887, -0.1470,  0.3546]], requires_grad=True)\n",
            "fc2.bias: torch.Size([1])\n",
            "Parameter containing:\n",
            "tensor([-0.1859], requires_grad=True)\n",
            "Params inside for loop before trainig\n",
            "fc1.weight: torch.Size([5, 10])\n",
            "Parameter containing:\n",
            "tensor([[-0.1162,  0.2634,  0.1875, -0.0235, -0.0256,  0.2533,  0.0796,  0.2646,\n",
            "         -0.0984,  0.2451],\n",
            "        [-0.2812, -0.2001,  0.2053, -0.1780, -0.0478, -0.1943,  0.0698,  0.1200,\n",
            "         -0.1301,  0.2273],\n",
            "        [-0.1823,  0.2006,  0.2017,  0.1809,  0.3051,  0.2525,  0.0379,  0.2843,\n",
            "         -0.0006, -0.2122],\n",
            "        [-0.2074,  0.0755,  0.1686,  0.2384,  0.0754,  0.0169,  0.0911, -0.1756,\n",
            "         -0.2960, -0.1666],\n",
            "        [-0.0687, -0.2372, -0.1785, -0.0823,  0.2023,  0.1287,  0.1412, -0.1003,\n",
            "         -0.0730, -0.2037]], requires_grad=True)\n",
            "fc1.bias: torch.Size([5])\n",
            "Parameter containing:\n",
            "tensor([-0.2826,  0.2949, -0.2931,  0.1935,  0.2417], requires_grad=True)\n",
            "fc2.weight: torch.Size([1, 5])\n",
            "Parameter containing:\n",
            "tensor([[ 0.1373, -0.1633, -0.2887, -0.1470,  0.3546]], requires_grad=True)\n",
            "fc2.bias: torch.Size([1])\n",
            "Parameter containing:\n",
            "tensor([-0.1859], requires_grad=True)\n",
            "Params inside function trainig\n",
            "fc1.weight: torch.Size([5, 10])\n",
            "Parameter containing:\n",
            "tensor([[-0.1162,  0.2634,  0.1875, -0.0235, -0.0256,  0.2533,  0.0796,  0.2646,\n",
            "         -0.0984,  0.2451],\n",
            "        [-0.2812, -0.2001,  0.2053, -0.1780, -0.0478, -0.1943,  0.0698,  0.1200,\n",
            "         -0.1301,  0.2273],\n",
            "        [-0.1823,  0.2006,  0.2017,  0.1809,  0.3051,  0.2525,  0.0379,  0.2843,\n",
            "         -0.0006, -0.2122],\n",
            "        [-0.2074,  0.0755,  0.1686,  0.2384,  0.0754,  0.0169,  0.0911, -0.1756,\n",
            "         -0.2960, -0.1666],\n",
            "        [-0.0687, -0.2372, -0.1785, -0.0823,  0.2023,  0.1287,  0.1412, -0.1003,\n",
            "         -0.0730, -0.2037]], requires_grad=True)\n",
            "fc1.bias: torch.Size([5])\n",
            "Parameter containing:\n",
            "tensor([-0.2826,  0.2949, -0.2931,  0.1935,  0.2417], requires_grad=True)\n",
            "fc2.weight: torch.Size([1, 5])\n",
            "Parameter containing:\n",
            "tensor([[ 0.1373, -0.1633, -0.2887, -0.1470,  0.3546]], requires_grad=True)\n",
            "fc2.bias: torch.Size([1])\n",
            "Parameter containing:\n",
            "tensor([-0.1859], requires_grad=True)\n",
            "Params after optimizer inside function trainig\n",
            "fc1.weight: torch.Size([5, 10])\n",
            "Parameter containing:\n",
            "tensor([[-0.1162,  0.2634,  0.1875, -0.0235, -0.0256,  0.2533,  0.0796,  0.2646,\n",
            "         -0.0984,  0.2451],\n",
            "        [-0.2812, -0.2001,  0.2053, -0.1780, -0.0478, -0.1943,  0.0698,  0.1200,\n",
            "         -0.1301,  0.2273],\n",
            "        [-0.1823,  0.2006,  0.2017,  0.1809,  0.3051,  0.2525,  0.0379,  0.2843,\n",
            "         -0.0006, -0.2122],\n",
            "        [-0.2074,  0.0755,  0.1686,  0.2384,  0.0754,  0.0169,  0.0911, -0.1756,\n",
            "         -0.2960, -0.1666],\n",
            "        [-0.0687, -0.2372, -0.1785, -0.0823,  0.2023,  0.1287,  0.1412, -0.1003,\n",
            "         -0.0730, -0.2037]], requires_grad=True)\n",
            "fc1.bias: torch.Size([5])\n",
            "Parameter containing:\n",
            "tensor([-0.2826,  0.2949, -0.2931,  0.1935,  0.2417], requires_grad=True)\n",
            "fc2.weight: torch.Size([1, 5])\n",
            "Parameter containing:\n",
            "tensor([[ 0.1373, -0.1633, -0.2887, -0.1470,  0.3546]], requires_grad=True)\n",
            "fc2.bias: torch.Size([1])\n",
            "Parameter containing:\n",
            "tensor([-0.1859], requires_grad=True)\n",
            "\n",
            "Parameters in Optimizer:\n",
            "Parameter containing:\n",
            "tensor([[-0.1162,  0.2634,  0.1875, -0.0235, -0.0256,  0.2533,  0.0796,  0.2646,\n",
            "         -0.0984,  0.2451],\n",
            "        [-0.2812, -0.2001,  0.2053, -0.1780, -0.0478, -0.1943,  0.0698,  0.1200,\n",
            "         -0.1301,  0.2273],\n",
            "        [-0.1823,  0.2006,  0.2017,  0.1809,  0.3051,  0.2525,  0.0379,  0.2843,\n",
            "         -0.0006, -0.2122],\n",
            "        [-0.2074,  0.0755,  0.1686,  0.2384,  0.0754,  0.0169,  0.0911, -0.1756,\n",
            "         -0.2960, -0.1666],\n",
            "        [-0.0687, -0.2372, -0.1785, -0.0823,  0.2023,  0.1287,  0.1412, -0.1003,\n",
            "         -0.0730, -0.2037]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.2826,  0.2949, -0.2931,  0.1935,  0.2417], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.1373, -0.1633, -0.2887, -0.1470,  0.3546]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1859], requires_grad=True)\n",
            "Params after training inside function trainig\n",
            "fc1.weight: torch.Size([5, 10])\n",
            "Parameter containing:\n",
            "tensor([[-0.1170,  0.2634,  0.1873, -0.0232, -0.0252,  0.2535,  0.0796,  0.2646,\n",
            "         -0.0983,  0.2453],\n",
            "        [-0.2798, -0.2006,  0.2051, -0.1786, -0.0484, -0.1940,  0.0704,  0.1205,\n",
            "         -0.1305,  0.2268],\n",
            "        [-0.1804,  0.2006,  0.2020,  0.1800,  0.3042,  0.2522,  0.0383,  0.2843,\n",
            "         -0.0006, -0.2125],\n",
            "        [-0.2061,  0.0748,  0.1683,  0.2375,  0.0749,  0.0171,  0.0918, -0.1750,\n",
            "         -0.2961, -0.1668],\n",
            "        [-0.0712, -0.2360, -0.1783, -0.0814,  0.2031,  0.1281,  0.1401, -0.1015,\n",
            "         -0.0724, -0.2034]], requires_grad=True)\n",
            "fc1.bias: torch.Size([5])\n",
            "Parameter containing:\n",
            "tensor([-0.2823,  0.2944, -0.2938,  0.1930,  0.2427], requires_grad=True)\n",
            "fc2.weight: torch.Size([1, 5])\n",
            "Parameter containing:\n",
            "tensor([[ 0.1379, -0.1610, -0.2870, -0.1422,  0.3545]], requires_grad=True)\n",
            "fc2.bias: torch.Size([1])\n",
            "Parameter containing:\n",
            "tensor([-0.1812], requires_grad=True)\n",
            "Loss after stage 2: 0.9320842027664185\n",
            "Params inside for loop after trainig\n",
            "fc1.weight: torch.Size([5, 10])\n",
            "Parameter containing:\n",
            "tensor([[-0.1170,  0.2634,  0.1873, -0.0232, -0.0252,  0.2535,  0.0796,  0.2646,\n",
            "         -0.0983,  0.2453],\n",
            "        [-0.2798, -0.2006,  0.2051, -0.1786, -0.0484, -0.1940,  0.0704,  0.1205,\n",
            "         -0.1305,  0.2268],\n",
            "        [-0.1804,  0.2006,  0.2020,  0.1800,  0.3042,  0.2522,  0.0383,  0.2843,\n",
            "         -0.0006, -0.2125],\n",
            "        [-0.2061,  0.0748,  0.1683,  0.2375,  0.0749,  0.0171,  0.0918, -0.1750,\n",
            "         -0.2961, -0.1668],\n",
            "        [-0.0712, -0.2360, -0.1783, -0.0814,  0.2031,  0.1281,  0.1401, -0.1015,\n",
            "         -0.0724, -0.2034]], requires_grad=True)\n",
            "fc1.bias: torch.Size([5])\n",
            "Parameter containing:\n",
            "tensor([-0.2823,  0.2944, -0.2938,  0.1930,  0.2427], requires_grad=True)\n",
            "fc2.weight: torch.Size([1, 5])\n",
            "Parameter containing:\n",
            "tensor([[ 0.1379, -0.1610, -0.2870, -0.1422,  0.3545]], requires_grad=True)\n",
            "fc2.bias: torch.Size([1])\n",
            "Parameter containing:\n",
            "tensor([-0.1812], requires_grad=True)\n",
            "Params inside for loop before trainig\n",
            "fc1.weight: torch.Size([5, 10])\n",
            "Parameter containing:\n",
            "tensor([[-0.1170,  0.2634,  0.1873, -0.0232, -0.0252,  0.2535,  0.0796,  0.2646,\n",
            "         -0.0983,  0.2453],\n",
            "        [-0.2798, -0.2006,  0.2051, -0.1786, -0.0484, -0.1940,  0.0704,  0.1205,\n",
            "         -0.1305,  0.2268],\n",
            "        [-0.1804,  0.2006,  0.2020,  0.1800,  0.3042,  0.2522,  0.0383,  0.2843,\n",
            "         -0.0006, -0.2125],\n",
            "        [-0.2061,  0.0748,  0.1683,  0.2375,  0.0749,  0.0171,  0.0918, -0.1750,\n",
            "         -0.2961, -0.1668],\n",
            "        [-0.0712, -0.2360, -0.1783, -0.0814,  0.2031,  0.1281,  0.1401, -0.1015,\n",
            "         -0.0724, -0.2034]], requires_grad=True)\n",
            "fc1.bias: torch.Size([5])\n",
            "Parameter containing:\n",
            "tensor([-0.2823,  0.2944, -0.2938,  0.1930,  0.2427], requires_grad=True)\n",
            "fc2.weight: torch.Size([1, 5])\n",
            "Parameter containing:\n",
            "tensor([[ 0.1379, -0.1610, -0.2870, -0.1422,  0.3545]], requires_grad=True)\n",
            "fc2.bias: torch.Size([1])\n",
            "Parameter containing:\n",
            "tensor([-0.1812], requires_grad=True)\n",
            "Params inside function trainig\n",
            "fc1.weight: torch.Size([5, 10])\n",
            "Parameter containing:\n",
            "tensor([[-0.1170,  0.2634,  0.1873, -0.0232, -0.0252,  0.2535,  0.0796,  0.2646,\n",
            "         -0.0983,  0.2453],\n",
            "        [-0.2798, -0.2006,  0.2051, -0.1786, -0.0484, -0.1940,  0.0704,  0.1205,\n",
            "         -0.1305,  0.2268],\n",
            "        [-0.1804,  0.2006,  0.2020,  0.1800,  0.3042,  0.2522,  0.0383,  0.2843,\n",
            "         -0.0006, -0.2125],\n",
            "        [-0.2061,  0.0748,  0.1683,  0.2375,  0.0749,  0.0171,  0.0918, -0.1750,\n",
            "         -0.2961, -0.1668],\n",
            "        [-0.0712, -0.2360, -0.1783, -0.0814,  0.2031,  0.1281,  0.1401, -0.1015,\n",
            "         -0.0724, -0.2034]], requires_grad=True)\n",
            "fc1.bias: torch.Size([5])\n",
            "Parameter containing:\n",
            "tensor([-0.2823,  0.2944, -0.2938,  0.1930,  0.2427], requires_grad=True)\n",
            "fc2.weight: torch.Size([1, 5])\n",
            "Parameter containing:\n",
            "tensor([[ 0.1379, -0.1610, -0.2870, -0.1422,  0.3545]], requires_grad=True)\n",
            "fc2.bias: torch.Size([1])\n",
            "Parameter containing:\n",
            "tensor([-0.1812], requires_grad=True)\n",
            "Params after optimizer inside function trainig\n",
            "fc1.weight: torch.Size([5, 10])\n",
            "Parameter containing:\n",
            "tensor([[-0.1170,  0.2634,  0.1873, -0.0232, -0.0252,  0.2535,  0.0796,  0.2646,\n",
            "         -0.0983,  0.2453],\n",
            "        [-0.2798, -0.2006,  0.2051, -0.1786, -0.0484, -0.1940,  0.0704,  0.1205,\n",
            "         -0.1305,  0.2268],\n",
            "        [-0.1804,  0.2006,  0.2020,  0.1800,  0.3042,  0.2522,  0.0383,  0.2843,\n",
            "         -0.0006, -0.2125],\n",
            "        [-0.2061,  0.0748,  0.1683,  0.2375,  0.0749,  0.0171,  0.0918, -0.1750,\n",
            "         -0.2961, -0.1668],\n",
            "        [-0.0712, -0.2360, -0.1783, -0.0814,  0.2031,  0.1281,  0.1401, -0.1015,\n",
            "         -0.0724, -0.2034]], requires_grad=True)\n",
            "fc1.bias: torch.Size([5])\n",
            "Parameter containing:\n",
            "tensor([-0.2823,  0.2944, -0.2938,  0.1930,  0.2427], requires_grad=True)\n",
            "fc2.weight: torch.Size([1, 5])\n",
            "Parameter containing:\n",
            "tensor([[ 0.1379, -0.1610, -0.2870, -0.1422,  0.3545]], requires_grad=True)\n",
            "fc2.bias: torch.Size([1])\n",
            "Parameter containing:\n",
            "tensor([-0.1812], requires_grad=True)\n",
            "\n",
            "Parameters in Optimizer:\n",
            "Parameter containing:\n",
            "tensor([[-0.1170,  0.2634,  0.1873, -0.0232, -0.0252,  0.2535,  0.0796,  0.2646,\n",
            "         -0.0983,  0.2453],\n",
            "        [-0.2798, -0.2006,  0.2051, -0.1786, -0.0484, -0.1940,  0.0704,  0.1205,\n",
            "         -0.1305,  0.2268],\n",
            "        [-0.1804,  0.2006,  0.2020,  0.1800,  0.3042,  0.2522,  0.0383,  0.2843,\n",
            "         -0.0006, -0.2125],\n",
            "        [-0.2061,  0.0748,  0.1683,  0.2375,  0.0749,  0.0171,  0.0918, -0.1750,\n",
            "         -0.2961, -0.1668],\n",
            "        [-0.0712, -0.2360, -0.1783, -0.0814,  0.2031,  0.1281,  0.1401, -0.1015,\n",
            "         -0.0724, -0.2034]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.2823,  0.2944, -0.2938,  0.1930,  0.2427], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.1379, -0.1610, -0.2870, -0.1422,  0.3545]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1812], requires_grad=True)\n",
            "Params after training inside function trainig\n",
            "fc1.weight: torch.Size([5, 10])\n",
            "Parameter containing:\n",
            "tensor([[-0.1177,  0.2634,  0.1871, -0.0229, -0.0249,  0.2537,  0.0796,  0.2647,\n",
            "         -0.0982,  0.2455],\n",
            "        [-0.2785, -0.2010,  0.2048, -0.1792, -0.0489, -0.1937,  0.0710,  0.1210,\n",
            "         -0.1310,  0.2264],\n",
            "        [-0.1786,  0.2007,  0.2024,  0.1792,  0.3033,  0.2520,  0.0388,  0.2843,\n",
            "         -0.0007, -0.2127],\n",
            "        [-0.2048,  0.0742,  0.1681,  0.2366,  0.0745,  0.0174,  0.0924, -0.1744,\n",
            "         -0.2962, -0.1670],\n",
            "        [-0.0737, -0.2348, -0.1781, -0.0806,  0.2039,  0.1275,  0.1389, -0.1027,\n",
            "         -0.0719, -0.2032]], requires_grad=True)\n",
            "fc1.bias: torch.Size([5])\n",
            "Parameter containing:\n",
            "tensor([-0.2820,  0.2940, -0.2945,  0.1925,  0.2437], requires_grad=True)\n",
            "fc2.weight: torch.Size([1, 5])\n",
            "Parameter containing:\n",
            "tensor([[ 0.1385, -0.1588, -0.2855, -0.1376,  0.3545]], requires_grad=True)\n",
            "fc2.bias: torch.Size([1])\n",
            "Parameter containing:\n",
            "tensor([-0.1768], requires_grad=True)\n",
            "Loss after stage 2: 0.9240174293518066\n",
            "Params inside for loop after trainig\n",
            "fc1.weight: torch.Size([5, 10])\n",
            "Parameter containing:\n",
            "tensor([[-0.1177,  0.2634,  0.1871, -0.0229, -0.0249,  0.2537,  0.0796,  0.2647,\n",
            "         -0.0982,  0.2455],\n",
            "        [-0.2785, -0.2010,  0.2048, -0.1792, -0.0489, -0.1937,  0.0710,  0.1210,\n",
            "         -0.1310,  0.2264],\n",
            "        [-0.1786,  0.2007,  0.2024,  0.1792,  0.3033,  0.2520,  0.0388,  0.2843,\n",
            "         -0.0007, -0.2127],\n",
            "        [-0.2048,  0.0742,  0.1681,  0.2366,  0.0745,  0.0174,  0.0924, -0.1744,\n",
            "         -0.2962, -0.1670],\n",
            "        [-0.0737, -0.2348, -0.1781, -0.0806,  0.2039,  0.1275,  0.1389, -0.1027,\n",
            "         -0.0719, -0.2032]], requires_grad=True)\n",
            "fc1.bias: torch.Size([5])\n",
            "Parameter containing:\n",
            "tensor([-0.2820,  0.2940, -0.2945,  0.1925,  0.2437], requires_grad=True)\n",
            "fc2.weight: torch.Size([1, 5])\n",
            "Parameter containing:\n",
            "tensor([[ 0.1385, -0.1588, -0.2855, -0.1376,  0.3545]], requires_grad=True)\n",
            "fc2.bias: torch.Size([1])\n",
            "Parameter containing:\n",
            "tensor([-0.1768], requires_grad=True)\n"
          ]
        }
      ]
    }
  ]
}